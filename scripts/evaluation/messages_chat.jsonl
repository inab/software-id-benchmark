{"slimfastq/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"github/slimfastq/None/v2.04\",\n    \"name\": \"slimfastq\",\n    \"description\": [\n      \"Fast, efficient, lossless compression of fastq files.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/Infinidat/slimfastq\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://sf.net/p/slimfastq/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"github\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"sourceforge/slimfastq/None/None\",\n    \"name\": \"slimfastq\",\n    \"description\": [\n      \"Slimfastq is a cli application that compresses/decompresses fastq files.\\nIt features:\\n\\n* High compression ratio\\n* Relatively low cpu/memory usage\\n* Truly lossless compression/decompression.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://sourceforge.net/projects/slimfastq\",\n        \"kind\": \"sourceforge\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://slimfastq.sourceforge.io/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"sourceforge\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/Infinidat/slimfastq:\n```\nREADME content:\n['slimfastq ========= slimfastq would efficiently compresses/decompresses fastq files. It features: * High compression ratio * Relatively low CPU, memory usage * Truly lossless compression/decompression * Posix piping friendly (i.e. fastq input/output steam is serialzed during compression/decompression) Usage ----- <code> % slimfastq *file.fastq* *new-file.sfq* : compress *file.fastq* to *new-file.sfq* % slimfastq -1 *file.fastq* *new-file.sfq*: compress *file.fastq* to *new-file.sfq*, using little CPU/memory resources (-1 to -4 are levels of compression/resources trade-offs, -3 is default) % slimfastq *file.sfq* : decompress *file.sfq* to stdout (format is determined by stamp, not name) % slimfastq *file.sfq* *file.fastq : decompress *file.sfq* to *file.fastq* % slimfastq -h : get help pipe usage: % gzip -dc *file.fastq.gz* | slimfastq -f *file.sfq* : convert from gzip to sfq format (and save a lot of disk space) % slimfastq *file.sfq* | md5sum - : get the checksum of the decompressed file without creating a file </code> The multi threads FAQ --------------------- The main reason slimfastq is a single thread application is to avoid the overhead of semaphores, L2 flushes, and context switches. The goal is to focus on the speed of N files compression/decompression instead of a single file. Use slimfastq.multi script (located under tools/) to compress/decompress multiple files in parallel. The \\'-h\\' argument, as expected, will provide help. This script can be easily edited for a sepcial setup. Please do not hesitate to email me if any help is needed. Compile ------- Simple compilation: * run \"make\" or \"make test\" (If the compilation fails, please let me know). Profile optimized compilation: * Compile slimfastq in a gcc profile generator mode (use \\'make slimfastq.prof\\'). * Compress/decompress some of your fastq files - this will generate some *.gcda files. * Recompile with optimization flags for the profiler generated data (use \\'make slimfastq.opt\\'). (For whatever it\\'s worth, the author of this page could not notice any significant performance change yield by the optimized compilation.) Test ------- \"make test\" will compress, decompress and compare all the fastq files in the ./samples dir. Some testing tips to check your own fastq files: 1) slimfastq is a lossless compression and posix pipes friendly, therefore it\\'s easy to check the integrity of a large file with checksums: <code> % md5sum large-file.fq % ./slimfastq largefile.fq -O /tmp/tst.sfq % ./slimfastq /tmp/tst.sfq | md5sum - </code> (if md5sum don\\'t match, one can use ./tools/mydiff.pl to the bad line. And I\\'ll be grateful for a bug report) 2) use time. Example: <code> % /usr/bin/time -f \" IO : io=%I faults=%F\\\\n MEM: max=%M kb Average=%K kb\\\\n CPU: Percentage=%P real=%e sys=%S user=%U\" slimfastq large-file.fq /tmp/a.tst -O </code> 3) Performance wise, a single file compression/decompression is not very interesting. The script tools/slimfastq.multi can be used to evaluate performance of concurrent files compression/decompression. This script can be edited to use with other compression softwares - general or fastq specific. 4) Using slimfastq.multi, try to increase/decrease thread count to find the optimal number for a specific system. Install ------- After compile * run \"sudo make install\" * Alternatively to \"make install\", copy the \"slimfastq\" executable and the \"tools/slimfastq.multi\" script to any location. License ------- The BSD 3-Clause Platform -------- slimfastq was developed and optimized for x86_64 GNU/Linux and Darwin OS. For other system\\'s support requests, please contact Josef Ezra. Contact ------- Josef Ezra (jezra at appple.com), (jezra at cpan.org)']```"}, {"role": "user", "content": "Content from https://github.com/Infinidat/slimfastq:\n```\nRepository metadata:\n{'name': 'slimfastq', 'label': ['slimfastq'], 'description': [{'term': 'Fast, efficient, lossless compression of fastq files ', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://sf.net/p/slimfastq/', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v2.04', 'v2.03', '2.02', 'v2.01'], 'license': [], 'repository': [{'term': 'https://github.com/Infinidat/slimfastq', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Josef Ezra', 'type': 'person', 'email': 'jezra@apple.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Josef Ezra', 'type': 'person', 'email': 'jezra@infinidat.com', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/Infinidat/slimfastq/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://slimfastq.sourceforge.io/:\n```\nProject metadata:\n{'description': 'slimfastq is a cli application that compresses/decompresses fastq files.\\nIt features:\\n* High compression ratio\\n* Relatively low cpu/memory usage\\n* Truly lossless compression/decompression', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['https://sourceforge.net/auth/?return_to=%2Fprojects%2Fslimfastq%2F', '/p/slimfastq/activity/feed', '/p/slimfastq/activity/']}, {'text': 'Features\\nfastq Compression\\nFeatures\\nfastq Compression', 'hrefs': []}, {'text': 'Categories\\nBio-Informatics\\nLicense\\nBSD License\\nCategories\\nBio-Informatics\\nLicense\\nBSD License', 'hrefs': ['/directory/bio-informatics/']}, {'text': 'Follow slimfastq\\nslimfastq Web Site\\nFollow slimfastq\\nslimfastq Web Site', 'hrefs': ['https://slimfastq.sourceforge.io/']}, {'text': 'User Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nUser Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5', 'hrefs': []}, {'text': 'User Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\npsttrash\\nEdited 2020-01-24\\nExcellent speed and compression ratio. Beats gzip by miles. I am using it for long-term fastq storage. It is saving us lots of disk space. The author is very responsive to requests and changes. I highly recommend slimfastq.\\nRead more reviews >\\nUser Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\npsttrash\\nEdited 2020-01-24\\nExcellent speed and compression ratio. Beats gzip by miles. I am using it for long-term fastq storage. It is saving us lots of disk space. The author is very responsive to requests and changes. I highly recommend slimfastq.\\nRead more reviews >', 'hrefs': ['/projects/slimfastq/reviews/#reviews', '#']}, {'text': 'Additional Project Details\\nIntended Audience\\nHealthcare Industry, Science/Research, Advanced End Users\\nUser Interface\\nCommand-line\\nProgramming Language\\nC++\\nRelated Categories\\nC++ Bio-Informatics Software\\nRegistered\\n2013-12-12\\nAdditional Project Details\\nIntended Audience\\nHealthcare Industry, Science/Research, Advanced End Users\\nUser Interface\\nCommand-line\\nProgramming Language\\nC++\\nRelated Categories\\nC++ Bio-Informatics Software\\nRegistered\\n2013-12-12', 'hrefs': ['/directory/c-plus-plus/', '/directory/bio-informatics/c++/']}]}```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/slimfastq:\n```\nProject metadata:\n{'description': 'slimfastq is a cli application that compresses/decompresses fastq files.\\nIt features:\\n* High compression ratio\\n* Relatively low cpu/memory usage\\n* Truly lossless compression/decompression', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['https://sourceforge.net/auth/?return_to=%2Fprojects%2Fslimfastq%2F', '/p/slimfastq/activity/feed', '/p/slimfastq/activity/']}, {'text': 'Features\\nfastq Compression\\nFeatures\\nfastq Compression', 'hrefs': []}, {'text': 'Categories\\nBio-Informatics\\nLicense\\nBSD License\\nCategories\\nBio-Informatics\\nLicense\\nBSD License', 'hrefs': ['/directory/bio-informatics/']}, {'text': 'Follow slimfastq\\nslimfastq Web Site\\nFollow slimfastq\\nslimfastq Web Site', 'hrefs': ['https://slimfastq.sourceforge.io/']}, {'text': 'User Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nUser Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5', 'hrefs': []}, {'text': 'User Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\npsttrash\\nEdited 2020-01-24\\nExcellent speed and compression ratio. Beats gzip by miles. I am using it for long-term fastq storage. It is saving us lots of disk space. The author is very responsive to requests and changes. I highly recommend slimfastq.\\nRead more reviews >\\nUser Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\npsttrash\\nEdited 2020-01-24\\nExcellent speed and compression ratio. Beats gzip by miles. I am using it for long-term fastq storage. It is saving us lots of disk space. The author is very responsive to requests and changes. I highly recommend slimfastq.\\nRead more reviews >', 'hrefs': ['/projects/slimfastq/reviews/#reviews', '#']}, {'text': 'Additional Project Details\\nIntended Audience\\nHealthcare Industry, Science/Research, Advanced End Users\\nUser Interface\\nCommand-line\\nProgramming Language\\nC++\\nRelated Categories\\nC++ Bio-Informatics Software\\nRegistered\\n2013-12-12\\nAdditional Project Details\\nIntended Audience\\nHealthcare Industry, Science/Research, Advanced End Users\\nUser Interface\\nCommand-line\\nProgramming Language\\nC++\\nRelated Categories\\nC++ Bio-Informatics Software\\nRegistered\\n2013-12-12', 'hrefs': ['/directory/c-plus-plus/', '/directory/bio-informatics/c++/']}]}```"}, {"role": "user", "content": "Content from https://sf.net/p/slimfastq/:\n```\nProject metadata:\n{'description': 'slimfastq is a cli application that compresses/decompresses fastq files.\\nIt features:\\n* High compression ratio\\n* Relatively low cpu/memory usage\\n* Truly lossless compression/decompression', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['https://sourceforge.net/auth/?return_to=%2Fprojects%2Fslimfastq%2F', '/p/slimfastq/activity/feed', '/p/slimfastq/activity/']}, {'text': 'Features\\nfastq Compression\\nFeatures\\nfastq Compression', 'hrefs': []}, {'text': 'Categories\\nBio-Informatics\\nLicense\\nBSD License\\nCategories\\nBio-Informatics\\nLicense\\nBSD License', 'hrefs': ['/directory/bio-informatics/']}, {'text': 'Follow slimfastq\\nslimfastq Web Site\\nFollow slimfastq\\nslimfastq Web Site', 'hrefs': ['https://slimfastq.sourceforge.io/']}, {'text': 'User Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nUser Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5', 'hrefs': []}, {'text': 'User Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\npsttrash\\nEdited 2020-01-24\\nExcellent speed and compression ratio. Beats gzip by miles. I am using it for long-term fastq storage. It is saving us lots of disk space. The author is very responsive to requests and changes. I highly recommend slimfastq.\\nRead more reviews >\\nUser Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\npsttrash\\nEdited 2020-01-24\\nExcellent speed and compression ratio. Beats gzip by miles. I am using it for long-term fastq storage. It is saving us lots of disk space. The author is very responsive to requests and changes. I highly recommend slimfastq.\\nRead more reviews >', 'hrefs': ['/projects/slimfastq/reviews/#reviews', '#']}, {'text': 'Additional Project Details\\nIntended Audience\\nHealthcare Industry, Science/Research, Advanced End Users\\nUser Interface\\nCommand-line\\nProgramming Language\\nC++\\nRelated Categories\\nC++ Bio-Informatics Software\\nRegistered\\n2013-12-12\\nAdditional Project Details\\nIntended Audience\\nHealthcare Industry, Science/Research, Advanced End Users\\nUser Interface\\nCommand-line\\nProgramming Language\\nC++\\nRelated Categories\\nC++ Bio-Informatics Software\\nRegistered\\n2013-12-12', 'hrefs': ['/directory/c-plus-plus/', '/directory/bio-informatics/c++/']}]}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"pyfastx/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"github/pyfastx/None/2.2.0\",\n    \"name\": \"pyfastx\",\n    \"description\": [\n      \"A python package for fast random access to sequences from plain and gzipped FASTA/Q files.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/lmdu/pyfastx\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://pyfastx.readthedocs.io/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"github\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/pyfastx/undefined/None\",\n    \"name\": \"pyfastx\",\n    \"description\": [\n      \"The pyfastx is a lightweight Python C extension that enables users to randomly access to sequences from plain and gzipped FASTA Q files.\\n\\nAll tools support for random access were used to perform benchmark.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://pypi.org/project/pyfastx\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Bisong Yue\",\n        \"email\": \"zhaokelei@cdu.edu.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Kelei Zhao\",\n        \"email\": \"bsyue@scu.edu.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8328c10e2f3aa0b3bb220\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://pyfastx.readthedocs.io/:\n```\nContent:\n['* Single file for the Python extension * Lightweight, memory efficient for parsing FASTA file * Fast random access to sequences from**gzipped**FASTA file * Read sequences from FASTA file line by line * Calculate assembly N50 and L50 * Calculate GC content and nucleotides composition * Extract reverse, complement and antisense sequence * Excellent compatibility, support for parsing nonstandard FASTA file * Support for random access reads from FASTQ file']```"}, {"role": "user", "content": "Content from https://github.com/lmdu/pyfastx:\n```\nREADME content:\n['pyfastx ####### .. image:: https://github.com/lmdu/pyfastx/actions/workflows/wheel.yml/badge.svg :target: https://github.com/lmdu/pyfastx/actions/workflows/wheel.yml :alt: Action .. image:: https://readthedocs.org/projects/pyfastx/badge/?version=latest :target: https://pyfastx.readthedocs.io/en/latest/?badge=latest :alt: Readthedocs .. image:: https://codecov.io/gh/lmdu/pyfastx/branch/master/graph/badge.svg :target: https://codecov.io/gh/lmdu/pyfastx :alt: Codecov .. image:: https://img.shields.io/pypi/v/pyfastx.svg :target: https://pypi.org/project/pyfastx :alt: PyPI .. image:: https://img.shields.io/pypi/wheel/pyfastx.svg :target: https://pypi.org/project/pyfastx :alt: Wheel .. image:: https://api.codacy.com/project/badge/Grade/80790fa30f444d9d9ece43689d512dae :target: https://www.codacy.com/manual/lmdu/pyfastx?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=lmdu/pyfastx&amp;utm_campaign=Badge_Grade :alt: Codacy .. image:: https://img.shields.io/pypi/implementation/pyfastx :target: https://pypi.org/project/pyfastx :alt: Language .. image:: https://img.shields.io/pypi/pyversions/pyfastx.svg :target: https://pypi.org/project/pyfastx :alt: Pyver .. image:: https://img.shields.io/pypi/dm/pyfastx :target: https://pypi.org/project/pyfastx :alt: Downloads .. image:: https://img.shields.io/pypi/l/pyfastx :target: https://pypi.org/project/pyfastx :alt: License .. image:: https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat :target: http://bioconda.github.io/recipes/pyfastx/README.html :alt: Bioconda **Citation:** `Lianming Du, Qin Liu, Zhenxin Fan, Jie Tang, Xiuyue Zhang, Megan Price, Bisong Yue, Kelei Zhao. Pyfastx: a robust Python package for fast random access to sequences from plain and gzipped FASTA/Q files. Briefings in Bioinformatics, 2021, 22(4):bbaa368 <https://doi.org/10.1093/bib/bbaa368>`_. .. contents:: Table of Contents Introduction ============ The ``pyfastx`` is a lightweight Python C extension that enables users to randomly access to sequences from plain and **gzipped** FASTA/Q files. This module aims to provide simple APIs for users to extract seqeunce from FASTA and reads from FASTQ by identifier and index number. The ``pyfastx`` will build indexes stored in a sqlite3 database file for random access to avoid consuming excessive amount of memory. In addition, the ``pyfastx`` can parse standard (*sequence is spread into multiple lines with same length*) and nonstandard (*sequence is spread into one or more lines with different length*) FASTA format. This module used `kseq.h <https://github.com/attractivechaos/klib/blob/master/kseq.h>`_ written by `@attractivechaos <https://github.com/attractivechaos>`_ in `klib <https://github.com/attractivechaos/klib>`_ project to parse plain FASTA/Q file and zran.c written by `@pauldmccarthy <https://github.com/pauldmccarthy>`_ in project `indexed_gzip <https://github.com/pauldmccarthy/indexed_gzip>`_ to index gzipped file for random access. This project was heavily inspired by `@mdshw5 <https://github.com/mdshw5>`_\\'s project `pyfaidx <https://github.com/mdshw5/pyfaidx>`_ and `@brentp <https://github.com/brentp>`_\\'s project `pyfasta <https://github.com/brentp/pyfasta>`_. Features ======== - Single file for the Python extension - Lightweight, memory efficient for parsing FASTA/Q file - Fast random access to sequences from ``gzipped`` FASTA/Q file - Read sequences from FASTA file line by line - Calculate N50 and L50 of sequences in FASTA file - Calculate GC content and nucleotides composition - Extract reverse, complement and antisense sequences - Excellent compatibility, support for parsing nonstandard FASTA file - Support for FASTQ quality score conversion - Provide command line interface for splitting FASTA/Q file Installation ============ Currently, ``pyfastx`` supports Python 3.6, 3.7, 3.8, 3.9, 3.10, 3.11. Make sure you have installed both `pip <https://pip.pypa.io/en/stable/installing/>`_ and Python before starting. You can install ``pyfastx`` via the Python Package Index (PyPI) :: pip install pyfastx Update ``pyfastx`` module :: pip install -U pyfastx FASTX ===== New in ``pyfastx`` 0.8.0. Pyfastx provide a simple and fast python binding for kseq.h to iterate over sequences or reads in fasta/q file. The FASTX object will automatically detect the input sequence format (fasta or fastq) to return different tuple. FASTA sequences iteration ------------------------- When iterating over sequences on FASTX object, a tuple ``(name, seq)`` will be returned. .. code:: python >>> fa = pyfastx.Fastx(\\'tests/data/test.fa.gz\\') >>> for name,seq in fa: >>> print(name) >>> print(seq) >>> #always output uppercase sequence >>> for item in pyfastx.Fastx(\\'tests/data/test.fa\\', uppercase=True): >>> print(item) >>> #Manually specify sequence format >>> for item in pyfastx.Fastx(\\'tests/data/test.fa\\', format=\"fasta\"): >>> print(item) If you want the sequence comment, you can set comment to True, New in ``pyfastx`` 0.9.0. .. code:: python >>> fa = pyfastx.Fastx(\\'tests/data/test.fa.gz\\', comment=True) >>> for name,seq,comment in fa: >>> print(name) >>> print(seq) >>> print(comment) The comment is the content of header line after the first white space or tab character. FASTQ reads iteration --------------------- When iterating over reads on FASTX object, a tuple ``(name, seq, qual)`` will be returned. .. code:: python >>> fq = pyfastx.Fastx(\\'tests/data/test.fq.gz\\') >>> for name,seq,qual in fq: >>> print(name) >>> print(seq) >>> print(qual) If you want the read comment, you can set comment to True, New in ``pyfastx`` 0.9.0. .. code:: python >>> fq = pyfastx.Fastx(\\'tests/data/test.fq.gz\\', comment=True) >>> for name,seq,qual,comment in fq: >>> print(name) >>> print(seq) >>> print(qual) >>> print(comment) The comment is the content of header line after the first white space or tab character. FASTA ===== Read FASTA file --------------- Read plain or gzipped FASTA file and build index, support for random access to FASTA. .. code:: python >>> import pyfastx >>> fa = pyfastx.Fasta(\\'test/data/test.fa.gz\\') >>> fa <Fasta> test/data/test.fa.gz contains 211 seqs .. note:: Building index may take some times. The time required to build index depends on the size of FASTA file. If index built, you can randomly access to any sequences in FASTA file. The index file can be reused to save time when you read seqeunces from FASTA file next time. FASTA records iteration ----------------------- The fastest way to iterate plain or gzipped FASTA file without building index, the iteration will return a tuple contains name and sequence. .. code:: python >>> import pyfastx >>> for name, seq in pyfastx.Fasta(\\'test/data/test.fa.gz\\', build_index=False): >>> print(name, seq) You can also iterate sequence object from FASTA object like this: .. code:: python >>> import pyfastx >>> for seq in pyfastx.Fasta(\\'test/data/test.fa.gz\\'): >>> print(seq.name) >>> print(seq.seq) >>> print(seq.description) Iteration with ``build_index=True`` (default) return sequence object which allows you to access attributions of sequence. New in pyfastx 0.6.3. Get FASTA information --------------------- .. code:: python >>> # get sequence counts in FASTA >>> len(fa) 211 >>> # get total sequence length of FASTA >>> fa.size 86262 >>> # get GC content of DNA sequence of FASTA >>> fa.gc_content 43.529014587402344 >>> # get GC skew of DNA sequences in FASTA >>> # New in pyfastx 0.3.8 >>> fa.gc_skew 0.004287730902433395 >>> # get composition of nucleotides in FASTA >>> fa.composition {\\'A\\': 24534, \\'C\\': 18694, \\'G\\': 18855, \\'T\\': 24179} >>> # get fasta type (DNA, RNA, or protein) >>> fa.type \\'DNA\\' >>> # check fasta file is gzip compressed >>> fa.is_gzip True Get longest and shortest sequence --------------------------------- New in ``pyfastx`` 0.3.0 .. code:: python >>> # get longest sequence >>> s = fa.longest >>> s <Sequence> JZ822609.1 with length of 821 >>> s.name \\'JZ822609.1\\' >>> len(s) 821 >>> # get shortest sequence >>> s = fa.shortest >>> s <Sequence> JZ822617.1 with length of 118 >>> s.name \\'JZ822617.1\\' >>> len(s) 118 Calculate N50 and L50 --------------------- New in ``pyfastx`` 0.3.0 Calculate assembly N50 and L50, return (N50, L50), learn more about `N50,L50 <https://www.molecularecologist.com/2017/03/whats-n50/>`_ .. code:: python >>> # get FASTA N50 and L50 >>> fa.nl(50) (516, 66) >>> # get FASTA N90 and L90 >>> fa.nl(90) (231, 161) >>> # get FASTA N75 and L75 >>> fa.nl(75) (365, 117) Get sequence mean and median length ----------------------------------- New in ``pyfastx`` 0.3.0 .. code:: python >>> # get sequence average length >>> fa.mean 408 >>> # get seqeunce median length >>> fa.median 430 Get sequence counts ------------------- New in ``pyfastx`` 0.3.0 Get counts of sequences whose length >= specified length .. code:: python >>> # get counts of sequences with length >= 200 bp >>> fa.count(200) 173 >>> # get counts of sequences with length >= 500 bp >>> fa.count(500) 70 Get subsequences ---------------- Subsequences can be retrieved from FASTA file by using a list of [start, end] coordinates .. code:: python >>> # get subsequence with start and end position >>> interval = (1, 10) >>> fa.fetch(\\'JZ822577.1\\', interval) \\'CTCTAGAGAT\\' >>> # get subsequences with a list of start and end position >>> intervals = [(1, 10), (50, 60)] >>> fa.fetch(\\'JZ822577.1\\', intervals) \\'CTCTAGAGATTTTAGTTTGAC\\' >>> # get subsequences with reverse strand >>> fa.fetch(\\'JZ822577.1\\', (1, 10), strand=\\'-\\') \\'ATCTCTAGAG\\' Key function ------------ New in ``pyfastx`` 0.5.1 Sometimes your fasta will have a long header which contains multiple identifiers and description, for example, \">JZ822577.1 contig1 cDNA library of flower petals in tree peony by suppression subtractive hybridization Paeonia suffruticosa cDNA, mRNA sequence\". In this case, both \"JZ822577.1\" and \"contig1\" can be used as identifer. you can specify the key function to select one as identifier. .. code:: python >>> #default use JZ822577.1 as identifier >>> #specify key_func to select contig1 as identifer >>> fa = pyfastx.Fasta(\\'tests/data/test.fa.gz\\', key_func=lambda x: x.split()[1]) >>> fa <Fasta> tests/data/test.fa.gz contains 211 seqs Sequence ======== Get a sequence from FASTA ------------------------- .. code:: python >>> # get sequence like a dictionary by identifier >>> s1 = fa[\\'JZ822577.1\\'] >>> s1 <Sequence> JZ822577.1 with length of 333 >>> # get sequence like a list by index >>> s2 = fa[2] >>> s2 <Sequence> JZ822579.1 with length of 176 >>> # get last sequence >>> s3 = fa[-1] >>> s3 <Sequence> JZ840318.1 with length of 134 >>> # check a sequence name weather in FASTA file >>> \\'JZ822577.1\\' in fa True Get sequence information ------------------------ .. code:: python >>> s = fa[-1] >>> s <Sequence> JZ840318.1 with length of 134 >>> # get sequence order number in FASTA file >>> # New in pyfastx 0.3.7 >>> s.id 211 >>> # get sequence name >>> s.name \\'JZ840318.1\\' >>> # get sequence description >>> # New in pyfastx 0.3.1 >>> s.description \\'R283 cDNA library of flower petals in tree peony by suppression subtractive hybridization Paeonia suffruticosa cDNA, mRNA sequence\\' >>> # get sequence string >>> s.seq \\'ACTGGAGGTTCTTCTTCCTGTGGAAAGTAACTTGTTTTGCCTTCACCTGCCTGTTCTTCACATCAACCTTGTTCCCACACAAAACAATGGGAATGTTCTCACACACCCTGCAGAGATCACGATGCCATGTTGGT\\' >>> # get sequence raw string, New in pyfastx 0.6.3 >>> print(s.raw) >JZ840318.1 R283 cDNA library of flower petals in tree peony by suppression subtractive hybridization Paeonia suffruticosa cDNA, mRNA sequence ACTGGAGGTTCTTCTTCCTGTGGAAAGTAACTTGTTTTGCCTTCACCTGCCTGTTCTTCACATCAACCTT GTTCCCACACAAAACAATGGGAATGTTCTCACACACCCTGCAGAGATCACGATGCCATGTTGGT >>> # get sequence length >>> len(s) 134 >>> # get GC content if dna sequence >>> s.gc_content 46.26865768432617 >>> # get nucleotide composition if dna sequence >>> s.composition {\\'A\\': 31, \\'C\\': 37, \\'G\\': 25, \\'T\\': 41, \\'N\\': 0} Sequence slice -------------- Sequence object can be sliced like a python string .. code:: python >>> # get a sub seq from sequence >>> s = fa[-1] >>> ss = s[10:30] >>> ss <Sequence> JZ840318.1 from 11 to 30 >>> ss.name \\'JZ840318.1:11-30\\' >>> ss.seq \\'CTTCTTCCTGTGGAAAGTAA\\' >>> ss = s[-10:] >>> ss <Sequence> JZ840318.1 from 125 to 134 >>> ss.name \\'JZ840318.1:125-134\\' >>> ss.seq \\'CCATGTTGGT\\' .. note:: Slicing start and end coordinates are 0-based. Currently, pyfastx does not support an optional third ``step`` or ``stride`` argument. For example ``ss[::-1]`` Reverse and complement sequence ------------------------------- .. code:: python >>> # get sliced sequence >>> fa[0][10:20].seq \\'GTCAATTTCC\\' >>> # get reverse of sliced sequence >>> fa[0][10:20].reverse \\'CCTTTAACTG\\' >>> # get complement of sliced sequence >>> fa[0][10:20].complement \\'CAGTTAAAGG\\' >>> # get reversed complement sequence, corresponding to sequence in antisense strand >>> fa[0][10:20].antisense \\'GGAAATTGAC\\' Read sequence line by line -------------------------- New in ``pyfastx`` 0.3.0 The sequence object can be iterated line by line as they appear in FASTA file. .. code:: python >>> for line in fa[0]: ... print(line) ... CTCTAGAGATTACTTCTTCACATTCCAGATCACTCAGGCTCTTTGTCATTTTAGTTTGACTAGGATATCG AGTATTCAAGCTCATCGCTTTTGGTAATCTTTGCGGTGCATGCCTTTGCATGCTGTATTGCTGCTTCATC ATCCCCTTTGACTTGTGTGGCGGTGGCAAGACATCCGAAGAGTTAAGCGATGCTTGTCTAGTCAATTTCC CCATGTACAGAATCATTGTTGTCAATTGGTTGTTTCCTTGATGGTGAAGGGGCTTCAATACATGAGTTCC AAACTAACATTTCTTGACTAACACTTGAGGAAGAAGGACAAGGGTCCCCATGT .. note:: Sliced sequence (e.g. fa[0][10:50]) cannot be read line by line Search for subsequence ---------------------- New in ``pyfastx`` 0.3.6 Search for subsequence from given sequence and get one-based start position of the first occurrence .. code:: python >>> # search subsequence in sense strand >>> fa[0].search(\\'GCTTCAATACA\\') 262 >>> # check subsequence weather in sequence >>> \\'GCTTCAATACA\\' in fa[0] True >>> # search subsequence in antisense strand >>> fa[0].search(\\'CCTCAAGT\\', \\'-\\') 301 FastaKeys ========= New in ``pyfastx`` 0.8.0. We have changed ``Identifier`` object to ``FastaKeys`` object. Get keys -------------- Get all names of sequence as a list-like object. .. code:: python >>> ids = fa.keys() >>> ids <FastaKeys> contains 211 keys >>> # get count of sequence >>> len(ids) 211 >>> # get key by index >>> ids[0] \\'JZ822577.1\\' >>> # check key whether in fasta >>> \\'JZ822577.1\\' in ids True >>> # iterate over keys >>> for name in ids: >>> print(name) >>> # convert to a list >>> list(ids) Sort keys ---------------- Sort keys by sequence id, name, or length for iteration New in ``pyfastx`` 0.5.0 .. code:: python >>> # sort keys by length with descending order >>> for name in ids.sort(by=\\'length\\', reverse=True): >>> print(name) >>> # sort keys by name with ascending order >>> for name in ids.sort(by=\\'name\\'): >>> print(name) >>> # sort keys by id with descending order >>> for name in ids.sort(by=\\'id\\', reverse=True) >>> print(name) Filter keys ------------------ Filter keys by sequence length and name New in ``pyfastx`` 0.5.10 .. code:: python >>> # get keys with length > 600 >>> ids.filter(ids > 600) <FastaKeys> contains 48 keys >>> # get keys with length >= 500 and <= 700 >>> ids.filter(ids>=500, ids<=700) <FastaKeys> contains 48 keys >>> # get keys with length > 500 and < 600 >>> ids.filter(500<ids<600) <FastaKeys> contains 22 keys >>> # get keys contain JZ8226 >>> ids.filter(ids % \\'JZ8226\\') <FastaKeys> contains 90 keys >>> # get keys contain JZ8226 with length > 550 >>> ids.filter(ids % \\'JZ8226\\', ids>550) <FastaKeys> contains 17 keys >>> # clear sort order and filters >>> ids.reset() <FastaKeys> contains 211 keys >>> # list a filtered result >>> ids.filter(ids % \\'JZ8226\\', ids>730) >>> list(ids) [\\'JZ822609.1\\', \\'JZ822650.1\\', \\'JZ822664.1\\', \\'JZ822699.1\\'] >>> # list a filtered result with sort order >>> ids.filter(ids % \\'JZ8226\\', ids>730).sort(\\'length\\', reverse=True) >>> list(ids) [\\'JZ822609.1\\', \\'JZ822699.1\\', \\'JZ822664.1\\', \\'JZ822650.1\\'] >>> ids.filter(ids % \\'JZ8226\\', ids>730).sort(\\'name\\', reverse=True) >>> list(ids) [\\'JZ822699.1\\', \\'JZ822664.1\\', \\'JZ822650.1\\', \\'JZ822609.1\\'] FASTQ ===== New in ``pyfastx`` 0.4.0 Read FASTQ file --------------- Read plain or gzipped file and build index, support for random access to reads from FASTQ. .. code:: python >>> import pyfastx >>> fq = pyfastx.Fastq(\\'tests/data/test.fq.gz\\') >>> fq <Fastq> tests/data/test.fq.gz contains 100 reads FASTQ records iteration ----------------------- The fastest way to parse plain or gzipped FASTQ file without building index, the iteration will return a tuple contains read name, seq and quality. .. code:: python >>> import pyfastx >>> for name,seq,qual in pyfastx.Fastq(\\'tests/data/test.fq.gz\\', build_index=False): >>> print(name) >>> print(seq) >>> print(qual) You can also iterate read object from FASTQ object like this: .. code:: python >>> import pyfastx >>> for read in pyfastx.Fastq(\\'test/data/test.fq.gz\\'): >>> print(read.name) >>> print(read.seq) >>> print(read.qual) >>> print(read.quali) Iteration with ``build_index=True`` (default) return read object which allows you to access attribution of read. New in pyfastx 0.6.3. Get FASTQ information --------------------- .. code:: python >>> # get read counts in FASTQ >>> len(fq) 800 >>> # get total bases >>> fq.size 120000 >>> # get GC content of FASTQ file >>> fq.gc_content 66.17471313476562 >>> # get composition of bases in FASTQ >>> fq.composition {\\'A\\': 20501, \\'C\\': 39705, \\'G\\': 39704, \\'T\\': 20089, \\'N\\': 1} >>> # New in pyfastx 0.6.10 >>> # get average length of reads >>> fq.avglen 150.0 >>> # get maximum lenth of reads >>> fq.maxlen 150 >>> # get minimum length of reas >>> fq.minlen 150 >>> # get maximum quality score >>> fq.maxqual 70 >>> # get minimum quality score >>> fq.minqual 35 >>> # get phred which affects the quality score conversion >>> fq.phred 33 >>> # Guess fastq quality encoding system >>> # New in pyfastx 0.4.1 >>> fq.encoding_type [\\'Sanger Phred+33\\', \\'Illumina 1.8+ Phred+33\\'] Read ==== Get read from FASTQ ------------------- .. code:: python >>> #get read like a dict by read name >>> r1 = fq[\\'A00129:183:H77K2DMXX:1:1101:4752:1047\\'] >>> r1 <Read> A00129:183:H77K2DMXX:1:1101:4752:1047 with length of 150 >>> # get read like a list by index >>> r2 = fq[10] >>> r2 <Read> A00129:183:H77K2DMXX:1:1101:18041:1078 with length of 150 >>> # get the last read >>> r3 = fq[-1] >>> r3 <Read> A00129:183:H77K2DMXX:1:1101:31575:4726 with length of 150 >>> # check a read weather in FASTQ file >>> \\'A00129:183:H77K2DMXX:1:1101:4752:1047\\' in fq True Get read information -------------------- .. code:: python >>> r = fq[-10] >>> r <Read> A00129:183:H77K2DMXX:1:1101:1750:4711 with length of 150 >>> # get read order number in FASTQ file >>> r.id 791 >>> # get read name >>> r.name \\'A00129:183:H77K2DMXX:1:1101:1750:4711\\' >>> # get read full header line, New in pyfastx 0.6.3 >>> r.description \\'@A00129:183:H77K2DMXX:1:1101:1750:4711 1:N:0:CAATGGAA+CGAGGCTG\\' >>> # get read length >>> len(r) 150 >>> # get read sequence >>> r.seq', \"'CGAGGAAATCGACGTCACCGATCTGGAAGCCCTGCGCGCCCATCTCAACCAGAAATGGGGTGGCCAGCGCGGCAAGCTGACCCTGCTGCCGTTCCTGGTCCGCGCCATGGTCGTGGCGCTGCGCGACTTCCCGCAGTTGAACGCGCGCTA' >>> # get raw string of read, New in pyfastx 0.6.3 >>> print(r.raw) @A00129:183:H77K2DMXX:1:1101:1750:4711 1:N:0:CAATGGAA+CGAGGCTG CGAGGAAATCGACGTCACCGATCTGGAAGCCCTGCGCGCCCATCTCAACCAGAAATGGGGTGGCCAGCGCGGCAAGCTGACCCTGCTGCCGTTCCTGGTCCGCGCCATGGTCGTGGCGCTGCGCGACTTCCCGCAGTTGAACGCGCGCTA + FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FF,FFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFF: >>> # get read quality ascii string >>> r.qual 'FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FF,FFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFF:' >>> # get read quality integer value, ascii - 33 or 64 >>> r.quali [37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 37, 37, 11, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 11, 37, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25] >>> # get read length >>> len(r) 150 FastqKeys ========= New in ``pyfastx`` 0.8.0. Get fastq keys --------------- Get all names of read as a list-like object. .. code:: python >>> ids = fq.keys() >>> ids <FastqKeys> contains 800 keys >>> # get count of read >>> len(ids) 800 >>> # get key by index >>> ids[0] 'A00129:183:H77K2DMXX:1:1101:6804:1031' >>> # check key whether in fasta >>> 'A00129:183:H77K2DMXX:1:1101:14416:1031' in ids True Command line interface ====================== New in ``pyfastx`` 0.5.0 .. code:: bash $ pyfastx -h usage: pyfastx COMMAND [OPTIONS] A command line tool for FASTA/Q file manipulation optional arguments: -h, --help show this help message and exit -v, --version show program's version number and exit Commands: index build index for fasta/q file stat show detailed statistics information of fasta/q file split split fasta/q file into multiple files fq2fa convert fastq file to fasta file subseq get subsequences from fasta file by region sample randomly sample sequences from fasta or fastq file extract extract full sequences or reads from fasta/q file Build index ----------- New in ``pyfastx`` 0.6.10 .. code:: bash $ pyfastx index -h usage: pyfastx index [-h] [-f] fastx [fastx ...] positional arguments: fastx fasta or fastq file, gzip support optional arguments: -h, --help show this help message and exit -f, --full build full index, base composition will be calculated Show statistics information --------------------------- .. code:: bash $ pyfastx stat -h usage: pyfastx info [-h] fastx positional arguments: fastx input fasta or fastq file, gzip support optional arguments: -h, --help show this help message and exit Split FASTA/Q file ------------------ .. code:: bash $ pyfastx split -h usage: pyfastx split [-h] (-n int | -c int) [-o str] fastx positional arguments: fastx fasta or fastq file, gzip support optional arguments: -h, --help show this help message and exit -n int split a fasta/q file into N new files with even size -c int split a fasta/q file into multiple files containing the same sequence counts -o str, --out-dir str output directory, default is current folder Convert FASTQ to FASTA file --------------------------- .. code:: bash $ pyfastx fq2fa -h usage: pyfastx fq2fa [-h] [-o str] fastx positional arguments: fastx fastq file, gzip support optional arguments: -h, --help show this help message and exit -o str, --out-file str output file, default: output to stdout Get subsequence with region --------------------------- .. code:: bash $ pyfastx subseq -h usage: pyfastx subseq [-h] [-r str | -b str] [-o str] fastx [region [region ...]] positional arguments: fastx input fasta file, gzip support region format is chr:start-end, start and end position is 1-based, multiple names were separated by space optional arguments: -h, --help show this help message and exit -r str, --region-file str tab-delimited file, one region per line, both start and end position are 1-based -b str, --bed-file str tab-delimited BED file, 0-based start position and 1-based end position -o str, --out-file str output file, default: output to stdout Sample sequences ---------------- .. code:: bash $ pyfastx sample -h usage: pyfastx sample [-h] (-n int | -p float) [-s int] [--sequential-read] [-o str] fastx positional arguments: fastx fasta or fastq file, gzip support optional arguments: -h, --help show this help message and exit -n int number of sequences to be sampled -p float proportion of sequences to be sampled, 0~1 -s int, --seed int random seed, default is the current system time --sequential-read start sequential reading, particularly suitable for sampling large numbers of sequences -o str, --out-file str output file, default: output to stdout Extract sequences ----------------- New in ``pyfastx`` 0.6.10 .. code:: bash $ pyfastx extract -h usage: pyfastx extract [-h] [-l str] [--reverse-complement] [--out-fasta] [-o str] [--sequential-read] fastx [name [name ...]] positional arguments: fastx fasta or fastq file, gzip support name sequence name or read name, multiple names were separated by space optional arguments: -h, --help show this help message and exit -l str, --list-file str a file containing sequence or read names, one name per line --reverse-complement output reverse complement sequence --out-fasta output fasta format when extract reads from fastq, default output fastq format -o str, --out-file str output file, default: output to stdout --sequential-read start sequential reading, particularly suitable for extracting large numbers of sequences Drawbacks ========= If you intensively check sequence names exists in FASTA file using ``in`` operator on FASTA object like: .. code:: python >>> fa = pyfastx.Fasta('tests/data/test.fa.gz') >>> # Suppose seqnames has 100000 names >>> for seqname in seqnames: >>> if seqname in fa: >>> do something This will take a long time to finish. Becuase, pyfastx does not load the index into memory, the ``in`` operating is corresponding to sql query existence from index database. The faster alternative way to do this is: .. code:: python >>> fa = pyfastx.Fasta('tests/data/test.fa.gz') >>> # load all sequence names into a set object >>> all_names = set(fa.keys()) >>> for seqname in seqnames: >>> if seqname in all_names: >>> do something Testing ======= The ``pyfaidx`` module was used to test ``pyfastx``. First, make sure you have a suitable version installed:: pip install pyfastx To test pyfastx, you should also install pyfaidx 0.5.8:: pip install pyfaidx==0.5.8 Then, to run the tests:: $ python setup.py test Acknowledgements ================ `kseq.h <https://github.com/attractivechaos/klib/blob/master/kseq.h>`_ and `zlib <https://www.zlib.net/>`_ was used to parse FASTA format. `Sqlite3 <https://www.sqlite.org/index.html>`_ was used to store built indexes. ``pyfastx`` can randomly access to sequences from gzipped FASTA file mainly attributed to `indexed_gzip <https://github.com/pauldmccarthy/indexed_gzip>`_.\"]```"}, {"role": "user", "content": "Content from https://github.com/lmdu/pyfastx:\n```\nRepository metadata:\n{'name': 'pyfastx', 'label': ['pyfastx'], 'description': [{'term': 'a python package for fast random access to sequences from plain and gzipped FASTA/Q files', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://pyfastx.readthedocs.io', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['2.2.0', '2.1.0', '2.0.2', '2.0.1', '2.0.0', '1.1.0', '1.0.1', '1.0.0', '0.9.1', '0.9.0', '0.8.4', '0.8.3', '0.8.2', '0.8.1', '0.8.0', '0.7.0', '0.6.17', '0.6.16', '0.6.15', '0.6.14', '0.6.13', '0.6.12', '0.6.11', '0.6.10', '0.6.9', '0.6.8', '0.6.7', '0.6.6', '0.6.5', '0.6.4', '0.6.3', '0.6.2', '0.6.1', '0.6.0', '0.5.10', '0.5.9', '0.5.8', '0.5.7', '0.5.6', '0.5.5', '0.5.4', '0.5.3', '0.5.2', '0.5.1', '0.5.0', '0.4.1', '0.4.0', '0.3.10', '0.3.9', '0.3.8', '0.3.7', '0.3.6', '0.3.5', '0.3.4', '0.3.3', '0.3.1', '0.3.0', '0.2.11', '0.2.10', '0.2.9', '0.2.8', '0.2.7', '0.2.6', '0.2.5', '0.2.4', '0.2.3', '0.2.2', '0.2.1', '0.2.0', '0.1.12', '0.1.11', '0.1.10', '0.1.9', '0.1.8', '0.1.7', '0.1.6', '0.1.5', '0.1.4', '0.1.3', '0.1.2', '0.1.1', '0.1.0'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/lmdu/pyfastx', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/fasta', 'term': 'fasta', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/python', 'term': 'python', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/biology', 'term': 'biology', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/dna', 'term': 'dna', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/genome', 'term': 'genome', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/assembly', 'term': 'assembly', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/sequence', 'term': 'sequence', 'vocabulary': ''}, 'id': 6}, {'term': {'uri': 'https://github.com/topics/fastq', 'term': 'fastq', 'vocabulary': ''}, 'id': 7}, {'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 8}], 'operations': [], 'authors': [{'term': {'name': 'Lianming Du', 'type': 'person', 'email': 'adullb@qq.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Lianming Du', 'type': 'person', 'email': 'lmdu@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Pino Toscano', 'type': 'person', 'email': 'toscano.pino@tiscali.it', 'maintainer': False}, 'id': 2}, {'term': {'name': 'L. Drew Pihera', 'type': 'person', 'email': 'dpihera@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Andrea Telatin, M1', 'type': 'person', 'email': '15690844+telatin@users.noreply.github.com', 'maintainer': False}, 'id': 4}, {'term': {'name': '\u00c9tienne Mollier', 'type': 'person', 'email': 'emollier@debian.org', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Cornelius Roemer', 'type': 'person', 'email': 'cornelius.roemer@gmail.com', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Thomas Sibley', 'type': 'person', 'email': 'tsibley@fredhutch.org', 'maintainer': False}, 'id': 7}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'requirements', 'url': 'https://github.com/lmdu/pyfastx/blob/main/docs/requirements.txt'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://pypi.org/project/pyfastx:\n```\nProject metadata:\n{'author': 'Lianming Du', 'author_email': 'adullb@qq.com', 'classifiers': ['Development Status :: 5 - Production/Stable', 'Intended Audience :: Developers', 'Intended Audience :: Education', 'Intended Audience :: Science/Research', 'License :: OSI Approved :: MIT License', 'Natural Language :: English', 'Operating System :: Microsoft :: Windows', 'Operating System :: POSIX :: Linux', 'Operating System :: Unix', 'Programming Language :: C', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Programming Language :: Python :: 3.12', 'Programming Language :: Python :: 3.13', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: 3.7', 'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9', 'Topic :: Scientific/Engineering :: Bio-Informatics'], 'description': 'pyfastx\\n#######\\n\\n.. image:: https://github.com/lmdu/pyfastx/actions/workflows/main.yml/badge.svg\\n   :target: https://github.com/lmdu/pyfastx/actions/workflows/main.yml\\n   :alt: Action\\n\\n.. image:: https://readthedocs.org/projects/pyfastx/badge/?version=latest\\n   :target: https://pyfastx.readthedocs.io/en/latest/?badge=latest\\n   :alt: Readthedocs\\n\\n.. image:: https://codecov.io/gh/lmdu/pyfastx/branch/master/graph/badge.svg\\n   :target: https://codecov.io/gh/lmdu/pyfastx\\n   :alt: Codecov\\n\\n.. image:: https://img.shields.io/pypi/v/pyfastx.svg\\n   :target: https://pypi.org/project/pyfastx\\n   :alt: PyPI\\n\\n.. image:: https://img.shields.io/pypi/implementation/pyfastx\\n   :target: https://pypi.org/project/pyfastx\\n   :alt: Language\\n\\n.. image:: https://img.shields.io/pypi/pyversions/pyfastx.svg\\n   :target: https://pypi.org/project/pyfastx\\n   :alt: Pyver\\n\\n.. image:: https://img.shields.io/pypi/wheel/pyfastx.svg\\n   :target: https://pypi.org/project/pyfastx\\n   :alt: Wheel\\n\\n.. image:: https://api.codacy.com/project/badge/Grade/80790fa30f444d9d9ece43689d512dae\\n   :target: https://www.codacy.com/manual/lmdu/pyfastx?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=lmdu/pyfastx&amp;utm_campaign=Badge_Grade\\n   :alt: Codacy\\n\\n.. image:: https://img.shields.io/pypi/dm/pyfastx\\n   :target: https://pypi.org/project/pyfastx\\n   :alt: Downloads\\n\\n.. image:: https://img.shields.io/pypi/l/pyfastx\\n   :target: https://pypi.org/project/pyfastx\\n   :alt: License\\n\\n.. image:: https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat\\n   :target: http://bioconda.github.io/recipes/pyfastx/README.html\\n   :alt: Bioconda\\n\\n**Citation:** \\n`Lianming Du, Qin Liu, Zhenxin Fan, Jie Tang, Xiuyue Zhang, Megan Price, Bisong Yue, Kelei Zhao. Pyfastx: a robust Python package for fast random access to sequences from plain and gzipped FASTA/Q files. Briefings in Bioinformatics, 2021, 22(4):bbaa368 <https://doi.org/10.1093/bib/bbaa368>`_.\\n\\n.. contents:: Table of Contents\\n\\nIntroduction\\n============\\n\\nThe ``pyfastx`` is a lightweight Python C extension that enables users to randomly access to sequences from plain and **gzipped** FASTA/Q files. This module aims to provide simple APIs for users to extract seqeunce from FASTA and reads from FASTQ by identifier and index number. The ``pyfastx`` will build indexes stored in a sqlite3 database file for random access to avoid consuming excessive amount of memory. In addition, the ``pyfastx`` can parse standard (*sequence is spread into multiple lines with same length*) and nonstandard (*sequence is spread into one or more lines with different length*) FASTA format. This module used `kseq.h <https://github.com/attractivechaos/klib/blob/master/kseq.h>`_ written by `@attractivechaos <https://github.com/attractivechaos>`_ in `klib <https://github.com/attractivechaos/klib>`_ project to parse plain FASTA/Q file and zran.c written by `@pauldmccarthy <https://github.com/pauldmccarthy>`_ in project `indexed_gzip <https://github.com/pauldmccarthy/indexed_gzip>`_ to index gzipped file for random access.\\n\\nThis project was heavily inspired by `@mdshw5 <https://github.com/mdshw5>`_\\'s project `pyfaidx <https://github.com/mdshw5/pyfaidx>`_ and `@brentp <https://github.com/brentp>`_\\'s project `pyfasta <https://github.com/brentp/pyfasta>`_.\\n\\nFeatures\\n========\\n\\n- Single file for the Python extension\\n- Lightweight, memory efficient for parsing FASTA/Q file\\n- Fast random access to sequences from ``gzipped`` FASTA/Q file\\n- Read sequences from FASTA file line by line\\n- Calculate N50 and L50 of sequences in FASTA file\\n- Calculate GC content and nucleotides composition\\n- Extract reverse, complement and antisense sequences\\n- Excellent compatibility, support for parsing nonstandard FASTA file\\n- Support for FASTQ quality score conversion\\n- Provide command line interface for splitting FASTA/Q file\\n\\nInstallation\\n============\\n\\nCurrently, ``pyfastx`` supports Python 3.6, 3.7, 3.8, 3.9, 3.10, 3.11. Make sure you have installed both `pip <https://pip.pypa.io/en/stable/installing/>`_ and Python before starting.\\n\\nYou can install ``pyfastx`` via the Python Package Index (PyPI)\\n\\n::\\n\\n    pip install pyfastx\\n\\nUpdate ``pyfastx`` module\\n\\n::\\n\\n\\tpip install -U pyfastx\\n\\nFASTX\\n=====\\n\\nNew in ``pyfastx`` 0.8.0.\\n\\nPyfastx provide a simple and fast python binding for kseq.h to iterate over sequences or reads in fasta/q file. The FASTX object will automatically detect the input sequence format (fasta or fastq) to return different tuple.\\n\\nFASTA sequences iteration\\n-------------------------\\n\\nWhen iterating over sequences on FASTX object, a tuple ``(name, seq)`` will be returned.\\n\\n.. code:: python\\n\\n    >>> fa = pyfastx.Fastx(\\'tests/data/test.fa.gz\\')\\n    >>> for name,seq in fa:\\n    >>>     print(name)\\n    >>>     print(seq)\\n\\n    >>> #always output uppercase sequence\\n    >>> for item in pyfastx.Fastx(\\'tests/data/test.fa\\', uppercase=True):\\n    >>>     print(item)\\n\\n    >>> #Manually specify sequence format\\n    >>> for item in pyfastx.Fastx(\\'tests/data/test.fa\\', format=\"fasta\"):\\n    >>>     print(item)\\n\\nIf you want the sequence comment, you can set comment to True, New in ``pyfastx`` 0.9.0.\\n\\n.. code:: python\\n\\n    >>> fa = pyfastx.Fastx(\\'tests/data/test.fa.gz\\', comment=True)\\n    >>> for name,seq,comment in fa:\\n    >>>     print(name)\\n    >>>     print(seq)\\n    >>>     print(comment)\\n\\nThe comment is the content of header line after the first white space or tab character.\\n\\nFASTQ reads iteration\\n---------------------\\n\\nWhen iterating over reads on FASTX object, a tuple ``(name, seq, qual)`` will be returned.\\n\\n.. code:: python\\n\\n    >>> fq = pyfastx.Fastx(\\'tests/data/test.fq.gz\\')\\n    >>> for name,seq,qual in fq:\\n    >>>     print(name)\\n    >>>     print(seq)\\n    >>>     print(qual)\\n\\nIf you want the read comment, you can set comment to True, New in ``pyfastx`` 0.9.0.\\n\\n.. code:: python\\n\\n    >>> fq = pyfastx.Fastx(\\'tests/data/test.fq.gz\\', comment=True)\\n    >>> for name,seq,qual,comment in fq:\\n    >>>     print(name)\\n    >>>     print(seq)\\n    >>>     print(qual)\\n    >>>     print(comment)\\n\\nThe comment is the content of header line after the first white space or tab character.\\n\\nFASTA\\n=====\\n\\nRead FASTA file\\n---------------\\n\\nRead plain or gzipped FASTA file and build index, support for random access to FASTA.\\n\\n.. code:: python\\n\\n    >>> import pyfastx\\n    >>> fa = pyfastx.Fasta(\\'test/data/test.fa.gz\\')\\n    >>> fa\\n    <Fasta> test/data/test.fa.gz contains 211 seqs\\n\\n.. note::\\n    Building index may take some times. The time required to build index depends on the size of FASTA file. If index built, you can randomly access to any sequences in FASTA file. The index file can be reused to save time when you read seqeunces from FASTA file next time.\\n\\nFASTA records iteration\\n-----------------------\\n\\nThe fastest way to iterate plain or gzipped FASTA file without building index, the iteration will return a tuple contains name and sequence.\\n\\n.. code:: python\\n\\n    >>> import pyfastx\\n    >>> for name, seq in pyfastx.Fasta(\\'test/data/test.fa.gz\\', build_index=False):\\n    >>>     print(name, seq)\\n\\nYou can also iterate sequence object from FASTA object like this:\\n\\n.. code:: python\\n\\n    >>> import pyfastx\\n    >>> for seq in pyfastx.Fasta(\\'test/data/test.fa.gz\\'):\\n    >>>     print(seq.name)\\n    >>>     print(seq.seq)\\n    >>>     print(seq.description)\\n\\nIteration with ``build_index=True`` (default) return sequence object which allows you to access attributions of sequence. New in pyfastx 0.6.3.\\n\\n\\nGet FASTA information\\n---------------------\\n\\n.. code:: python\\n\\n    >>> # get sequence counts in FASTA\\n    >>> len(fa)\\n    211\\n\\n    >>> # get total sequence length of FASTA\\n    >>> fa.size\\n    86262\\n\\n    >>> # get GC content of DNA sequence of FASTA\\n    >>> fa.gc_content\\n    43.529014587402344\\n\\n    >>> # get GC skew of DNA sequences in FASTA\\n    >>> # New in pyfastx 0.3.8\\n    >>> fa.gc_skew\\n    0.004287730902433395\\n\\n    >>> # get composition of nucleotides in FASTA\\n    >>> fa.composition\\n    {\\'A\\': 24534, \\'C\\': 18694, \\'G\\': 18855, \\'T\\': 24179}\\n\\n    >>> # get fasta type (DNA, RNA, or protein)\\n    >>> fa.type\\n    \\'DNA\\'\\n\\n    >>> # check fasta file is gzip compressed\\n    >>> fa.is_gzip\\n    True\\n\\nGet longest and shortest sequence\\n---------------------------------\\n\\nNew in ``pyfastx`` 0.3.0\\n\\n.. code:: python\\n\\n    >>> # get longest sequence\\n    >>> s = fa.longest\\n    >>> s\\n    <Sequence> JZ822609.1 with length of 821\\n\\n    >>> s.name\\n    \\'JZ822609.1\\'\\n\\n    >>> len(s)\\n    821\\n\\n    >>> # get shortest sequence\\n    >>> s = fa.shortest\\n    >>> s\\n    <Sequence> JZ822617.1 with length of 118\\n\\n    >>> s.name\\n    \\'JZ822617.1\\'\\n\\n    >>> len(s)\\n    118\\n\\nCalculate N50 and L50\\n---------------------\\n\\nNew in ``pyfastx`` 0.3.0\\n\\nCalculate assembly N50 and L50, return (N50, L50), learn more about `N50,L50 <https://www.molecularecologist.com/2017/03/whats-n50/>`_\\n\\n.. code:: python\\n\\n\\t>>> # get FASTA N50 and L50\\n\\t>>> fa.nl(50)\\n\\t(516, 66)\\n\\n\\t>>> # get FASTA N90 and L90\\n\\t>>> fa.nl(90)\\n\\t(231, 161)\\n\\n\\t>>> # get FASTA N75 and L75\\n\\t>>> fa.nl(75)\\n\\t(365, 117)\\n\\nGet sequence mean and median length\\n-----------------------------------\\n\\nNew in ``pyfastx`` 0.3.0\\n\\n.. code:: python\\n\\n\\t>>> # get sequence average length\\n\\t>>> fa.mean\\n\\t408\\n\\n\\t>>> # get seqeunce median length\\n\\t>>> fa.median\\n\\t430\\n\\nGet sequence counts\\n-------------------\\n\\nNew in ``pyfastx`` 0.3.0\\n\\nGet counts of sequences whose length >= specified length\\n\\n.. code:: python\\n\\n\\t>>> # get counts of sequences with length >= 200 bp\\n\\t>>> fa.count(200)\\n\\t173\\n\\n\\t>>> # get counts of sequences with length >= 500 bp\\n\\t>>> fa.count(500)\\n\\t70\\n\\nGet subsequences\\n----------------\\n\\nSubsequences can be retrieved from FASTA file by using a list of [start, end] coordinates\\n\\n.. code:: python\\n\\n    >>> # get subsequence with start and end position\\n    >>> interval = (1, 10)\\n    >>> fa.fetch(\\'JZ822577.1\\', interval)\\n    \\'CTCTAGAGAT\\'\\n\\n    >>> # get subsequences with a list of start and end position\\n    >>> intervals = [(1, 10), (50, 60)]\\n    >>> fa.fetch(\\'JZ822577.1\\', intervals)\\n    \\'CTCTAGAGATTTTAGTTTGAC\\'\\n\\n    >>> # get subsequences with reverse strand\\n    >>> fa.fetch(\\'JZ822577.1\\', (1, 10), strand=\\'-\\')\\n    \\'ATCTCTAGAG\\'\\n\\nKey function\\n------------\\n\\nNew in ``pyfastx`` 0.5.1\\n\\nSometimes your fasta will have a long header which contains multiple identifiers and description, for example, \">JZ822577.1 contig1 cDNA library of flower petals in tree peony by suppression subtractive hybridization Paeonia suffruticosa cDNA, mRNA sequence\". In this case, both \"JZ822577.1\" and \"contig1\" can be used as identifer. you can specify the key function to select one as identifier.\\n\\n.. code:: python\\n\\n\\t>>> #default use JZ822577.1 as identifier\\n\\t>>> #specify key_func to select contig1 as identifer\\n\\t>>> fa = pyfastx.Fasta(\\'tests/data/test.fa.gz\\', key_func=lambda x: x.split()[1])\\n\\t>>> fa\\n\\t<Fasta> tests/data/test.fa.gz contains 211 seqs\\n\\nSequence\\n========\\n\\nGet a sequence from FASTA\\n-------------------------\\n\\n.. code:: python\\n\\n    >>> # get sequence like a dictionary by identifier\\n    >>> s1 = fa[\\'JZ822577.1\\']\\n    >>> s1\\n    <Sequence> JZ822577.1 with length of 333\\n\\n    >>> # get sequence like a list by index\\n    >>> s2 = fa[2]\\n    >>> s2\\n    <Sequence> JZ822579.1 with length of 176\\n\\n    >>> # get last sequence\\n    >>> s3 = fa[-1]\\n    >>> s3\\n    <Sequence> JZ840318.1 with length of 134\\n\\n    >>> # check a sequence name weather in FASTA file\\n    >>> \\'JZ822577.1\\' in fa\\n    True\\n\\nGet sequence information\\n------------------------\\n\\n.. code:: python\\n\\n    >>> s = fa[-1]\\n    >>> s\\n    <Sequence> JZ840318.1 with length of 134\\n\\n    >>> # get sequence order number in FASTA file\\n    >>> # New in pyfastx 0.3.7\\n    >>> s.id\\n    211\\n\\n    >>> # get sequence name\\n    >>> s.name\\n    \\'JZ840318.1\\'\\n\\n    >>> # get sequence description\\n    >>> # New in pyfastx 0.3.1\\n    >>> s.description\\n    \\'R283 cDNA library of flower petals in tree peony by suppression subtractive hybridization Paeonia suffruticosa cDNA, mRNA sequence\\'\\n\\n    >>> # get sequence string\\n    >>> s.seq\\n    \\'ACTGGAGGTTCTTCTTCCTGTGGAAAGTAACTTGTTTTGCCTTCACCTGCCTGTTCTTCACATCAACCTTGTTCCCACACAAAACAATGGGAATGTTCTCACACACCCTGCAGAGATCACGATGCCATGTTGGT\\'\\n\\n    >>> # get sequence raw string, New in pyfastx 0.6.3\\n    >>> print(s.raw)\\n    >JZ840318.1 R283 cDNA library of flower petals in tree peony by suppression subtractive hybridization Paeonia suffruticosa cDNA, mRNA sequence\\n    ACTGGAGGTTCTTCTTCCTGTGGAAAGTAACTTGTTTTGCCTTCACCTGCCTGTTCTTCACATCAACCTT\\n    GTTCCCACACAAAACAATGGGAATGTTCTCACACACCCTGCAGAGATCACGATGCCATGTTGGT\\n\\n    >>> # get sequence length\\n    >>> len(s)\\n    134\\n\\n    >>> # get GC content if dna sequence\\n    >>> s.gc_content\\n    46.26865768432617\\n\\n    >>> # get nucleotide composition if dna sequence\\n    >>> s.composition\\n    {\\'A\\': 31, \\'C\\': 37, \\'G\\': 25, \\'T\\': 41, \\'N\\': 0}\\n\\nSequence slice\\n--------------\\n\\nSequence object can be sliced like a python string\\n\\n.. code:: python\\n\\n    >>> # get a sub seq from sequence\\n    >>> s = fa[-1]\\n    >>> ss = s[10:30]\\n    >>> ss\\n    <Sequence> JZ840318.1 from 11 to 30\\n\\n    >>> ss.name\\n    \\'JZ840318.1:11-30\\'\\n\\n    >>> ss.seq\\n    \\'CTTCTTCCTGTGGAAAGTAA\\'\\n\\n    >>> ss = s[-10:]\\n    >>> ss\\n    <Sequence> JZ840318.1 from 125 to 134\\n\\n    >>> ss.name\\n    \\'JZ840318.1:125-134\\'\\n\\n    >>> ss.seq\\n    \\'CCATGTTGGT\\'\\n\\n\\n.. note::\\n\\n\\tSlicing start and end coordinates are 0-based. Currently, pyfastx does not support an optional third ``step`` or ``stride`` argument. For example ``ss[::-1]``\\n\\nReverse and complement sequence\\n-------------------------------\\n\\n.. code:: python\\n\\n    >>> # get sliced sequence\\n    >>> fa[0][10:20].seq\\n    \\'GTCAATTTCC\\'\\n\\n    >>> # get reverse of sliced sequence\\n    >>> fa[0][10:20].reverse\\n    \\'CCTTTAACTG\\'\\n\\n    >>> # get complement of sliced sequence\\n    >>> fa[0][10:20].complement\\n    \\'CAGTTAAAGG\\'\\n\\n    >>> # get reversed complement sequence, corresponding to sequence in antisense strand\\n    >>> fa[0][10:20].antisense\\n    \\'GGAAATTGAC\\'\\n\\nRead sequence line by line\\n--------------------------\\n\\nNew in ``pyfastx`` 0.3.0\\n\\nThe sequence object can be iterated line by line as they appear in FASTA file.\\n\\n.. code:: python\\n\\n\\t>>> for line in fa[0]:\\n\\t... \\tprint(line)\\n\\t...\\n\\tCTCTAGAGATTACTTCTTCACATTCCAGATCACTCAGGCTCTTTGTCATTTTAGTTTGACTAGGATATCG\\n\\tAGTATTCAAGCTCATCGCTTTTGGTAATCTTTGCGGTGCATGCCTTTGCATGCTGTATTGCTGCTTCATC\\n\\tATCCCCTTTGACTTGTGTGGCGGTGGCAAGACATCCGAAGAGTTAAGCGATGCTTGTCTAGTCAATTTCC\\n\\tCCATGTACAGAATCATTGTTGTCAATTGGTTGTTTCCTTGATGGTGAAGGGGCTTCAATACATGAGTTCC\\n\\tAAACTAACATTTCTTGACTAACACTTGAGGAAGAAGGACAAGGGTCCCCATGT\\n\\n.. note::\\n\\n    Sliced sequence (e.g. fa[0][10:50]) cannot be read line by line\\n\\nSearch for subsequence\\n----------------------\\n\\nNew in ``pyfastx`` 0.3.6\\n\\nSearch for subsequence from given sequence and get one-based start position of the first occurrence\\n\\n.. code:: python\\n\\n    >>> # search subsequence in sense strand\\n    >>> fa[0].search(\\'GCTTCAATACA\\')\\n    262\\n\\n    >>> # check subsequence weather in sequence\\n    >>> \\'GCTTCAATACA\\' in fa[0]\\n    True\\n\\n    >>> # search subsequence in antisense strand\\n    >>> fa[0].search(\\'CCTCAAGT\\', \\'-\\')\\n    301\\n\\nFastaKeys\\n=========\\n\\nNew in ``pyfastx`` 0.8.0. We have changed ``Identifier`` object to ``FastaKeys`` object.\\n\\nGet keys\\n--------------\\n\\nGet all names of sequence as a list-like object.\\n\\n.. code:: python\\n\\n    >>> ids = fa.keys()\\n    >>> ids\\n    <FastaKeys> contains 211 keys\\n\\n    >>> # get count of sequence\\n    >>> len(ids)\\n    211\\n\\n    >>> # get key by index\\n    >>> ids[0]\\n    \\'JZ822577.1\\'\\n\\n    >>> # check key whether in fasta\\n    >>> \\'JZ822577.1\\' in ids\\n    True\\n\\n    >>> # iterate over keys\\n    >>> for name in ids:\\n    >>>     print(name)\\n\\n    >>> # convert to a list\\n    >>> list(ids)\\n\\nSort keys\\n----------------\\n\\nSort keys by sequence id, name, or length for iteration\\n\\nNew in ``pyfastx`` 0.5.0\\n\\n.. code:: python\\n\\n    >>> # sort keys by length with descending order\\n    >>> for name in ids.sort(by=\\'length\\', reverse=True):\\n    >>>     print(name)\\n\\n    >>> # sort keys by name with ascending order\\n    >>> for name in ids.sort(by=\\'name\\'):\\n    >>>     print(name)\\n\\n    >>> # sort keys by id with descending order\\n    >>> for name in ids.sort(by=\\'id\\', reverse=True)\\n    >>>     print(name)\\n\\nFilter keys\\n------------------\\n\\nFilter keys by sequence length and name\\n\\nNew in ``pyfastx`` 0.5.10\\n\\n.. code:: python\\n\\n    >>> # get keys with length > 600\\n    >>> ids.filter(ids > 600)\\n    <FastaKeys> contains 48 keys\\n\\n    >>> # get keys with length >= 500 and <= 700\\n    >>> ids.filter(ids>=500, ids<=700)\\n    <FastaKeys> contains 48 keys\\n\\n    >>> # get keys with length > 500 and < 600\\n    >>> ids.filter(500<ids<600)\\n    <FastaKeys> contains 22 keys\\n\\n    >>> # get keys contain JZ8226\\n    >>> ids.filter(ids % \\'JZ8226\\')\\n    <FastaKeys> contains 90 keys\\n\\n    >>> # get keys contain JZ8226 with length > 550\\n    >>> ids.filter(ids % \\'JZ8226\\', ids>550)\\n    <FastaKeys> contains 17 keys\\n\\n    >>> # clear sort order and filters\\n    >>> ids.reset()\\n    <FastaKeys> contains 211 keys\\n\\n    >>> # list a filtered result\\n    >>> ids.filter(ids % \\'JZ8226\\', ids>730)\\n    >>> list(ids)\\n    [\\'JZ822609.1\\', \\'JZ822650.1\\', \\'JZ822664.1\\', \\'JZ822699.1\\']\\n\\n    >>> # list a filtered result with sort order\\n    >>> ids.filter(ids % \\'JZ8226\\', ids>730).sort(\\'length\\', reverse=True)\\n    >>> list(ids)\\n    [\\'JZ822609.1\\', \\'JZ822699.1\\', \\'JZ822664.1\\', \\'JZ822650.1\\']\\n\\n    >>> ids.filter(ids % \\'JZ8226\\', ids>730).sort(\\'name\\', reverse=True)\\n    >>> list(ids)\\n    [\\'JZ822699.1\\', \\'JZ822664.1\\', \\'JZ822650.1\\', \\'JZ822609.1\\']\\n\\nFASTQ\\n=====\\n\\nNew in ``pyfastx`` 0.4.0\\n\\nRead FASTQ file\\n---------------\\n\\nRead plain or gzipped file and build index, support for random access to reads from FASTQ.\\n\\n.. code:: python\\n\\n    >>> import pyfastx\\n    >>> fq = pyfastx.Fastq(\\'tests/data/test.fq.gz\\')\\n    >>> fq\\n    <Fastq> tests/data/test.fq.gz contains 100 reads\\n\\nFASTQ records iteration\\n-----------------------\\n\\nThe fastest way to parse plain or gzipped FASTQ file without building index, the iteration will return a tuple contains read name, seq and quality.\\n\\n.. code:: python\\n\\n    >>> import pyfastx\\n    >>> for name,seq,qual in pyfastx.Fastq(\\'tests/data/test.fq.gz\\', build_index=False):\\n    >>>     print(name)\\n    >>>     print(seq)\\n    >>>     print(qual)\\n\\nYou can also iterate read object from FASTQ object like this:\\n\\n.. code:: python\\n\\n    >>> import pyfastx\\n    >>> for read in pyfastx.Fastq(\\'test/data/test.fq.gz\\'):\\n    >>>     print(read.name)\\n    >>>     print(read.seq)\\n    >>>     print(read.qual)\\n    >>>     print(read.quali)\\n\\nIteration with ``build_index=True`` (default) return read object which allows you to access attribution of read. New in pyfastx 0.6.3.\\n\\n\\nGet FASTQ information\\n---------------------\\n\\n.. code:: python\\n\\n    >>> # get read counts in FASTQ\\n    >>> len(fq)\\n    800\\n\\n    >>> # get total bases\\n    >>> fq.size\\n    120000\\n\\n    >>> # get GC content of FASTQ file\\n    >>> fq.gc_content\\n    66.17471313476562\\n\\n    >>> # get composition of bases in FASTQ\\n    >>> fq.composition\\n    {\\'A\\': 20501, \\'C\\': 39705, \\'G\\': 39704, \\'T\\': 20089, \\'N\\': 1}\\n\\n    >>> # New in pyfastx 0.6.10\\n    >>> # get average length of reads\\n    >>> fq.avglen\\n    150.0\\n\\n    >>> # get maximum lenth of reads\\n    >>> fq.maxlen\\n    150\\n\\n    >>> # get minimum length of reas\\n    >>> fq.minlen\\n    150\\n\\n    >>> # get maximum quality score\\n    >>> fq.maxqual\\n    70\\n\\n    >>> # get minimum quality score\\n    >>> fq.minqual\\n    35\\n\\n    >>> # get phred which affects the quality score conversion\\n    >>> fq.phred\\n    33\\n\\n    >>> # Guess fastq quality encoding system\\n    >>> # New in pyfastx 0.4.1\\n    >>> fq.encoding_type\\n    [\\'Sanger Phred+33\\', \\'Illumina 1.8+ Phred+33\\']\\n\\nRead\\n====\\n\\nGet read from FASTQ\\n-------------------\\n\\n.. code:: python\\n\\n    >>> #get read like a dict by read name\\n    >>> r1 = fq[\\'A00129:183:H77K2DMXX:1:1101:4752:1047\\']\\n    >>> r1\\n    <Read> A00129:183:H77K2DMXX:1:1101:4752:1047 with length of 150\\n\\n    >>> # get read like a list by index\\n    >>> r2 = fq[10]\\n    >>> r2\\n    <Read> A00129:183:H77K2DMXX:1:1101:18041:1078 with length of 150\\n\\n    >>> # get the last read\\n    >>> r3 = fq[-1]\\n    >>> r3\\n    <Read> A00129:183:H77K2DMXX:1:1101:31575:4726 with length of 150\\n\\n    >>> # check a read weather in FASTQ file\\n    >>> \\'A00129:183:H77K2DMXX:1:1101:4752:1047\\' in fq\\n    True\\n\\nGet read information\\n--------------------\\n\\n.. code:: python\\n\\n    >>> r = fq[-10]\\n    >>> r\\n    <Read> A00129:183:H77K2DMXX:1:1101:1750:4711 with length of 150\\n\\n    >>> # get read order number in FASTQ file\\n    >>> r.id\\n    791\\n\\n    >>> # get read name\\n    >>> r.name\\n    \\'A00129:183:H77K2DMXX:1:1101:1750:4711\\'\\n\\n    >>> # get read full header line, New in pyfastx 0.6.3\\n    >>> r.description\\n    \\'@A00129:183:H77K2DMXX:1:1101:1750:4711 1:N:0:CAATGGAA+CGAGGCTG\\'\\n\\n    >>> # get read length\\n    >>> len(r)\\n    150\\n\\n    >>> # get read sequence\\n    >>> r.seq\\n    \\'CGAGGAAATCGACGTCACCGATCTGGAAGCCCTGCGCGCCCATCTCAACCAGAAATGGGGTGGCCAGCGCGGCAAGCTGACCCTGCTGCCGTTCCTGGTCCGCGCCATGGTCGTGGCGCTGCGCGACTTCCCGCAGTTGAACGCGCGCTA\\'\\n\\n    >>> # get raw string of read, New in pyfastx 0.6.3\\n    >>> print(r.raw)\\n    @A00129:183:H77K2DMXX:1:1101:1750:4711 1:N:0:CAATGGAA+CGAGGCTG\\n    CGAGGAAATCGACGTCACCGATCTGGAAGCCCTGCGCGCCCATCTCAACCAGAAATGGGGTGGCCAGCGCGGCAAGCTGACCCTGCTGCCGTTCCTGGTCCGCGCCATGGTCGTGGCGCTGCGCGACTTCCCGCAGTTGAACGCGCGCTA\\n    +\\n    FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FF,FFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFF:\\n\\n    >>> # get read quality ascii string\\n    >>> r.qual\\n    \\'FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FF,FFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFF:\\'\\n\\n    >>> # get read quality integer value, ascii - 33 or 64\\n    >>> r.quali\\n    [37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25, 37, 37, 11, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 11, 37, 25, 37, 37, 37, 37, 37, 37, 37, 37, 37, 25]\\n\\n    >>> # get read length\\n    >>> len(r)\\n    150\\n\\nFastqKeys\\n=========\\n\\nNew in ``pyfastx`` 0.8.0.\\n\\nGet fastq keys\\n---------------\\n\\nGet all names of read as a list-like object.\\n\\n.. code:: python\\n\\n    >>> ids = fq.keys()\\n    >>> ids\\n    <FastqKeys> contains 800 keys\\n\\n    >>> # get count of read\\n    >>> len(ids)\\n    800\\n\\n    >>> # get key by index\\n    >>> ids[0]\\n    \\'A00129:183:H77K2DMXX:1:1101:6804:1031\\'\\n\\n    >>> # check key whether in fasta\\n    >>> \\'A00129:183:H77K2DMXX:1:1101:14416:1031\\' in ids\\n    True\\n\\nCommand line interface\\n======================\\n\\nNew in ``pyfastx`` 0.5.0\\n\\n.. code:: bash\\n\\n    $ pyfastx -h\\n\\n    usage: pyfastx COMMAND [OPTIONS]\\n\\n    A command line tool for FASTA/Q file manipulation\\n\\n    optional arguments:\\n      -h, --help     show this help message and exit\\n      -v, --version  show program\\'s version number and exit\\n\\n    Commands:\\n\\n        index        build index for fasta/q file\\n        stat         show detailed statistics information of fasta/q file\\n        split        split fasta/q file into multiple files\\n        fq2fa        convert fastq file to fasta file\\n        subseq       get subsequences from fasta file by region\\n        sample       randomly sample sequences from fasta or fastq file\\n        extract      extract full sequences or reads from fasta/q file\\n\\nBuild index\\n-----------\\n\\nNew in ``pyfastx`` 0.6.10\\n\\n.. code:: bash\\n\\n    $ pyfastx index -h\\n\\n    usage: pyfastx index [-h] [-f] fastx [fastx ...]\\n\\n    positional arguments:\\n      fastx       fasta or fastq file, gzip support\\n\\n    optional arguments:\\n      -h, --help  show this help message and exit\\n      -f, --full  build full index, base composition will be calculated\\n\\nShow statistics information\\n---------------------------\\n\\n.. code:: bash\\n\\n    $ pyfastx stat -h\\n\\n    usage: pyfastx info [-h] fastx\\n\\n    positional arguments:\\n      fastx       input fasta or fastq file, gzip support\\n\\n    optional arguments:\\n      -h, --help  show this help message and exit\\n\\nSplit FASTA/Q file\\n------------------\\n\\n.. code:: bash\\n\\n    $ pyfastx split -h\\n\\n    usage: pyfastx split [-h] (-n int | -c int) [-o str] fastx\\n\\n    positional arguments:\\n      fastx                 fasta or fastq file, gzip support\\n\\n    optional arguments:\\n      -h, --help            show this help message and exit\\n      -n int                split a fasta/q file into N new files with even size\\n      -c int                split a fasta/q file into multiple files containing the same sequence counts\\n      -o str, --out-dir str\\n                            output directory, default is current folder\\n\\nConvert FASTQ to FASTA file\\n---------------------------\\n\\n.. code:: bash\\n\\n    $ pyfastx fq2fa -h\\n\\n    usage: pyfastx fq2fa [-h] [-o str] fastx\\n\\n    positional arguments:\\n      fastx                 fastq file, gzip support\\n\\n    optional arguments:\\n      -h, --help            show this help message and exit\\n      -o str, --out-file str\\n                            output file, default: output to stdout\\n\\nGet subsequence with region\\n---------------------------\\n\\n.. code:: bash\\n\\n    $ pyfastx subseq -h\\n\\n    usage: pyfastx subseq [-h] [-r str | -b str] [-o str] fastx [region [region ...]]\\n\\n    positional arguments:\\n      fastx                 input fasta file, gzip support\\n      region                format is chr:start-end, start and end position is 1-based, multiple names were separated by space\\n\\n    optional arguments:\\n      -h, --help            show this help message and exit\\n      -r str, --region-file str\\n                            tab-delimited file, one region per line, both start and end position are 1-based\\n      -b str, --bed-file str\\n                            tab-delimited BED file, 0-based start position and 1-based end position\\n      -o str, --out-file str\\n                            output file, default: output to stdout\\n\\nSample sequences\\n----------------\\n\\n.. code:: bash\\n\\n    $ pyfastx sample -h\\n\\n    usage: pyfastx sample [-h] (-n int | -p float) [-s int] [--sequential-read] [-o str] fastx\\n\\n    positional arguments:\\n      fastx                 fasta or fastq file, gzip support\\n\\n    optional arguments:\\n      -h, --help            show this help message and exit\\n      -n int                number of sequences to be sampled\\n      -p float              proportion of sequences to be sampled, 0~1\\n      -s int, --seed int    random seed, default is the current system time\\n      --sequential-read     start sequential reading, particularly suitable for sampling large numbers of sequences\\n      -o str, --out-file str\\n                            output file, default: output to stdout\\n\\nExtract sequences\\n-----------------\\n\\nNew in ``pyfastx`` 0.6.10\\n\\n.. code:: bash\\n\\n    $ pyfastx extract -h\\n\\n    usage: pyfastx extract [-h] [-l str] [--reverse-complement] [--out-fasta] [-o str] [--sequential-read]\\n                           fastx [name [name ...]]\\n\\n    positional arguments:\\n      fastx                 fasta or fastq file, gzip support\\n      name                  sequence name or read name, multiple names were separated by space\\n\\n    optional arguments:\\n      -h, --help            show this help message and exit\\n      -l str, --list-file str\\n                            a file containing sequence or read names, one name per line\\n      --reverse-complement  output reverse complement sequence\\n      --out-fasta           output fasta format when extract reads from fastq, default output fastq format\\n      -o str, --out-file str\\n                            output file, default: output to stdout\\n      --sequential-read     start sequential reading, particularly suitable for extracting large numbers of sequences\\n\\nDrawbacks\\n=========\\n\\nIf you intensively check sequence names exists in FASTA file using ``in`` operator on FASTA object like:\\n\\n.. code:: python\\n\\n\\t>>> fa = pyfastx.Fasta(\\'tests/data/test.fa.gz\\')\\n\\t>>> # Suppose seqnames has 100000 names\\n\\t>>> for seqname in seqnames:\\n\\t>>>     if seqname in fa:\\n\\t>>>\\t        do something\\n\\nThis will take a long time to finish. Becuase, pyfastx does not load the index into memory, the ``in`` operating is corresponding to sql query existence from index database. The faster alternative way to do this is:\\n\\n.. code:: python\\n\\n\\t>>> fa = pyfastx.Fasta(\\'tests/data/test.fa.gz\\')\\n\\t>>> # load all sequence names into a set object\\n\\t>>> all_names = set(fa.keys())\\n\\t>>> for seqname in seqnames:\\n\\t>>>     if seqname in all_names:\\n\\t>>>\\t        do something\\n\\nTesting\\n=======\\n\\nThe ``pyfaidx`` module was used to test ``pyfastx``. First, make sure you have a suitable version installed::\\n\\n    pip install pyfastx\\n\\nTo test pyfastx, you should also install pyfaidx 0.5.8::\\n\\n    pip install pyfaidx==0.5.8\\n\\nThen, to run the tests::\\n\\n\\t$ python setup.py test\\n\\nAcknowledgements\\n================\\n\\n`kseq.h <https://github.com/attractivechaos/klib/blob/master/kseq.h>`_ and `zlib <https://www.zlib.net/>`_ was used to parse FASTA format. `Sqlite3 <https://www.sqlite.org/index.html>`_ was used to store built indexes. ``pyfastx`` can randomly access to sequences from gzipped FASTA file mainly attributed to `indexed_gzip <https://github.com/pauldmccarthy/indexed_gzip>`_.\\n', 'downloads': {'last_day': -1, 'last_month': -1, 'last_week': -1}, 'home_page': 'https://github.com/lmdu/pyfastx', 'keywords': 'fasta fastq sequence bioinformatics', 'license': 'MIT', 'name': 'pyfastx', 'package_url': 'https://pypi.org/project/pyfastx/', 'project_url': 'https://pypi.org/project/pyfastx/', 'project_urls': {'Homepage': 'https://github.com/lmdu/pyfastx'}, 'release_url': 'https://pypi.org/project/pyfastx/2.2.0/', 'summary': 'pyfastx is a python module for fast random access to sequences from plain and gzipped FASTA/Q file', 'version': '2.2.0', 'yanked': False, 'releases': ['0.2.0', '0.2.1', '0.2.10', '0.2.11', '0.2.2', '0.2.3', '0.2.4', '0.2.5', '0.2.6', '0.2.7', '0.2.8', '0.2.9', '0.3.0', '0.3.1', '0.3.10', '0.3.3', '0.3.4', '0.3.5', '0.3.6', '0.3.7', '0.3.8', '0.3.9', '0.4.0', '0.4.1', '0.5.0', '0.5.1', '0.5.10', '0.5.2', '0.5.3', '0.5.4', '0.5.5', '0.5.6', '0.5.7', '0.5.8', '0.5.9', '0.6.1', '0.6.10', '0.6.11', '0.6.12', '0.6.13', '0.6.14', '0.6.15', '0.6.16', '0.6.17', '0.6.2', '0.6.3', '0.6.4', '0.6.5', '0.6.6', '0.6.7', '0.6.8', '0.6.9', '0.7.0', '0.8.0', '0.8.1', '0.8.2', '0.8.3', '0.8.4', '0.9.0', '0.9.1', '1.0.0', '1.0.1', '1.1.0', '2.0.0', '2.0.1', '2.0.2', '2.1.0', '2.2.0']}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"prinseq/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/prinseq/cmd/0.20.4\",\n    \"name\": \"prinseq\",\n    \"description\": [\n      \"PRINSEQ can be used to filter, reformat, or trim your genomic and metagenomic sequence data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://prinseq.sourceforge.net/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/prinseq/cmd/0.20.4\",\n    \"name\": \"prinseq\",\n    \"description\": [\n      \"To process quality of sequences.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fprinseq%2Fprinseq%2F0.20.4\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://prinseq.sourceforge.net/:\n```\nContent:\n['PRINSEQ. **PR** eprocessing and **IN** formation of **SEQ** uence data. ### 11 / 2013 **Release of lite version 0.20.4:** Fixed error caused by empty lines at the end of paired-end datasets. Restricted sequences in graph data complexity statistics output to 1000bp to keep the .gd files small for inputs with very long sequences (e.g. whole genomes). ### 03 / 2013 **Release of lite version 0.20.3:** Fixed issue of incorrect duplicate counts when a sequence is both an exact duplicate and reverse complement exact duplicate of another sequence. ### 01 / 2013 **Release of lite version 0.20.2:** Added support for STDOUT output to paired-end processing. **Release of lite version 0.20.1:** Fixed issue with FASTA inputs that caused the program to exit. **Release of web version 0.20.1:** Release of web version files to run the web version on a local machine. ### 12 / 2012 **Release of lite version 0.20:** Fixed depricated use of \\'defined\\' on aggregates. Added options \"trim_left_p\" and \"trim_right_p\" to trim reads by a percentage value in addition to options that trim by number of nucleotides. Added option \"stats_assembly\" to report N50, N90, etc contig size in the standalone version\\'s summary statistics output. Added support for paired-end data (new options \"fasta2\" and \"fastq2\"). **Release of graphs version 0.6:** Added support for paired-end data. ### 11 / 2012 **There was an attack on our servers this week** and we had to switch to the backup server. Data on the backup server is about 2 weeks out of date, which means that data you uploaded is most likely not available. There is not much we can do until Sunday (the servers need to be examined by the IT guys). ### 09 / 2012 **Release of lite version 0.19.5:** Fixed issue of incorrect quality trimming with arguments \"min\" and \"max\" for option -trim_qual_type. **Release of lite version 0.19.4 and graphs version 0.5.1:** Fixed issues related to the use of qw() in loops for Perl version 5.14+ (thanks to Evan Staton for pointing out the issue and providing the link with details: [http://search.cpan.org/~jesse/perl-5.14.0/pod/perldelta.pod#Use_of_qw(...)_as_parentheses](http://search.cpan.org/~jesse/perl-5.14.0/pod/perldelta.pod#Use_of_qw%28...%29_as_parentheses) ). Fixed issue with 5\\'/3\\' duplicate removal that forced option -exact_only (thanks to Stephanie Pierson for reporting the issue). Fixed issue with missing duplicate statistics in graph data output if -derep or -graph_stats was not specified. Suppressed output of PCA module when generating PCA plots. ### 06 / 2012 **Release of lite version 0.19.3:** Added new output file option to keep track of sequence identifier renaming (option -seq_id_mappings). Fixed trim_qual_rule parameter listed twice in the log file. Fixed issue with sequences of length 3bp when calculating DUST scores. Fixed issue with exact_only parameter check. ### 05 / 2012 **Release of lite version 0.19.2:** Increased memory efficiency for graph data calculation on big input files. **Release of lite version 0.19.1:** Fixed rounding issue in sequence complexity calculation. **Release of lite version 0.19 and graphs version 0.5:** Added check for counts of filtered sequence to report when no sequences were filtered. Optimized dinucleotide calculation (~400% faster), sequence complexity calculation (~80% faster) and quality filtering and trimming (~90% faster when both filtering and trimming). Added option (-graph_stats) to select what statistics should be calculated and included in the graph_data file (useful if you e.g. do not need sequence complexity information, which requires a lot of computation). Added binned base quality data to graph data output (as generated in web version up to 0.17.4). Removed annotations from length distribution graph if standard deviation is zero. **Release of lite version 0.18.3:** Fixed phred64 scaling issue for graph data outputs (thanks to Komal Jain for pointing out the issue). ### 04 / 2012 **Release of lite version 0.18.2:** Fixed typo in selection of graph data elements that resulted in missing quality data (since version 0.18). **Release of lite version 0.18.1 and graphs version 0.4.1:** Fixed missing zero count for Ns when generating graphs data file. Fixed duplication count table output for HTML report. **Release of lite version 0.18 and graphs version 0.4:** Added options for web version processing (lite+graphs).Added custom parameter processing (same as already available in web version). Added option to input parameters saved in a file (lite). Fixed issue with output to STDOUT for \"-out_format 4\" option. Added counts by type for filtered sequences to verbose output (lite). Updated layout of HTML report to match web version and to use less colors to reduce printing costs and increase readability (graphs). **Release of web version 0.18:** Fixed issue with spaces and parenthesis in filename when compressing files. Switched from Sencha to JQuery for front-end javascript. New web frontend fully relying on lite and graphs Perl scripts (all further lite and graphs updates will automatically apply to the web version). For feedback on the new front-end, please send an email or use the feedback option (click button on the right). ### 02 / 2012 **Release of lite version 0.17.4:** Fixed issue with MID tag output when using the -graph_data option. ### 01 / 2012 **Release of lite version 0.17.3:** Fixed issue with non-exact duplicate removal that caused incorrect out_bad files (filtered out outputs) introduced in last version. **Release of lite version 0.17.2:** Fixed issue with non-exact duplicate removal when graph data and data processing is performed at the same time. ### 12 / 2011 **Release of lite version 0.17.1 and graphs version 0.3:** Added support for tag sequence check to the HTML output. ### 11 / 2011 **Release of lite version 0.17 and graphs version 0.2:** Added error message if statistics and graph data are generated at the same time. Prevented generation of graphs for missing data that might otherwise generate errors. Prevent the use of -stats outputs when generating graphs data. Added example data for prinseq-graphs. Fixed issue with filenames containing a non-alphanuerical sign after the period sign (thanks to Marmaduke for pointing out the issue). New option -no_qual_header allows to reduce the file size of FASTQ files by preventing any header information output for the quality data. New option -derep_min to specify the duplication threshold (e.g. only filter sequences that occur more than 5 times). ### 09 / 2011 **Release of web version 0.16.1:** Fixed issue with mean and max quality score rule for trimming and changed trim \"until\" to \"while\" (web only, lite version is not affected). **Release of version 0.16:** Check if sequence qualities are in Phred+64 format, if specified. Added the reporting of errors during processing of data. Multiple output formats are now supported (prinseq-lite). Extended the input format from ACGTN to full nucleic acid ambiguity code (ACGTURYKMSWBDHVNX-). Allow processing of amino acid sequences (prinseq-lite). Replace option -si13 with -phred64 to specify input files in Phred+64 format. New options to generate graphs in standalone lite version (using prinseq-graphs.pl or online form). **Release of version 0.15.1:** Fixed problem with dots in directory names (prinseq-lite). Fixed problem with trimming from left of reads that are shorter than the specified trim length. Fixed error in calculation of Phred quality scores for Solexa/Illumina 1.3+ data. ### 06 / 2011 **Release of version 0.15:** New file input by URL (web version). Corrected typo in regex (missing \\\\ before s*) and sequence id hash value (was seqi_d instead of seq_id). Added quality score scaling for Solexa/Illumina 1.3+ data. New option to trim poly-N tails. New option to read from STDIN and write to STDOUT (lite version). Adjusted graph labels for datasets with more than 1 million reads (web version). ### 05 / 2011 **Release of version 0.14.4:** Fixed warnings in tag sequence function. Corrected line break possition in output format for QUAL files. Fixed warnings for quality trimming from the 3\\'-ends (lite version). ### 04 / 2011 **Release of version 0.14.2:** Fixed issue of file format check with non-Unix line breaks causing misidentification of FASTQ files. Fixed warning when trimming and dereplicating. ### 03 / 2011 **Release of version 0.14:** Added status report for writing data after duplicate removal and added number of bases and mean length to output summary statistics in verbose and log mode (lite version). Modified data processing to allow larger files and higher compressed input files that previously caused callback timeout (web version). Fixed warning when out_good or out_bad is set to null and of renaming sequence identifiers when additionally removing read duplicates. ### 02 / 2011 **Release of version 0.13:** Fixed issue with leading spaces in first quality score. Added length check that ensures that the number of bases matches the number of quality scores. (This also ensures that each sequence has quality scores, if a QUAL file is provided as input.) **Release of version 0.12:** Fixed issue when sequences are 3bp or shorter that caused a division by zero and incorrect DUST complexity scores. Added -log option to generate a log file with the used command and basic input/output statistics (lite version). Fixed renaming issue for duplicate removal (lite version). Fixed issue for sequences with a single base and a quality score of 0. ### 01 / 2011 **Release of version 0.11:** Improved tag sequence probability estimation with additional check for MID tags (454 GSMIDs and RLMIDs) and report of MID sequence if found. Visualization for odds ratios to easily identify over- and under-represented dinucleotides (web version only). Added table with minimum and maximum complexity values and the respective sequece to the web version. **Release of version 0.10:** Corrected typos in option description and user interface. Fixed bug when both output options out_good and out_bad are set to \"null\" in standalone version. Added summary statistics calculation for basic infos (stats_info), length (stats_len), dinucleotide odds ratios (stats_dinuc), tag probabilities (stats_tag), sequence duplicates (stats_dupl), ambiguous base N (stats_ns) and all together (stats_all) to the standalone version. See \"Manual\" for details on the new options. ### 12 / 2010 **Release of version 0.9:** Fixed parameter loading for JSON data. Fixed ID type in sequence complexity method. Changed order of tail trimming and quality trimming. Fixed 3\\'-end tail trimming bug. Extended documentation and verbose print output of lite-version. Added option to prevent output generation of certain files to lite-version. Fixed issue of maximum number of sequences in combination with duplicate removal. Fixed missing quality trimming in trimming of sequence to fixed length. Fixed GC content range filtering. Changed integer to float for percentage value filtering. Removed debugging output. Forced single line output for FASTQ format. ### 11 / 2010 **Release of version 0.8:** Use JSON to manage parameters on server and user site. Added mean and standard deviations to length and GC content plot to guide choice of minimum and maximum values. Added example data. ### 10 / 2010 **Release of version 0.7:** Added dinucleotide odds ratio calculation and PCA plots including several viral and microbial metagenomes. Add sequence complexity plots and filters using DUST and entropy methods. Reorganize input counts and input info to merge into single table. Add tables with counts to most plots. Add percentage for sequence numbers. **Release of version 0.6:** Use Cairo graphics library to generate graphics. Added parameter management functionality and pre-defined parameter sets. Separate duplication plot into separate plot and add reverse complement counts. Add two plots to show duplication level and number of duplicate counts. Use box-plots for quality scores. **Release of version 0.5:** Switched to ExtJS for web-interface. Change progress bars and other functionality to JS. Use bi-histograms for duplicate identification in GC content and length distribution plots. Only show graphs when there are values to plot to reduce load on user site. Added sequence quality scores plot and filter functionality. ### 09 / 2010 **PRINSEQ release 0.4:** Removed rarely used information shown in \"Input stats\". Added base frequencies at sequence ends and tag sequence probability for tag sequence check. Added line width formatting option for FASTA (and QUAL) output. Use binning for datasets with long sequences. **PRINSEQ release 0.3** including more information to \"Reformat Options\" field for renaming sequence ids, automatic removal of spaces, \">\", and quotes from sequence ids before renaming. Fixed issue with saving \"0\" values instead of default values into parameters file and \"division by 0\" when calculating 1/length for sequence fractions. PRINSEQ now automatically removes space and dash from sequences when parsing the input data. Added function to convert base U to T for RNA input and DNA output files.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bioconductor-lipidr/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bioconductor-lipidr/lib/2.16.0\",\n    \"name\": \"bioconductor-lipidr\",\n    \"description\": [\n      \"Lipidr an easy-to-use R package implementing a complete workflow for downstream analysis of targeted and untargeted lipidomics data. lipidomics results can be imported into lipidr as a numerical matrix or a Skyline export, allowing integration into current analysis frameworks. Data mining of lipidomics datasets is enabled through integration with Metabolomics Workbench API. lipidr allows data inspection, normalization, univariate and multivariate analysis, displaying informative visualizations. lipidr also implements a novel Lipid Set Enrichment Analysis (LSEA), harnessing molecular information such as lipid class, total chain length and unsaturation.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/3.18/bioc/html/lipidr.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/lipidr/lib/None\",\n    \"name\": \"lipidr\",\n    \"description\": [\n      \"A Software Tool for Data Mining and Analysis of Lipidomics Datasets.\\n\\nData Mining and Analysis of Lipidomics Datasets.\\n\\nlipidr: Data Mining and Analysis of Lipidomics Datasets in R.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://www.lipidr.org/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82df910e2f3aa0b3ba174\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://www.lipidr.org/:\n```\nContent:\n['## Using Docker<#using-docker> You can use lipidr in a containerized form by pulling the image from docker hub. docker pull ahmohamed/lipidr docker run -e PASSWORD=bioc -p 8787:8787 ahmohamed/lipidr:latest In your browser, navigate to RStudio will be available on your web browser at http://localhost:8787 . The USER is fixed to always being rstudio . The password in the above command is given as bioc but it can be set to anything. For more information on how-to-use, refer to [Bioconductor help page](https://www.bioconductor.org/help/docker/) . You can access your local files by mapping to the container: docker run -e PASSWORD=bioc -p 8787:8787 \\\\ -v \"path/to/data_folder\":\"/home/rstudio/data_folder\" \\\\ ahmohamed/lipidr:latest You should see data_folder in your working directory.']```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/3.18/bioc/html/lipidr.html:\n```\nContent:\n['This package is for version 3.18 of Bioconductor; for the stable, up-to-date release version, see [lipidr](/packages/lipidr/) . ## Data Mining and Analysis of Lipidomics Datasets **Bioconductor version:** 3.18 lipidr an easy-to-use R package implementing a complete workflow for downstream analysis of targeted and untargeted lipidomics data. lipidomics results can be imported into lipidr as a numerical matrix or a Skyline export, allowing integration into current analysis frameworks. Data mining of lipidomics datasets is enabled through integration with Metabolomics Workbench API. lipidr allows data inspection, normalization, univariate and multivariate analysis, displaying informative visualizations. lipidr also implements a novel Lipid Set Enrichment Analysis (LSEA), harnessing molecular information such as lipid class, total chain length and unsaturation. **Author:** Ahmed Mohamed [cre] <https://orcid.org/0000-0001-6507-5300> , Ahmed Mohamed [aut], Jeffrey Molendijk [aut] **Maintainer:** Ahmed Mohamed <mohamed at kuicr.kyoto-u.ac.jp> **Citation (from within R, entercitation(\"lipidr\")):** ### Installation To install this package, start R (version \"4.3\") and enter: if (! require ( \"BiocManager\" , quietly = TRUE )) install. packages ( \"BiocManager\" ) BiocManager :: install ( \"lipidr\" ) For older versions of R, please refer to the appropriate [Bioconductor release](/about/release-announcements/) . ### Documentation To view documentation for the version of this package installed in your system, start R and enter: browseVignettes (\"lipidr\") ### Details biocViews [Lipidomics](../../BiocViews.html#___Lipidomics) , [MassSpectrometry](../../BiocViews.html#___MassSpectrometry) , [Normalization](../../BiocViews.html#___Normalization) , [QualityControl](../../BiocViews.html#___QualityControl) , [Software](../../BiocViews.html#___Software) , [Visualization](../../BiocViews.html#___Visualization) Version 2.16.0 In Bioconductor since BioC 3.9 (R-3.6) (5 years) License MIT + file [LICENSE](../licenses/lipidr/LICENSE) Depends R (>= 3.6.0), [SummarizedExperiment](../../bioc/html/SummarizedExperiment.html) Imports methods, stats, utils, data.table, [S4Vectors](../../bioc/html/S4Vectors.html) , rlang, dplyr, tidyr, forcats, ggplot2, [limma](../../bioc/html/limma.html) , [fgsea](../../bioc/html/fgsea.html) , [ropls](../../bioc/html/ropls.html) , imputeLCMD, magrittr System Requirements URL [https://github.com/ahmohamed/lipidr](https://github.com/ahmohamed/lipidr) Bug Reports [https://github.com/ahmohamed/lipidr/issues/](https://github.com/ahmohamed/lipidr/issues/) See More Suggests knitr, rmarkdown, [BiocStyle](../../bioc/html/BiocStyle.html) , ggrepel, plotly, spelling, testthat Linking To Enhances Depends On Me Imports Me Suggests Me [rgoslin](../../bioc/html/rgoslin.html) Links To Me Build Report [Build Report](https://bioconductor.org/checkResults/3.18/bioc-LATEST/lipidr/) ### Package Archives Follow [Installation](/install/#install-bioconductor-packages) instructions to use this package in your R session.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"repet/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/repet/cmd/v3.0\",\n    \"name\": \"repet\",\n    \"description\": [\n      \"The REPET package integrates bioinformatics pipelines dedicated to detect, annotate and analyze transposable elements (TEs) in genomic sequences. The main pipelines are (i) TEdenovo, which search for interspersed repeats, build consensus sequences and classify them according to TE features, and (ii)\\n TEannot, which mines a genome with a library of TE sequences, for instance the one produced by the TEdenovo pipeline, to provide TE annotations exported into GFF3 files.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://urgi.versailles.inrae.fr/Tools/REPET\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Organization\",\n        \"name\": \"URGI\",\n        \"email\": \"urgi-repet@versailles.inrae.fr\",\n        \"maintainer\": false,\n        \"url\": \"http://urgi.versailles.inrae.fr/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82c0510e2f3aa0b3b96db\"\n      },\n      {\n        \"$oid\": \"67c82c0510e2f3aa0b3b96dc\"\n      },\n      {\n        \"$oid\": \"67c796ced3b8acd1a2f9b364\"\n      },\n      {\n        \"$oid\": \"67c82a2a10e2f3aa0b3b8b5a\"\n      },\n      {\n        \"$oid\": \"67c82c0510e2f3aa0b3b96dd\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/repet/cmd/None\",\n    \"name\": \"repet\",\n    \"description\": [\n      \"A software of detection, annotation and analysis of repeats in genomic sequences, specifically designed for transposable elements.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://urgi.versailles.inra.fr/Tools/REPET\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"urgi-repet@versailles.inra.fr\",\n        \"maintainer\": false,\n        \"url\": \"http://urgi.versailles.inra.fr/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82a2a10e2f3aa0b3b8b5a\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://urgi.versailles.inra.fr/Tools/REPET:\n```\nContent:\n['## REPET The REPET package integrates bioinformatics programs in order to tackle biological issues at the genomic scale. It is distributed under and deposited to the Agence de Protection des Programmes ( [APP](http://www.app.asso.fr/) ) under the Inter Deposit Digital Number FR 001 480007 000 R P 2008 000 31 235. To download the last [REPET package v3.0](https://urgi.versailles.inrae.fr/download/repet/REPET_linux-x64-3.0.tar.gz) . What\\'s new in this release [CHANGELOG](/content/download/5981/45487/file/CHANGELOG) (19.10 kB) ? Previous releases are available [here](https://urgi.versailles.inrae.fr/download/repet/old_versions) For a quick install, REPET and its dependencies are containerized in [a free docker image downloadable](https://hub.docker.com/r/urgi/docker_vre_aio) **** **NEW!** A video tutorial for the usage of the REPET docker image is available [here](https://www.youtube.com/watch?v=jDYvbBgX42k&t=3s) **How to cite REPET package:** The REPET package has evolved with improvements over time. The 3 pipelines of the REPET package are TEdenovo (building TE library), PASTEC (consensus classification) and TEannot (TE annotation). Here is the full list of publications : **TEfinder (Blaster, Grouper, Matcher):** Quesneville, H., Nouaud, D. & Anxolab\u00e9h\u00e8re, D. Detection of New Transposable Element Families in Drosophila melanogaster and Anopheles gambiae Genomes . J Mol Evol57 (Suppl 1), S50\u2013S59 (2003). [https://doi.org/10.1007/s00239-003-0007-2](https://doi.org/10.1007/s00239-003-0007-2) **TEdenovo** : Flutre T, Duprat E, Feuillet C, Quesneville H (2011) Considering Transposable Element Diversification in De Novo Annotation Approaches. PLoS ONE 6(1): e16526. [https://doi.org/10.1371/journal.pone.0016526](https://doi.org/10.1371/journal.pone.0016526) **PASTEC (included in TEdenovo):** Hoede C, Arnoux S, Moisset M, Chaumier T, Inizan O, Jamilloux V, et al. (2014) PASTEC: An Automatic Transposable Element Classification Tool. PLoS ONE 9(5): e91929. [https://doi.org/10.1371/journal.pone.0091929](https://doi.org/10.1371/journal.pone.0091929) **TEannot** : Quesneville H, Bergman CM, Andrieu O, Autard D, Nouaud D, Ashburner M, et al. (2005) Combined Evidence Annotation of Transposable Elements in Genome Sequences. PLoS Comput Biol 1(2): e22. [https://doi.org/10.1371/journal.pcbi.0010022](https://doi.org/10.1371/journal.pcbi.0010022) **Long join procedure (included in TEannot):** Ahmed, I., Sarazin, A., Bowler, C., Colot, V., & Quesneville, H. (2011). Genome-wide evidence for local DNA methylation spreading from small RNA-targeted sequences in Arabidopsis. Nucleic acids research, 39(16), 6919-6931. [https://doi.org/10.1093/nar/gkr324](https://doi.org/10.1093/nar/gkr324) A methodological article to be able to tackle the TE annotation of large genomes, reduce the resources necessary for the construction of the TE library, improve the consensus library and have metrics on the quality of the annotations (NTE, LTE) is available here: V. Jamilloux, J. Daron, F. Choulet and H. Quesneville, \"De Novo Annotation of Transposable Elements: Tackling the Fat Genome Issue,\" in Proceedings of the IEEE, vol. 105, no. 3, pp. 474-481, March 2017, doi: 10.1109/JPROC.2016.2590833. [https://ieeexplore.ieee.org/document/7562280](https://ieeexplore.ieee.org/document/7562280) Brief description of REPET Its two main pipelines are dedicated to detecte, annotate and analyse repeats in genomic sequences, specifically designed for transposable elements (TEs). * TEdenovo: this pipeline starts by comparing the genome with itself using BLASTER. Then it clusters matches with GROUPER, RECON and PILER, clustering programs specific for interspersed repeats. For each cluster, it builds a multiple alignment from which a consensus sequence is derived. Finally these consensus are classified according to TE features and redundancy is removed. At the end we obtain a library of classified, non-redundant consensus sequences. * TEannot: this pipeline mines a genome with a library of TE sequences, for instance the one produced by the TEdenovo pipeline, using BLASTER, RepeatMasker and CENSOR. An empirical statistical filter is applied to discard false-positive matches. Short simple repeats (SSRs) are annotated along the way with TRF, RepeatMasker and MREPS. Then the pipeline chains, with MATCHER via dynamic programming, TE fragments belonging to the same, disrupted copy. A \"long join\" procedure is subsequently applied to connect distant fragments. Finally annotations are exported into GFF3 and gameXML files. Repet package also contains [PASTEClassifier](/Tools/PASTEClassifier) ( [Hoede C. et al. 2014](/About-us/Publications/20142/PASTEC-An-Automatic-Transposable-Element-Classification-Tool) ) classifies TEs based on Wicker classification ( [Wicker T. et al. 2007](http://www.ncbi.nlm.nih.gov/pubmed/17984973) ) Prerequisites and installation notes are described in [INSTALL](/Tools/REPET/INSTALL) The use of reference banks is optional but much advised to improve your consensus classification. Currently there is different banks available formatted for a REPET use. Choose your bank according to its content in reference transposable element families on your species to be annotated : There is a file specially formatted for REPET (\"REPET edition\") available with fees. * Here is a link to[REXdb](http://repeatexplorer.org/?page_id=918): The Viridiplantae v3.0 formatted for REPET is available [here](https://urgi.versailles.inrae.fr/download/repet/banks/REXdb/Viridiplantae_v3.0_ALL_protein-domains_repet_formated.fsa) The Dfam3.6 formatted for REPET is available [here](https://urgi.versailles.inrae.fr/download/repet/banks/dfam/) . Optional: if you want to search for protein domain by HMM profiles in TE consensus, you need to have hmmpfam (from package hmmer2) or hmmpress and hmmscan (from package hmmer3) and an appropriate bank of HMM profiles ( [http://hmmer.janelia.org/](http://hmmer.janelia.org/) ). It is advisable to use our last version [ProfilesBankForREPET_Pfam35.0_GypsyDB.hmm](https://urgi.versailles.inrae.fr/download/repet/profiles/ProfilesBankForREPET_Pfam35.0_GypsyDB.hmm.tar.gz) The old bank version is still available here: [ProfilesBankForREPET_Pfam27.0_GypsyDB.hmm.tar.gz bank](/download/repet/profiles/ProfilesBankForREPET_Pfam27.0_GypsyDB.hmm.tar.gz) , specially formatted for REPET. Help and documentation To discover and learn how to use REPET pipelines : read [TEdenovo](/Tools/REPET/TEdenovo-tuto) and [TEannot](/Tools/REPET/TEannot-tuto) tutorials, follow [practical work](https://forgemia.inra.fr/urgi-anagen/wiki-repet/-/wikis/REPET-V3.0-tutorial) . Development The development of REPET follows [eXtreme programming](http://en.wikipedia.org/wiki/Extreme_Programming) guidelines since the release 1.3 (in July 2009). Contact Reporting bugs or asking for features are much welcome! Please contact us via email at urgi-repet[[@]]inra.fr. If you want to receive updates, send an email to urgi-repet[[@]]inra.fr with the following information: * First name * Last name * Email * Institution * Address * City * Zip * Country * Architecture ( linux-x64 ) * Job scheduling system (SGE or Torque) Authors and contributors Hadi Quesneville Olivier Inizan Timoth\u00e9e Flutre Claire Hoede Elodie Duprat Sandie Arnoux Ga\u00ebl Faroux Fran\u00e7oise Alfama Delphine Autard Jonathan Kreplak Timoth\u00e9e Chaumier V\u00e9ronique Jamilloux Marc Bras Mark Moissette Benoit Bely Tina Alaeitabar Anna-Sophie Fiston-Lavier Emmanuelle Permal Erwan Ortie Emeric Henrion Valentin Marcon Eric Penne\u00e7ot Johann Confais Mari\u00e8ne Wan Funding * The \"Modulome\" project\" funded by * The[TransPLANT](/Projects/Achieved-projects/TransPLANT)FP7 european project (EU 7th Framework Programme, contract number 283496) #### Access mode(s): Downloadable #### Keyword(s): detection, annotation, transposable element, pipeline']```"}, {"role": "user", "content": "Content from https://urgi.versailles.inrae.fr/Tools/REPET:\n```\nContent:\n['## REPET The REPET package integrates bioinformatics programs in order to tackle biological issues at the genomic scale. It is distributed under and deposited to the Agence de Protection des Programmes ( [APP](http://www.app.asso.fr/) ) under the Inter Deposit Digital Number FR 001 480007 000 R P 2008 000 31 235. To download the last [REPET package v3.0](https://urgi.versailles.inrae.fr/download/repet/REPET_linux-x64-3.0.tar.gz) . What\\'s new in this release [CHANGELOG](/content/download/5981/45487/file/CHANGELOG) (19.10 kB) ? Previous releases are available [here](https://urgi.versailles.inrae.fr/download/repet/old_versions) For a quick install, REPET and its dependencies are containerized in [a free docker image downloadable](https://hub.docker.com/r/urgi/docker_vre_aio) **** **NEW!** A video tutorial for the usage of the REPET docker image is available [here](https://www.youtube.com/watch?v=jDYvbBgX42k&t=3s) **How to cite REPET package:** The REPET package has evolved with improvements over time. The 3 pipelines of the REPET package are TEdenovo (building TE library), PASTEC (consensus classification) and TEannot (TE annotation). Here is the full list of publications : **TEfinder (Blaster, Grouper, Matcher):** Quesneville, H., Nouaud, D. & Anxolab\u00e9h\u00e8re, D. Detection of New Transposable Element Families in Drosophila melanogaster and Anopheles gambiae Genomes . J Mol Evol57 (Suppl 1), S50\u2013S59 (2003). [https://doi.org/10.1007/s00239-003-0007-2](https://doi.org/10.1007/s00239-003-0007-2) **TEdenovo** : Flutre T, Duprat E, Feuillet C, Quesneville H (2011) Considering Transposable Element Diversification in De Novo Annotation Approaches. PLoS ONE 6(1): e16526. [https://doi.org/10.1371/journal.pone.0016526](https://doi.org/10.1371/journal.pone.0016526) **PASTEC (included in TEdenovo):** Hoede C, Arnoux S, Moisset M, Chaumier T, Inizan O, Jamilloux V, et al. (2014) PASTEC: An Automatic Transposable Element Classification Tool. PLoS ONE 9(5): e91929. [https://doi.org/10.1371/journal.pone.0091929](https://doi.org/10.1371/journal.pone.0091929) **TEannot** : Quesneville H, Bergman CM, Andrieu O, Autard D, Nouaud D, Ashburner M, et al. (2005) Combined Evidence Annotation of Transposable Elements in Genome Sequences. PLoS Comput Biol 1(2): e22. [https://doi.org/10.1371/journal.pcbi.0010022](https://doi.org/10.1371/journal.pcbi.0010022) **Long join procedure (included in TEannot):** Ahmed, I., Sarazin, A., Bowler, C., Colot, V., & Quesneville, H. (2011). Genome-wide evidence for local DNA methylation spreading from small RNA-targeted sequences in Arabidopsis. Nucleic acids research, 39(16), 6919-6931. [https://doi.org/10.1093/nar/gkr324](https://doi.org/10.1093/nar/gkr324) A methodological article to be able to tackle the TE annotation of large genomes, reduce the resources necessary for the construction of the TE library, improve the consensus library and have metrics on the quality of the annotations (NTE, LTE) is available here: V. Jamilloux, J. Daron, F. Choulet and H. Quesneville, \"De Novo Annotation of Transposable Elements: Tackling the Fat Genome Issue,\" in Proceedings of the IEEE, vol. 105, no. 3, pp. 474-481, March 2017, doi: 10.1109/JPROC.2016.2590833. [https://ieeexplore.ieee.org/document/7562280](https://ieeexplore.ieee.org/document/7562280) Brief description of REPET Its two main pipelines are dedicated to detecte, annotate and analyse repeats in genomic sequences, specifically designed for transposable elements (TEs). * TEdenovo: this pipeline starts by comparing the genome with itself using BLASTER. Then it clusters matches with GROUPER, RECON and PILER, clustering programs specific for interspersed repeats. For each cluster, it builds a multiple alignment from which a consensus sequence is derived. Finally these consensus are classified according to TE features and redundancy is removed. At the end we obtain a library of classified, non-redundant consensus sequences. * TEannot: this pipeline mines a genome with a library of TE sequences, for instance the one produced by the TEdenovo pipeline, using BLASTER, RepeatMasker and CENSOR. An empirical statistical filter is applied to discard false-positive matches. Short simple repeats (SSRs) are annotated along the way with TRF, RepeatMasker and MREPS. Then the pipeline chains, with MATCHER via dynamic programming, TE fragments belonging to the same, disrupted copy. A \"long join\" procedure is subsequently applied to connect distant fragments. Finally annotations are exported into GFF3 and gameXML files. Repet package also contains [PASTEClassifier](/Tools/PASTEClassifier) ( [Hoede C. et al. 2014](/About-us/Publications/20142/PASTEC-An-Automatic-Transposable-Element-Classification-Tool) ) classifies TEs based on Wicker classification ( [Wicker T. et al. 2007](http://www.ncbi.nlm.nih.gov/pubmed/17984973) ) Prerequisites and installation notes are described in [INSTALL](/Tools/REPET/INSTALL) The use of reference banks is optional but much advised to improve your consensus classification. Currently there is different banks available formatted for a REPET use. Choose your bank according to its content in reference transposable element families on your species to be annotated : There is a file specially formatted for REPET (\"REPET edition\") available with fees. * Here is a link to[REXdb](http://repeatexplorer.org/?page_id=918): The Viridiplantae v3.0 formatted for REPET is available [here](https://urgi.versailles.inrae.fr/download/repet/banks/REXdb/Viridiplantae_v3.0_ALL_protein-domains_repet_formated.fsa) The Dfam3.6 formatted for REPET is available [here](https://urgi.versailles.inrae.fr/download/repet/banks/dfam/) . Optional: if you want to search for protein domain by HMM profiles in TE consensus, you need to have hmmpfam (from package hmmer2) or hmmpress and hmmscan (from package hmmer3) and an appropriate bank of HMM profiles ( [http://hmmer.janelia.org/](http://hmmer.janelia.org/) ). It is advisable to use our last version [ProfilesBankForREPET_Pfam35.0_GypsyDB.hmm](https://urgi.versailles.inrae.fr/download/repet/profiles/ProfilesBankForREPET_Pfam35.0_GypsyDB.hmm.tar.gz) The old bank version is still available here: [ProfilesBankForREPET_Pfam27.0_GypsyDB.hmm.tar.gz bank](/download/repet/profiles/ProfilesBankForREPET_Pfam27.0_GypsyDB.hmm.tar.gz) , specially formatted for REPET. Help and documentation To discover and learn how to use REPET pipelines : read [TEdenovo](/Tools/REPET/TEdenovo-tuto) and [TEannot](/Tools/REPET/TEannot-tuto) tutorials, follow [practical work](https://forgemia.inra.fr/urgi-anagen/wiki-repet/-/wikis/REPET-V3.0-tutorial) . Development The development of REPET follows [eXtreme programming](http://en.wikipedia.org/wiki/Extreme_Programming) guidelines since the release 1.3 (in July 2009). Contact Reporting bugs or asking for features are much welcome! Please contact us via email at urgi-repet[[@]]inra.fr. If you want to receive updates, send an email to urgi-repet[[@]]inra.fr with the following information: * First name * Last name * Email * Institution * Address * City * Zip * Country * Architecture ( linux-x64 ) * Job scheduling system (SGE or Torque) Authors and contributors Hadi Quesneville Olivier Inizan Timoth\u00e9e Flutre Claire Hoede Elodie Duprat Sandie Arnoux Ga\u00ebl Faroux Fran\u00e7oise Alfama Delphine Autard Jonathan Kreplak Timoth\u00e9e Chaumier V\u00e9ronique Jamilloux Marc Bras Mark Moissette Benoit Bely Tina Alaeitabar Anna-Sophie Fiston-Lavier Emmanuelle Permal Erwan Ortie Emeric Henrion Valentin Marcon Eric Penne\u00e7ot Johann Confais Mari\u00e8ne Wan Funding * The \"Modulome\" project\" funded by * The[TransPLANT](/Projects/Achieved-projects/TransPLANT)FP7 european project (EU 7th Framework Programme, contract number 283496) #### Access mode(s): Downloadable #### Keyword(s): detection, annotation, transposable element, pipeline']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bmge/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bmge/cmd/1.12\",\n    \"name\": \"bmge\",\n    \"description\": [\n      \"BMGE (Block Mapping and Gathering with Entropy) is a program that selects\\nregions in a multiple sequence alignment that are suited for phylogenetic inference.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioweb.pasteur.fr/packages/pack@BMGE@1.12\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-2.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-2.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/bmge/cmd/1.12\",\n    \"name\": \"bmge\",\n    \"description\": [\n      \"Block Mapping and Gathering using Entropy.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://research.pasteur.fr/en/software/bmge-block-mapping-and-gathering-with-entropy/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Simonetta Gribaldo\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"https://research.pasteur.fr/en/member/simonetta-gribaldo/\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"Institut Pasteur\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"https://research.pasteur.fr/\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"C3BI\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"https://research.pasteur.fr/en/center/c3bi/\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"Bioinformatics and Biostatistics Hub\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"https://research.pasteur.fr/en/team/bioinformatics-and-biostatistics-hub/\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Alexis Criscuolo\",\n        \"email\": \"alexis.criscuolo@pasteur.fr\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Alexis Criscuolo\",\n        \"email\": \"alexis.criscuolo@pasteur.fr\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2bcd1b57beba26405ccd7\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://research.pasteur.fr/en/software/bmge-block-mapping-and-gathering-with-entropy/:\n```\nContent:\n['### About <https://research.pasteur.fr/wp-content/uploads/2015/09/research_pasteur-bmge-block-mapping-and-gathering-with-entropy-bmge.png> The quality of multiple sequence alignments plays an important role in the accuracy of phylogenetic inference. It has been shown that removing ambiguously aligned regions, but also other sources of bias such as highly variable (saturated) characters, can improve the overall performance of many phylogenetic reconstruction methods. A current scientific trend is to build phylogenetic trees from a large number of sequence datasets (semi-)automatically extracted from numerous complete genomes. Because these approaches do not allow a precise manual curation of each dataset, there exists a real need for efficient bioinformatic tools dedicated to this alignment character trimming step. BMGE (Block Mapping and Gathering with Entropy) was designed to select regions in a multiple sequence alignment that are suited for phylogenetic inference. For each character, BMGE computes a score closely related to an entropy value. Calculation of these entropy-like scores is weighted with BLOSUM or PAM similarity matrices in order to distinguish among biologically expected and unexpected variability for each aligned character. Sets of contiguous characters with a score above a given threshold are considered as not suited for phylogenetic inference and then removed. Simulation analyses show that the character trimming performed by BMGE produces datasets leading to accurate trees, especially with alignments including distantly-related sequences. BMGE also implements recoding methods aimed at minimizing phylogeny reconstruction artefacts due to compositional heterogeneity.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"rfmix/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/rfmix/cmd/2.03.r0.9505bfa\",\n    \"name\": \"rfmix\",\n    \"description\": [\n      \"RFMix implements a fast discriminative approach to modeling ancestry along an admixed chromosome given observed haplotype sequences of known or inferred ancestry.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/slowkoni/rfmix\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/slowkoni/rfmix\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Free for Academic Use\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a3a1d3b8acd1a2f9b559\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/rfmix/cmd/None\",\n    \"name\": \"rfmix\",\n    \"description\": [\n      \"Apowerful discriminative modeling approach that is faster (~30\u00d7) and more accurate than existing methods. It is capable of learning from the admixed samples themselves to boost performance and autocorrect phasing errors. RFMix shows high sensitivity and specificity in simulated Hispanics/Latinos and African Americans and admixed Europeans, Africans, and Asians.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://sites.google.com/site/rfmixlocalancestryinference/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"bmaples@stanford.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a3a1d3b8acd1a2f9b559\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/slowkoni/rfmix:\n```\nREADME content:\n['## RFMIX version 2 ## LICENSE NOTICE: **This software is available for use free of charge for academic research use only.** Commercial users, for profit companies or consultants, and non-profit institutions not qualifying as \"academic research\" must contact the [Stanford Office of Technology Licensing](http://otl.stanford.edu/) for a separate license to use RFMIX. This applies to this repository directly and any other repository that includes RFMIX source, executables, or git commands that pull/clone this repository as part of its function. Such repositories, whether mine or others, must include this notice. Academic users may fork this repository and modify and improve RFMIX to suit their research needs, but also inherit these terms and must include a licensing notice to that effect. ## Newer methods of Interest High Resolution Ancestry Deconvolution for Next Generation Genomic Data - [G-Nomix](https://github.com/AI-sandbox/gnomix) ## Changes made by Embark The forward-backward result output is a tab separated file with the name \\\\<output basename\\\\>.fb.tsv The first line is a comment line, that specifies the order of subpopulations: eg: ``` #reference_panel_population: golden_retriever labrador_retriever poodle poodle_small ``` The second line specifies the column names, and every following lines gives data on a chunk of the genome, called a conditional random field (CRF) point. The first four columns specify the chromosome, genetic marker\\'s physical position in basepair units and genetic position in centiMorgans, and the genetic marker\\'s numerical index in the rfmix genetic map input file. The remaining columns give the probabilities that the CRF point for a genotype\\'s haplotype was assigned to a specific reference panel population. A genotype has two haplotypes, so the number of probabilities for a genotype is 2*(number of reference panel populations). The number of columns in the file is 4 + (number of genotypes) * 2 * (number of reference panel populations. ## Building RFMIX The quick way: ```sh autoreconf --force --install # creates the configure script and all its dependencies ./configure # generates the Makefile make ``` The long way (in case the quick way gives any trouble): ```sh aclocal # creates aclocal.m4 autoheader # creates config.h.in autoconf # creates configure automake --add-missing # creates Makefile.in ./configure # generates the Makefile make ``` This will build `rfmix` and `simulate`.']```"}, {"role": "user", "content": "Content from https://github.com/slowkoni/rfmix:\n```\nRepository metadata:\n{'name': 'rfmix', 'label': ['rfmix'], 'description': [{'term': 'RFMIX - Local Ancestry and Admixture Inference Version 2', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [], 'repository': [{'term': 'https://github.com/slowkoni/rfmix', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Mark Koni Wright', 'type': 'person', 'email': 'markoni.wright@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'hguturu', 'type': 'person', 'email': 'hguturu@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Matt Barton', 'type': 'person', 'email': 'mattbarton@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Mark Koni Wright', 'type': 'person', 'email': 'mhwright@stanford.edu', 'maintainer': False}, 'id': 3}, {'term': {'name': 'davidriccardi', 'type': 'person', 'email': 'davidriccardi@users.noreply.github.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'David Riccardi', 'type': 'person', 'email': 'driccardi@embarkvet.com', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/slowkoni/rfmix/blob/main/MANUAL.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/slowkoni/rfmix/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"metilene/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/metilene/cmd/0.2.8\",\n    \"name\": \"metilene\",\n    \"description\": [\n      \"Fast and sensitive detection of differential DNA methylation.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.bioinf.uni-leipzig.de/Software/metilene/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-2.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-2.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/metilene/cmd/0.2.6.1\",\n    \"name\": \"metilene\",\n    \"description\": [\n      \"Calling differentially methylated regions from bisulfite sequencing data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Frnateam%2Fmetilene%2Fmetilene%2F0.2.6.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://www.bioinf.uni-leipzig.de/Software/metilene/:\n```\nContent:\n['# metilene **Fast and sensitive detection of differential DNA methylation** Frank J\u00fchling*, Helene Kretzmer*, Stephan H. Bernhart, Christian Otto, Peter F. Stadler & Steve Hoffmann ## Introduction The detection of differentially methylated regions (DMRs) is a necessary prerequisite for characterizing different epigenetic states. We present a novel program, metilene, to identify DMRs within whole-genome and targeted data with unrivaled specificity and sensitivity. A binary segmentation algorithm combined with a two-dimensional statistical test allows the detection of DMRs in large methylation experiments with multiple groups of samples in minutes rather than days using off-the-shelf hardware. metilene outperforms other state-of-the-art tools for low coverage data and can estimate missing data. Hence, metilene is a versatile tool to study the effect of epigenetic modifications in differentiation/development, tumorigenesis, and systems biology on a global, genome-wide level. Whether in the framework of international consortia with dozens of samples per group, or even without biological replicates, it produces highly significant and reliable results. [Link](http://genome.cshlp.org/content/early/2015/12/24/gr.196394.115) to the publication. ## License The software is published under the GNU GPL v2.0 license. From here you can search these documents. Enter your search terms below.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"hyphy/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"github/hyphy/None/2.5.65.actual\",\n    \"name\": \"hyphy\",\n    \"description\": [\n      \"HyPhy: Hypothesis testing using Phylogenies.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/veg/hyphy\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://www.hyphy.org/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"github\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"sourceforge/hyphy/None/None\",\n    \"name\": \"hyphy\",\n    \"description\": [\n      \"HyPhy is a comprehensive environment for maximum likelihood statistical analysis of genetic sequence data. It includes a large collection of out of the box analyses, a feature-rich graphical user interface for custom data analysis and a scripting languag.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://sourceforge.net/projects/hyphy\",\n        \"kind\": \"sourceforge\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://hyphy.sourceforge.io/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"sourceforge\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/hyphy:\n```\nProject metadata:\n{'description': 'HyPhy is a comprehensive environment for maximum likelihood statistical analysis of genetic sequence data. It includes a large collection of out of the box analyses, a feature-rich graphical user interface for custom data analysis and a scripting languag', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['/p/hyphy/activity/feed', 'https://sourceforge.net/auth/?return_to=%2Fprojects%2Fhyphy%2F', '/p/hyphy/activity/']}, {'text': 'Categories\\nBio-Informatics\\n,\\nData Analytics\\nLicense\\nGNU General Public License version 2.0 (GPLv2)\\nCategories\\nBio-Informatics\\n,\\nData Analytics\\nLicense\\nGNU General Public License version 2.0 (GPLv2)', 'hrefs': ['/directory/data-analytics/', '/directory/bio-informatics/']}, {'text': 'Follow HyPhy\\nHyPhy Web Site\\nFollow HyPhy\\nHyPhy Web Site', 'hrefs': ['https://hyphy.sourceforge.io']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof HyPhy!\\nUser Reviews\\nBe the first to\\npost a review\\nof HyPhy!', 'hrefs': ['/projects/hyphy/reviews/new']}, {'text': 'Additional Project Details\\nOperating Systems\\nLinux\\n,\\nBSD\\n,\\nMac\\n,\\nWindows\\nLanguages\\nEnglish\\nIntended Audience\\nScience/Research\\nUser Interface\\nWin32 (MS Windows), Console/Terminal, GTK+, Carbon (Mac OS X)\\nProgramming Language\\nC++\\nDatabase Environment\\nSQL-based\\nRelated Categories\\nC++ Bio-Informatics Software\\n,\\nC++ Data Analytics Tool\\nRegistered\\n2006-06-19\\nAdditional Project Details\\nOperating Systems\\nLinux\\n,\\nBSD\\n,\\nMac\\n,\\nWindows\\nLanguages\\nEnglish\\nIntended Audience\\nScience/Research\\nUser Interface\\nWin32 (MS Windows), Console/Terminal, GTK+, Carbon (Mac OS X)\\nProgramming Language\\nC++\\nDatabase Environment\\nSQL-based\\nRelated Categories\\nC++ Bio-Informatics Software\\n,\\nC++ Data Analytics Tool\\nRegistered\\n2006-06-19', 'hrefs': ['/directory/data-analytics/c++/', '/directory/windows/', '/directory/bio-informatics/c++/', '/directory/bsd/', '/directory/mac/', '/directory/c-plus-plus/', '/directory/linux/']}]}```"}, {"role": "user", "content": "Content from https://hyphy.sourceforge.io/:\n```\nProject metadata:\n{'description': 'HyPhy is a comprehensive environment for maximum likelihood statistical analysis of genetic sequence data. It includes a large collection of out of the box analyses, a feature-rich graphical user interface for custom data analysis and a scripting languag', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['/p/hyphy/activity/feed', 'https://sourceforge.net/auth/?return_to=%2Fprojects%2Fhyphy%2F', '/p/hyphy/activity/']}, {'text': 'Categories\\nBio-Informatics\\n,\\nData Analytics\\nLicense\\nGNU General Public License version 2.0 (GPLv2)\\nCategories\\nBio-Informatics\\n,\\nData Analytics\\nLicense\\nGNU General Public License version 2.0 (GPLv2)', 'hrefs': ['/directory/data-analytics/', '/directory/bio-informatics/']}, {'text': 'Follow HyPhy\\nHyPhy Web Site\\nFollow HyPhy\\nHyPhy Web Site', 'hrefs': ['https://hyphy.sourceforge.io']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof HyPhy!\\nUser Reviews\\nBe the first to\\npost a review\\nof HyPhy!', 'hrefs': ['/projects/hyphy/reviews/new']}, {'text': 'Additional Project Details\\nOperating Systems\\nLinux\\n,\\nBSD\\n,\\nMac\\n,\\nWindows\\nLanguages\\nEnglish\\nIntended Audience\\nScience/Research\\nUser Interface\\nWin32 (MS Windows), Console/Terminal, GTK+, Carbon (Mac OS X)\\nProgramming Language\\nC++\\nDatabase Environment\\nSQL-based\\nRelated Categories\\nC++ Bio-Informatics Software\\n,\\nC++ Data Analytics Tool\\nRegistered\\n2006-06-19\\nAdditional Project Details\\nOperating Systems\\nLinux\\n,\\nBSD\\n,\\nMac\\n,\\nWindows\\nLanguages\\nEnglish\\nIntended Audience\\nScience/Research\\nUser Interface\\nWin32 (MS Windows), Console/Terminal, GTK+, Carbon (Mac OS X)\\nProgramming Language\\nC++\\nDatabase Environment\\nSQL-based\\nRelated Categories\\nC++ Bio-Informatics Software\\n,\\nC++ Data Analytics Tool\\nRegistered\\n2006-06-19', 'hrefs': ['/directory/data-analytics/c++/', '/directory/windows/', '/directory/bio-informatics/c++/', '/directory/bsd/', '/directory/mac/', '/directory/c-plus-plus/', '/directory/linux/']}]}```"}, {"role": "user", "content": "Content from https://github.com/veg/hyphy:\n```\nREADME content:\n['[![Build Status](https://travis-ci.org/veg/hyphy.svg?branch=master)](https://travis-ci.org/veg/hyphy) # HyPhy - Hypothesis testing using Phylogenies HyPhy is an open-source software package for the analysis of genetic sequences using techniques in phylogenetics, molecular evolution, and machine learning. It features a complete graphical user interface (GUI) and a rich scripting language for limitless customization of analyses. Additionally, HyPhy features support for parallel computing environments (via message passing interface (MPI)) and it can be compiled as a shared library and called from other programming environments such as Python and R. HyPhy is the computational backbone powering datamonkey.org. Additional information is available at hyphy.org. ## Quick Start #### Install `conda install -c bioconda hyphy` #### Running with Docker You can also run HyPhy without having to install it on your system using the provided Dockerfile. Following the below instructions starts an interactive Docker container where HyPhy is already available. If you don\\'t have Docker installed, first look at [Docker Desktop](https://docs.docker.com/desktop/). **Please note you must change this code snippet to point to the appropriate location for your input data. This will be made available inside the container at `/hyphy/data`.** ``` git clone https://github.com/veg/hyphy.git cd hyphy docker build -t hyphy:latest . docker run --rm -v [path-to-your-input-data]:/hyphy/data -it hyphy:latest ``` #### Run with Command Line Arguments `hyphy <method_name> --alignment <path_to_alignment_file> <additional_method_specific_arguments>` + _`<method_name>` is the name of the analysis you wish to run (can be: absrel, bgm, busted, fade, fel, fubar, gard, meme, relax or slac)_ + _`<path_to_alignment_file>` is the relative or absolute path to a fasta, nexus or phylib file containing an alignment and tree_ + _A list of the available `<additional_method_specific_arguments>` can be seen by running `hyphy <method_name> --help`_ or #### Run in Interactive Mode `hyphy -i` ## Building from Source #### Requirements * cmake >= 3.12 * gcc >= 4.9 * libcurl * libpthread * openmp (can be installed on mac via `brew install libomp`) #### Download You can download a specific release [here](https://github.com/veg/hyphy/releases) or clone this repo with `git clone https://github.com/veg/hyphy.git` Change your directory to the downloaded/cloned directory `cd hyphy` #### Build `cmake .` `make install` ## Additional Options for Building from Source #### Build Systems If you prefer to use other build systems, such as Xcode, configure using the -G switch `cmake -G Xcode .` CMake supports a number of build system generators, feel free to peruse these and use them if you wish. If you are on an OS X platform, you can specify which OS X SDK to use `cmake -DCMAKE_OSX_SYSROOT=/Developer/SDKs/MacOSX10.9.sdk/ .` If building on a heterogeneous cluster with some nodes that do not support auto-vectorization `cmake -DNOAVX=ON .`. If you\\'re on a UNIX-compatible system, and you\\'re comfortable with GNU make, then run `make` with one of the following build targets: + MP or hyphy - build a HyPhy executable (This used to be \"HYPHYMP\" but is now just \"hyphy\") using pthreads to do multiprocessing + MPI or HYPHYMPI - build a HyPhy executable (HYPHYMPI) using openMPI to do multiprocessing + LIB - build a HyPhy library (libhyphy_mp) using pthreads to do multiprocessing - GTEST - build HyPhy\\'s gtest testing executable (HYPHYGTEST) #### Example (MPI build of hyphy using openMPI) Ensure that you have openmpi installed and available on your path. You can check if this is the case after running `cmake .` you should see something similar to this in your output `-- Found MPI_C: /opt/scyld/openmpi/1.6.3/gnu/lib/libmpi.so;/usr/lib64/libibverbs.so;/usr/lib64/libdat.so;/usr/lib64/librt.so;/usr/lib64/libnsl.so;/usr/lib64/libutil.so;/usr/lib64/libm.so;/usr/lib64/libtorque.so;/usr/lib64/libm.so;/usr/lib64/libnuma.so;/usr/lib64/librt.so;/usr/lib64/libnsl.so;/usr/lib64/libutil.so;/usr/lib64/libm.so ` `-- Found MPI_CXX: /opt/scyld/openmpi/1.6.3/gnu/lib/libmpi_cxx.so;/opt/scyld/openmpi/1.6.3/gnu/lib/libmpi.so;/usr/lib64/libibverbs.so;/usr/lib64/libdat.so;/usr/lib64/librt.so;/usr/lib64/libnsl.so;/usr/lib64/libutil.so;/usr/lib64/libm.so;/usr/lib64/libtorque.so;/usr/lib64/libm.so;/usr/lib64/libnuma.so;/usr/lib64/librt.so;/usr/lib64/libnsl.so;/usr/lib64/libutil.so;/usr/lib64/libm.so ` Then run `make HYPHYMPI` And then run make install to install the software `make install` + hyphy will be installed at `/location/of/choice/bin` + libhyphy_mp.(so/dylib/dll) will be installed at `/location/of/choice/lib` + HyPhy\\'s standard library of batchfiles will go into `/location/of/choice/lib/hyphy` #### Building HyPhy for Web Assembly Use [Emscripten](https://emscripten.org/docs/compiling/Building-Projects.html) to produce web assembly files, which can be run entirely within a modern browser. See https://observablehq.com/@spond/hyphy-biowasm for an example. ``` emcmake cmake -DCMAKE_EXE_LINKER_FLAGS=\"-sTOTAL_STACK=2097152 -02 -sASSERTIONS=1 -sMODULARIZE=1 -sALLOW_MEMORY_GROWTH -sFORCE_FILESYSTEM=1 -sEXIT_RUNTIME=0 -s EXPORTED_RUNTIME_METHODS=[\"callMain\",\"FS\",\"PROXYFS\",\"WORKERFS\",\"UTF8ToString\",\"getValue\",\"AsciiToString\"] -lworkerfs.js -lproxyfs.js -s INVOKE_RUN=0 -s ENVIRONMENT=\"web,worker\" ${EM_FLAGS//-s /-s} -fwasm-exceptions --preload-file res@/hyphy --preload-file tests/hbltests@/tests\" ``` ``` emmake make -j hyphy ``` This creates ``` hyphy.js hyphy.wasm hyphy.data ``` Which should be served from the same directory. #### Testing Use `make test` after running `cmake .`. #### Benchmarks for CMake Tests Benchmarks, using Github Actions, can be found at http://hyphy.org/bench #### Executable Location By default, HyPhy installs into `/usr/local` but it can be installed on any location of your system by providing an installation prefix `cmake -DCMAKE_INSTALL_PREFIX:PATH=/location/of/choice` For example, this configuration will install hyphy at /opt/hyphy `mkdir -p /opt/hyphy` `cmake -DCMAKE_INSTALL_PREFIX:PATH=/opt/hyphy .` #### Building Documentation ``` make docs cd docs python3 -m http.server ``` ## CLI notes As noted in the documentation [here](http://www.hyphy.org/tutorials/CLI-tutorial/) `hyphy` can be run as a command line tool. Indeed for many analyses the `hyphy` CLI will return useful help messages, showing which parameter values can be set to specify your analysis. For example, running `hyphy gard --help` ```{bash} hyphy gard --help Available analysis command line options --------------------------------------- Use --option VALUE syntax to invoke If a [reqired] option is not provided on the command line, the analysis will prompt for its value [conditionally required] options may or not be required based on the values of other options type The type of data to perform screening on default value: nucleotide code Genetic code to use (for codon alignments) default value: Universal applies to: Choose Genetic Code alignment [required] Sequence alignment to screen for recombination model The substitution model to use default value: JTT rv Site to site rate variation default value: None max-breakpoints Maximum number of breakpoints to consider default value: 10000 rate-classes How many site rate classes to use default value: 4 output Write the resulting JSON to this file (default is to save to the same path as the alignment file + \\'GARD.json\\') default value: gard.defaultJsonFilePath [computed at run time] mode Run mode (Normal or Faster) default value: Normal output-lf Write the best fitting HyPhy analysis snapshot to (default is to save to the same path as the alignment file + \\'best-gard\\') default value: gard.defaultFitFilePath [computed at run time] ``` will show you the options that can be set for the `gard` analysis. So for instance one could specify a `gard` run on the command line with the following command ```{bash} hyphy gard --alignment /path/to/file --rv GDD --mode Faster --rate-classes 3 ``` While this is a useful feature, it is not always the case that older analyses will have the same level of support for command line. For instance, the `acd` analysis does not have CLI support and so if one runs the help command ```{bash} hyphy acd --help Available analysis command line options --------------------------------------- Use --option VALUE syntax to invoke If a [reqired] option is not provided on the command line, the analysis will prompt for its value [conditionally required] options may or not be required based on the values of other options No annotated keyword arguments are available for this analysis ``` one will see that there are no options available. In this case, you can use a different CLI specification. Indeed the CLI will accept all of the options that are asked for in an interactive session, as positional arguments. In this case I could run the `acd` analysis with ```{bash} hyphy acd Universal <alignment.fa> MG94CUSTOMCF3X4 Global 012345 <treefile> Estimate ``` where the options are specified in the exact order that they are asked for in the interactive session. This will work for all `hyphy` analyses and provides a less readable but more flexible way to run `hyphy` analyses.']```"}, {"role": "user", "content": "Content from https://github.com/veg/hyphy:\n```\nRepository metadata:\n{'name': 'hyphy', 'label': ['hyphy'], 'description': [{'term': 'HyPhy: Hypothesis testing using Phylogenies', 'id': 0}], 'links': [], 'webpage': [{'term': 'http://www.hyphy.org', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['2.5.71', '2.5.70', '2.5.69', '2.5.68', '2.5.67', '2.5.66', '2.5.65', '2.5.64', '2.5.63', '2.5.62', '2.5.61', '2.5.60', '2.5.59', '2.5.58', '2.5.57', '2.5.56', '2.5.55', '2.5.53', '2.5.52', '2.5.51hf', '2.5.50', '2.5.49', '2.5.48', '2.5.47', '2.5.46hf1', '2.5.45hf1', '2.5.43rc', '2.5.42', '2.5.41', '2.5.40', '2.5.39hf1', '2.5.38', '2.5.36', '2.5.33', '2.5.32', '2.5.31', '2.5.30', '2.5.29', '2.5.28', '2.5.27', '2.5.26', '2.5.25', '2.5.24', '2.5.23', '2.5.22hf', '2.5.21hf', '2.5.20', '2.5.19', '2.5.18.hf1', '2.5.17', '2.5.16', '2.5.15', '2.5.14', '2.5.12', '2.5.11', '2.5.10', '2.5.9', '2.5.8', '2.5.7', '2.5.6', '2.5.5', '2.5.4', '2.5.3', '2.5.2', '2.5.1', '2.5.0', '2.4.0-alpha.2', '2.4.0-alpha.1', '2.3.14', '2.3.13', '2.3.12', '2.3.11', '2.3.10', '2.3.9', '2.3.8', '2.3.7', '2.3.6', '2.3.5', '2.3.4', '2.3.3', '2.3.2', '2.3.1', '2.3.0', '2.2.7', '2.3.0-alpha', '2.2.6', '2.2.5', '2.2.4', '2.2.3', '2.2.2', '2.2.1', '2.2.0'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/veg/hyphy', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/science', 'term': 'science', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/phylogenetics', 'term': 'phylogenetics', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/c-plus-plus', 'term': 'c-plus-plus', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/comparative-genomics', 'term': 'comparative-genomics', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/evolution', 'term': 'evolution', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/statistical-methods', 'term': 'statistical-methods', 'vocabulary': ''}, 'id': 6}], 'operations': [], 'authors': [{'term': {'name': 'Sergei Pond', 'type': 'person', 'email': 'spond@temple.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Danielle Callan', 'type': 'person', 'email': 'dmleake92@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Cornelius Roemer', 'type': 'person', 'email': 'cornelius.roemer@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Steven Weaver', 'type': 'person', 'email': 'sweaver@temple.edu', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/veg/hyphy/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'contributing', 'url': 'https://github.com/veg/hyphy/blob/main/CONTRIBUTING.md'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/veg/hyphy/blob/main/README.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://www.hyphy.org/:\n```\nContent:\n['# Hypothesis Testing using Phylogenies ## An open-source software package for comparative sequence analysis using stochastic evolutionary models [Install](installation/)']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"raresim/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconductor/raresim/lib/1.11.0\",\n    \"name\": \"raresim\",\n    \"description\": [\n      \"Haplotype simulations of rare variant genetic data that emulates real data can be performed with RAREsim. RAREsim uses the expected number of variants in MAC bins - either as provided by default parameters or estimated from target data - and an abundance of rare variants as simulated HAPGEN2 to probabilistically prune variants. RAREsim produces haplotypes that emulate real sequencing data with respect to the total number of variants, allele frequency spectrum, haplotype structure, and variant annotation.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://git.bioconductor.org/packages/RAREsim\",\n        \"kind\": \"bioconductor\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      },\n      {\n        \"url\": \"https://github.com/meganmichelle/RAREsim\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/meganmichelle/RAREsim\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Megan Null\",\n        \"email\": \"mnull@collegeofidaho.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Ryan Barnard\",\n        \"email\": \"rbarnard1107@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"bioconductor\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/raresim/lib/None\",\n    \"name\": \"raresim\",\n    \"description\": [\n      \"A simulation method for very rare genetic variants.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://www.ncbi.nlm.nih.gov/pubmed/?term=35298919\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Megan Null\",\n        \"email\": \"mnull@collegeofidaho.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Jos\u00e9e Dupuis\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Pezhman Sheinidashtegol\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Audrey E. Hendricks\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8355510e2f3aa0b3bc285\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://www.ncbi.nlm.nih.gov/pubmed/?term=35298919:\n```\nContent:\n[\"Full text links Cite Display options Display options Format Abstract PubMed PMID ## Abstract Identification of rare-variant associations is crucial to full characterization of the genetic architecture of complex traits and diseases. Essential in this process is the evaluation of novel methods in simulated data that mirror the distribution of rare variants and haplotype structure in real data. Additionally, importing real-variant annotation enables in silico comparison of methods, such as rare-variant association tests and polygenic scoring methods, that focus on putative causal variants. Existing simulation methods are either unable to employ real-variant annotation or severely under- or overestimate the number of singletons and doubletons, thereby reducing the ability to generalize simulation results to real studies. We present RAREsim, a flexible and accurate rare-variant simulation algorithm. Using parameters and haplotypes derived from real sequencing data, RAREsim efficiently simulates the expected variant distribution and enables real-variant annotations. We highlight RAREsim's utility across various genetic regions, sample sizes, ancestries, and variant classes. **Keywords:** RAREsim; rare variants; simulated data; simulated genetic variants. Copyright \u00a9 2022 American Society of Human Genetics. Published by Elsevier Inc. All rights reserved. [PubMed Disclaimer](/disclaimer/) ## Conflict of interest statement Declaration of interests C.R.G. owns stock in 23andMe. ## Figures <https://cdn.ncbi.nlm.nih.gov/pmc/blobs/04be/9069075/78817b4c30a6/gr1.jpg> **Figure 1** Flowchart of RAREsim Flowchart describing\u2026 **Figure 1** Flowchart of RAREsim Flowchart describing the RAREsim simulation process. Simulation parameters can be\u2026 **Figure 1** Flowchart of RAREsim Flowchart describing the RAREsim simulation process. Simulation parameters can be estimated from target data, default parameters, or user-defined parameters (gray). \u2217 Haplotypes must include information at all sequencing bases to allow for an abundance of variants for subsequent pruning. <https://cdn.ncbi.nlm.nih.gov/pmc/blobs/04be/9069075/a313f4b19b34/gr2.jpg> **Figure 2** Evaluation of function fit (A)\u2026 **Figure 2** Evaluation of function fit (A) \u201cNumber of variants\u201d function: fitted \u201cnumber of variants\u201d\u2026 **Figure 2** Evaluation of function fit (A) \u201cNumber of variants\u201d function: fitted \u201cnumber of variants\u201d functions for all cM blocks. The median block is shown in black, and the 5 th and 95 th blocks are shown in dark blue. The observed target data ( \u2217 ) for the median block are close to the fitted function for all four ancestries. Sample sizes up to 15,000 are shown here. The full sample size for the non-Finnish European sample is in Figure S3. (B) AFS function: difference between the gnomAD target data and estimates from the AFS function for the proportion of variants in each MAC bin for all chromosome 19 blocks by ancestry and MAC bin. All absolute differences are within 0.05, indicating the AFS function fits the target data well. <https://cdn.ncbi.nlm.nih.gov/pmc/blobs/04be/9069075/d2fe60b16465/gr3.jpg> **Figure 3** Evaluation of RAREsim The distribution\u2026 **Figure 3** Evaluation of RAREsim The distribution and number of variants simulated via RAREsim (pink),\u2026 **Figure 3** Evaluation of RAREsim The distribution and number of variants simulated via RAREsim (pink), HAPGEN2 with only polymorphic SNVs (default, green), and HAPGEN2 with all sequencing bases (yellow) are compared to those from gnomAD (blue) for the cM block with the median number of base pairs. Ancestry-specific simulations are shown for African (n = 8,128; left) and non-Finnish European (n = 56,885; right), where the sample size observed in gnomAD v2.1 was matched. RAREsim emulates the expected number of variants within each MAC bin, whereas the other simulation methods either grossly underestimate (HAPGEN2 with polymorphic SNVs) or overestimate (HAPGEN2 with all sequencing bp) the number of variants. <https://cdn.ncbi.nlm.nih.gov/pmc/blobs/04be/9069075/7ccb26a7eaa3/gr4.jpg> **Figure 4** Generalizability of default parameters Utility\u2026 **Figure 4** Generalizability of default parameters Utility of RAREsim\u2019s ancestry-specific default parameters for different chromosomes\u2026 **Figure 4** Generalizability of default parameters Utility of RAREsim\u2019s ancestry-specific default parameters for different chromosomes (A), sample sizes (B and C), intergenic regions (B), and other target datasets (B and C). RAREsim simulations closely approximate the observed number of variants (y axis) in each MAC bin (x axis) in all scenarios. (A) Simulations using South Asian default parameters for chromosome 6 GENCODE region. (B) Simulation of a sample size of 21,042 with African-ancestry default parameters (derived from n = 8,128) for an intergenic region from gnomAD v3. (C) Simulation of a sample size of 41,246 to match a British sample from the UK Biobank under non-Finnish European default parameters (derived from n = 56,885). ## Similar articles * [A Comparison Study of Fixed and Mixed Effect Models for Gene Level Association Studies of Complex Traits.](/27374056/)Fan R, Chiu CY, Jung J, Weeks DE, Wilson AF, Bailey-Wilson JE, Amos CI, Chen Z, Mills JL, Xiong M.Fan R, et al.Genet Epidemiol. 2016 Dec;40(8):702-721. doi: 10.1002/gepi.21984. Epub 2016 Jul 4.Genet Epidemiol. 2016.PMID:27374056Free PMC article. * [A haplotype-based framework for group-wise transmission/disequilibrium tests for rare variant association analysis.](/25568282/)Chen R, Wei Q, Zhan X, Zhong X, Sutcliffe JS, Cox NJ, Cook EH, Li C, Chen W, Li B.Chen R, et al.Bioinformatics. 2015 May 1;31(9):1452-9. doi: 10.1093/bioinformatics/btu860. Epub 2015 Jan 6.Bioinformatics. 2015.PMID:25568282Free PMC article. * [Simulation of Finnish population history, guided by empirical genetic data, to assess power of rare-variant tests in Finland.](/24768551/)Wang SR, Agarwala V, Flannick J, Chiang CW, Altshuler D; GoT2D Consortium; Hirschhorn JN.Wang SR, et al.Am J Hum Genet. 2014 May 1;94(5):710-20. doi: 10.1016/j.ajhg.2014.03.019. Epub 2014 Apr 24.Am J Hum Genet. 2014.PMID:24768551Free PMC article. * [Identifying rare variants associated with complex traits via sequencing.](/23853079/)Li B, Liu DJ, Leal SM.Li B, et al.Curr Protoc Hum Genet. 2013 Jul;Chapter 1:Unit 1.26. doi: 10.1002/0471142905.hg0126s78.Curr Protoc Hum Genet. 2013.PMID:23853079Free PMC article.Review. * [In search of low-frequency and rare variants affecting complex traits.](/23922232/)Panoutsopoulou K, Tachmazidou I, Zeggini E.Panoutsopoulou K, et al.Hum Mol Genet. 2013 Oct 15;22(R1):R16-21. doi: 10.1093/hmg/ddt376. Epub 2013 Aug 6.Hum Mol Genet. 2013.PMID:23922232Free PMC article.Review. ## Cited by * [Haplotype function score improves biological interpretation and cross-ancestry polygenic prediction of human complex traits.](/38639992/)Song W, Shi Y, Lin GN.Song W, et al.Elife. 2024 Apr 19;12:RP92574. doi: 10.7554/eLife.92574.Elife. 2024.PMID:38639992Free PMC article. ## References * Povysil G., Petrovski S., Hostyk J., Aggarwal V., Allen A.S., Goldstein D.B. Rare-variant collapsing analyses for complex traits: guidelines and applications. Nat. Rev. Genet. 2019;20:747\u2013759. doi: 10.1038/s41576-019-0177-4. -[DOI](https://doi.org/10.1038/s41576-019-0177-4)-[PubMed](/31605095/) * Karczewski K.J., Francioli L.C., Tiao G., Cummings B.B., Alf\u00f6ldi J., Wang Q., Collins R.L., Laricchia K.M., Ganna A., Birnbaum D.P., et al. Genome Aggregation Database Consortium The mutational constraint spectrum quantified from variation in 141,456 humans. Nature. 2020;581:434\u2013443. doi: 10.1038/s41586-020-2308-7. -[DOI](https://doi.org/10.1038/s41586-020-2308-7)-[PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC7334197/)-[PubMed](/32461654/) * Auton A., Brooks L.D., Durbin R.M., Garrison E.P., Kang H.M., Korbel J.O., Marchini J.L., McCarthy S., McVean G.A., Abecasis G.R., 1000 Genomes Project Consortium A global reference for human genetic variation. Nature. 2015;526:68\u201374. doi: 10.1038/nature15393. -[DOI](https://doi.org/10.1038/nature15393)-[PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC4750478/)-[PubMed](/26432245/) * Walter K., Min J.L., Huang J., Crooks L., Memari Y., McCarthy S., Perry J.R., Xu C., Futema M., Lawson D., et al. UK10K Consortium The UK10K project identifies rare variants in health and disease. Nature. 2015;526:82\u201390. doi: 10.1038/nature14962. -[DOI](https://doi.org/10.1038/nature14962)-[PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC4773891/)-[PubMed](/26367797/) * Barbitoff Y.A., Skitchenko R.K., Poleshchuk O.I., Shikov A.E., Serebryakova E.A., Nasykhova Y.A., Polev D.E., Shuvalova A.R., Shcherbakova I.V., Fedyakov M.A., et al. Whole-exome sequencing provides insights into monogenic disease prevalence in Northwest Russia. Mol. Genet. Genomic Med. 2019;7:e964. doi: 10.1002/mgg3.964. -[DOI](https://doi.org/10.1002/mgg3.964)-[PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC6825859/)-[PubMed](/31482689/) ## Publication types * Research Support, N.I.H., Extramural * Research Support, Non-U.S. Gov't ## MeSH terms * Genetic Variation* / geneticsMultifactorial Inheritance [x] Cite Copy Format: AMA APA MLA NLM\"]```"}, {"role": "user", "content": "Content from https://git.bioconductor.org/packages/RAREsim:\n```\nRepository metadata:\n{'url': 'https://git.bioconductor.org/packages/RAREsim'}```"}, {"role": "user", "content": "Content from https://github.com/meganmichelle/RAREsim:\n```\nREADME content:\n[\"# RAREsim The functions within the RAREsim R package are used to estimate the expected number of rare variants per minor allele count (MAC) bin via the *Number of Variants* and *AFS* functions. Users have the flexibility to estimate the number of variants with default parameters, user-defined parameters, or parameters estimated to match user-provided target data. The default parameters are provided for four ancestries and are expected to perform well for sample sizes up to 125,000. The RAREsim R package is used to calculate the estimated number of variants per MAC bin. Throught the _Number of Variants_ and _AFS_ functions. The output of the RAREsim functions can be directly input into the pruning package for RAREsim, implemented within python: https://github.com/ryanlayer/raresim. The pruning process requires the expected number of variants per rare MAC bin and over-simulated haplotypes created from simulating from HAPGEN2 with information at all sequencing bases - including monomorphic bases. A complete example with all steps can be found on the Example page: https://github.com/meganmichelle/RAREsim_Example. # Installation The RAREsim R package can be downloaded using devtools. ```{r} library(devtools) install_github('meganmichelle/RAREsim') ``` RAREsim also requires [HAPGEN2](https://mathgen.stats.ox.ac.uk/genetics_software/hapgen/hapgen2.html). The pruning process is done with the [RAREsim python package](https://github.com/ryanlayer/raresim). # More Information RAREsim is a flexible and accurate rare variant simulation algorithm that emulates real data. Using parameters and haplotypes derived from real sequencing data, RAREsim efficiently simulates the expected variant distribution and enables real variant annotations. See the full article here: https://doi.org/10.1016/j.ajhg.2022.02.009.\"]```"}, {"role": "user", "content": "Content from https://github.com/meganmichelle/RAREsim:\n```\nRepository metadata:\n{'name': 'RAREsim', 'label': ['RAREsim'], 'description': [], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [], 'repository': [{'term': 'https://github.com/meganmichelle/RAREsim', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Megan Null', 'type': 'person', 'email': 'mnull@collegeofidaho.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Ryan Barnard', 'type': 'person', 'email': 'rbarnard1107@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Ryan Barnard', 'type': 'person', 'email': '94932496+RMBarnard@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Megan Null', 'type': 'person', 'email': 'megansorenson@Megans-MacBook-Pro-2.local', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Megan Null', 'type': 'person', 'email': 'megan.sorenson@ucdenver.edu', 'maintainer': False}, 'id': 4}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/meganmichelle/RAREsim/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"kma/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/kma/cmd/1.4.0\",\n    \"name\": \"kma\",\n    \"description\": [\n      \"KMA is mapping a method designed to map raw reads directly against redundant databases, in an ultra-fast manner using seed and extend.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bitbucket.org/genomicepidemiology/kma/src/master/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a004d3b8acd1a2f9b47a\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/kma/cmd/1.4.14\",\n    \"name\": \"kma\",\n    \"description\": [\n      \"KMA is mapping a method designed to map raw reads directly against redundant databases, in an ultra-fast manner using seed and extend.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://bitbucket.org/genomicepidemiology/kma\",\n        \"kind\": \"bitbucket\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://bitbucket.org/genomicepidemiology/kma\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Apache-2.0\",\n        \"url\": \"https://spdx.org/licenses/Apache-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dc01ac7ee6cb5789128d\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bitbucket.org/genomicepidemiology/kma/src/master/:\n```\nRepository metadata:\n{'type': 'repository', 'full_name': 'genomicepidemiology/kma', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma'}, 'html': {'href': 'https://bitbucket.org/genomicepidemiology/kma'}, 'avatar': {'href': 'https://bytebucket.org/ravatar/%7B50aaec57-c34b-436b-9aff-67d252bde1d7%7D?ts=c'}, 'pullrequests': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/pullrequests'}, 'commits': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/commits'}, 'forks': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/forks'}, 'watchers': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/watchers'}, 'branches': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/refs/branches'}, 'tags': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/refs/tags'}, 'downloads': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/downloads'}, 'source': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/src'}, 'clone': [{'name': 'https', 'href': 'https://bitbucket.org/genomicepidemiology/kma.git'}, {'name': 'ssh', 'href': 'git@bitbucket.org:genomicepidemiology/kma.git'}], 'issues': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/issues'}, 'hooks': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/hooks'}}, 'name': 'kma', 'slug': 'kma', 'description': 'k-mer alignment', 'scm': 'git', 'website': None, 'owner': {'display_name': 'Genomic Epidemiology', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/%7Bec84c234-a1e2-4442-8d73-bc3bdc479f29%7D'}, 'avatar': {'href': 'https://bitbucket.org/account/genomicepidemiology/avatar/'}, 'html': {'href': 'https://bitbucket.org/%7Bec84c234-a1e2-4442-8d73-bc3bdc479f29%7D/'}}, 'type': 'team', 'uuid': '{ec84c234-a1e2-4442-8d73-bc3bdc479f29}', 'username': 'genomicepidemiology'}, 'workspace': {'type': 'workspace', 'uuid': '{ec84c234-a1e2-4442-8d73-bc3bdc479f29}', 'name': 'Genomic Epidemiology', 'slug': 'genomicepidemiology', 'links': {'avatar': {'href': 'https://bitbucket.org/workspaces/genomicepidemiology/avatar/?ts=1679912817'}, 'html': {'href': 'https://bitbucket.org/genomicepidemiology/'}, 'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/genomicepidemiology'}}}, 'is_private': False, 'project': {'type': 'project', 'key': 'CGE', 'uuid': '{cb734cd0-ec13-4bbd-aa6c-3d99d4d837a6}', 'name': 'CGE', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/genomicepidemiology/projects/CGE'}, 'html': {'href': 'https://bitbucket.org/genomicepidemiology/workspace/projects/CGE'}, 'avatar': {'href': 'https://bitbucket.org/genomicepidemiology/workspace/projects/CGE/avatar/32?ts=1474038005'}}}, 'fork_policy': 'allow_forks', 'created_on': '2017-10-19T06:34:14.576093+00:00', 'updated_on': '2025-04-04T10:18:27.864014+00:00', 'size': 177307056, 'language': 'c', 'uuid': '{50aaec57-c34b-436b-9aff-67d252bde1d7}', 'mainbranch': {'name': 'master', 'type': 'branch'}, 'override_settings': {'default_merge_strategy': False, 'branching_model': False}, 'parent': None, 'enforced_signed_commits': None, 'has_issues': True, 'has_wiki': False}```"}, {"role": "user", "content": "Content from https://bitbucket.org/genomicepidemiology/kma:\n```\nRepository metadata:\n{'type': 'repository', 'full_name': 'genomicepidemiology/kma', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma'}, 'html': {'href': 'https://bitbucket.org/genomicepidemiology/kma'}, 'avatar': {'href': 'https://bytebucket.org/ravatar/%7B50aaec57-c34b-436b-9aff-67d252bde1d7%7D?ts=c'}, 'pullrequests': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/pullrequests'}, 'commits': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/commits'}, 'forks': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/forks'}, 'watchers': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/watchers'}, 'branches': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/refs/branches'}, 'tags': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/refs/tags'}, 'downloads': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/downloads'}, 'source': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/src'}, 'clone': [{'name': 'https', 'href': 'https://bitbucket.org/genomicepidemiology/kma.git'}, {'name': 'ssh', 'href': 'git@bitbucket.org:genomicepidemiology/kma.git'}], 'issues': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/issues'}, 'hooks': {'href': 'https://api.bitbucket.org/2.0/repositories/genomicepidemiology/kma/hooks'}}, 'name': 'kma', 'slug': 'kma', 'description': 'k-mer alignment', 'scm': 'git', 'website': None, 'owner': {'display_name': 'Genomic Epidemiology', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/%7Bec84c234-a1e2-4442-8d73-bc3bdc479f29%7D'}, 'avatar': {'href': 'https://bitbucket.org/account/genomicepidemiology/avatar/'}, 'html': {'href': 'https://bitbucket.org/%7Bec84c234-a1e2-4442-8d73-bc3bdc479f29%7D/'}}, 'type': 'team', 'uuid': '{ec84c234-a1e2-4442-8d73-bc3bdc479f29}', 'username': 'genomicepidemiology'}, 'workspace': {'type': 'workspace', 'uuid': '{ec84c234-a1e2-4442-8d73-bc3bdc479f29}', 'name': 'Genomic Epidemiology', 'slug': 'genomicepidemiology', 'links': {'avatar': {'href': 'https://bitbucket.org/workspaces/genomicepidemiology/avatar/?ts=1679912817'}, 'html': {'href': 'https://bitbucket.org/genomicepidemiology/'}, 'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/genomicepidemiology'}}}, 'is_private': False, 'project': {'type': 'project', 'key': 'CGE', 'uuid': '{cb734cd0-ec13-4bbd-aa6c-3d99d4d837a6}', 'name': 'CGE', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/genomicepidemiology/projects/CGE'}, 'html': {'href': 'https://bitbucket.org/genomicepidemiology/workspace/projects/CGE'}, 'avatar': {'href': 'https://bitbucket.org/genomicepidemiology/workspace/projects/CGE/avatar/32?ts=1474038005'}}}, 'fork_policy': 'allow_forks', 'created_on': '2017-10-19T06:34:14.576093+00:00', 'updated_on': '2025-04-04T10:18:27.864014+00:00', 'size': 177307056, 'language': 'c', 'uuid': '{50aaec57-c34b-436b-9aff-67d252bde1d7}', 'mainbranch': {'name': 'master', 'type': 'branch'}, 'override_settings': {'default_merge_strategy': False, 'branching_model': False}, 'parent': None, 'enforced_signed_commits': None, 'has_issues': True, 'has_wiki': False}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"translig/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/translig/cmd/1.3\",\n    \"name\": \"translig\",\n    \"description\": [\n      \"A de novo transcriptome assembler that uses line graph iteration.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://sourceforge.net/projects/transcriptomeassembly/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dbfdac7ee6cb57891288\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/translig/cmd/None\",\n    \"name\": \"translig\",\n    \"description\": [\n      \"De novo transcriptome assembler that uses line graph iteration.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://sourceforge.net/projects/transcriptomeassembly/files/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82a9510e2f3aa0b3b8eda\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/transcriptomeassembly/files/:\n```\nProject metadata:\n{'description': None, 'sections': []}```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/transcriptomeassembly/:\n```\nProject metadata:\n{'description': None, 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['/p/transcriptomeassembly/activity/feed', '/p/transcriptomeassembly/activity/', 'https://sourceforge.net/auth/?return_to=%2Fprojects%2Ftranscriptomeassembly%2F']}, {'text': 'Follow Transcriptome Assembly\\nTranscriptome Assembly Web Site\\nFollow Transcriptome Assembly\\nTranscriptome Assembly Web Site', 'hrefs': ['https://transcriptomeassembly.sourceforge.io']}, {'text': 'User Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nUser Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n5\\n/ 5', 'hrefs': []}, {'text': 'User Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\njuntaosdu\\nEdited 2016-02-10\\n1. BinPacker is an efficient de novo trascriptome assembler for RNA-Seq data. It can assemble all transcripts from short reads (single or paired) without using a reference.  \\nThe software expects as input RNA-Seq reads in fasta or fastq format, and ouput all assembled candidate transcripts in fasta format. Briefly, it works in two step: first, BinPacker partitions the sequence data into many individual splicing graphs, each capturing the full transcriptional complexity at a given gene or no more than a few genes. Then BinPacker uses variants of the traditional bins packing model to search for minimum edge-path-cover over the splicng graph that can be supported by our data and could explain all observed splicing events of each locus.\\n  This software is free to use, modify, redistribute without any restrictions, except including the license provided with the distribution.\\n.............................................................................................\\n  2. TransComb is an efficient ab initio trascriptome assembler for RNA-Seq data. It can assemble all transcripts from short paired-end reads using a reference and analyze their abundances. The software expects as input alignment file by Tophat2 BAM format, and ouput all assembled candidate transcripts in gtf format.\\n  This software is free to use, modify, redistribute without any restrictions, except including the license provided with the distribution.\\n\u2026\\nRead More \u00bb\\nRead more reviews >\\nUser Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\njuntaosdu\\nEdited 2016-02-10\\n1. BinPacker is an efficient de novo trascriptome assembler for RNA-Seq data. It can assemble all transcripts from short reads (single or paired) without using a reference.  \\nThe software expects as input RNA-Seq reads in fasta or fastq format, and ouput all assembled candidate transcripts in fasta format. Briefly, it works in two step: first, BinPacker partitions the sequence data into many individual splicing graphs, each capturing the full transcriptional complexity at a given gene or no more than a few genes. Then BinPacker uses variants of the traditional bins packing model to search for minimum edge-path-cover over the splicng graph that can be supported by our data and could explain all observed splicing events of each locus.\\n  This software is free to use, modify, redistribute without any restrictions, except including the license provided with the distribution.\\n.............................................................................................\\n  2. TransComb is an efficient ab initio trascriptome assembler for RNA-Seq data. It can assemble all transcripts from short paired-end reads using a reference and analyze their abundances. The software expects as input alignment file by Tophat2 BAM format, and ouput all assembled candidate transcripts in gtf format.\\n  This software is free to use, modify, redistribute without any restrictions, except including the license provided with the distribution.\\n\u2026\\nRead More \u00bb\\nRead more reviews >', 'hrefs': ['/projects/transcriptomeassembly/reviews/#reviews', '#']}, {'text': 'Additional Project Details\\nRegistered\\n2014-11-25\\nAdditional Project Details\\nRegistered\\n2014-11-25', 'hrefs': []}]}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"racon/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"github/racon/None/1.5.0\",\n    \"name\": \"racon\",\n    \"description\": [\n      \"Ultrafast consensus module for raw de novo genome assembly of long uncorrected reads.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/lbcb-sci/racon\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"github\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/racon/undefined/None\",\n    \"name\": \"racon\",\n    \"description\": [\n      \"The Possibility to Use Oxford Nanopore Technology | Ultrafast consensus module for raw de novo genome assembly of long uncorrected reads. http://genome.cshlp.org/content/early/2017/01/18/gr.214270.116 Note: This was the original repository which will no longer be officially maintained. Please use the new official repository here: https://github.com/isovic/racon| Racon is intended as a standalone consensus module to correct raw contigs generated by rapid assembly methods which do not include a consensus step | Consensus module for raw de novo DNA assembly of long uncorrected reads.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/isovic/racon\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Chon-Sik Kang\",\n        \"email\": \"cskang@korea.kr\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Changsoo Kim\",\n        \"email\": \"changsookim@cnu.ac.kr\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8331310e2f3aa0b3bb706\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/isovic/racon:\n```\nREADME content:\n['# Racon [![Latest GitHub release](https://img.shields.io/github/release/lbcb-sci/racon.svg)](https://github.com/lbcb-sci/racon/releases/latest) [![Build status for gcc/clang](https://travis-ci.org/lbcb-sci/racon.svg?branch=master)](https://travis-ci.org/lbcb-sci/racon) [![Published in Genome Research](https://img.shields.io/badge/published%20in-Genome%20Research-blue.svg)](https://doi.org/10.1101/gr.214270.116) Consensus module for raw de novo DNA assembly of long uncorrected reads. ## Description Racon is intended as a standalone consensus module to correct raw contigs generated by rapid assembly methods which do not include a consensus step. The goal of Racon is to generate genomic consensus which is of similar or better quality compared to the output generated by assembly methods which employ both error correction and consensus steps, while providing a speedup of several times compared to those methods. It supports data produced by both Pacific Biosciences and Oxford Nanopore Technologies. Racon can be used as a polishing tool after the assembly with **either Illumina data or data produced by third generation of sequencing**. The type of data inputed is automatically detected. Racon takes as input only three files: contigs in FASTA/FASTQ format, reads in FASTA/FASTQ format and overlaps/alignments between the reads and the contigs in MHAP/PAF/SAM format. Output is a set of polished contigs in FASTA format printed to stdout. All input files **can be compressed with gzip** (which will have impact on parsing time). Racon can also be used as a read error-correction tool. In this scenario, the MHAP/PAF/SAM file needs to contain pairwise overlaps between reads **including dual overlaps**. A **wrapper script** is also available to enable easier usage to the end-user for large datasets. It has the same interface as racon but adds two additional features from the outside. Sequences can be **subsampled** to decrease the total execution time (accuracy might be lower) while target sequences can be **split** into smaller chunks and run sequentially to decrease memory consumption. Both features can be run at the same time as well. ## Dependencies 1. gcc 4.8+ or clang 3.4+ 2. cmake 3.2+ ### CUDA Support 1. gcc 5.0+ 2. cmake 3.10+ 4. CUDA 9.0+ ## Installation To install Racon run the following commands: ```bash git clone --recursive https://github.com/lbcb-sci/racon.git racon cd racon mkdir build cd build cmake -DCMAKE_BUILD_TYPE=Release .. make ``` After successful installation, an executable named `racon` will appear in `build/bin`. Optionally, you can run `sudo make install` to install racon executable to your machine. ***Note***: if you omitted `--recursive` from `git clone`, run `git submodule update --init --recursive` before proceeding with compilation. To build unit tests add `-Dracon_build_tests=ON` while running `cmake`. After installation, an executable named `racon_test` will be created in `build/bin`. To build the wrapper script add `-Dracon_build_wrapper=ON` while running `cmake`. After installation, an executable named `racon_wrapper` (python script) will be created in `build/bin`. ### CUDA Support Racon makes use of [NVIDIA\\'s GenomeWorks SDK](https://github.com/clara-parabricks/GenomeWorks) for CUDA accelerated polishing and alignment. To build `racon` with CUDA support, add `-Dracon_enable_cuda=ON` while running `cmake`. If CUDA support is unavailable, the `cmake` step will error out. Note that the CUDA support flag does not produce a new binary target. Instead it augments the existing `racon` binary itself. ```bash cd build cmake -DCMAKE_BUILD_TYPE=Release -Dracon_enable_cuda=ON .. make ``` ***Note***: Short read polishing with CUDA is still in development! ### Packaging To generate a Debian package for `racon`, run the following command from the build folder - ```bash make package ``` ## Usage Usage of `racon` is as following: racon [options ...] <sequences> <overlaps> <target sequences> # default output is stdout <sequences> input file in FASTA/FASTQ format (can be compressed with gzip) containing sequences used for correction <overlaps> input file in MHAP/PAF/SAM format (can be compressed with gzip) containing overlaps between sequences and target sequences <target sequences> input file in FASTA/FASTQ format (can be compressed with gzip) containing sequences which will be corrected options: -u, --include-unpolished output unpolished target sequences -f, --fragment-correction perform fragment correction instead of contig polishing (overlaps file should contain dual/self overlaps!) -w, --window-length <int> default: 500 size of window on which POA is performed -q, --quality-threshold <float> default: 10.0 threshold for average base quality of windows used in POA -e, --error-threshold <float> default: 0.3 maximum allowed error rate used for filtering overlaps --no-trimming disables consensus trimming at window ends -m, --match <int> default: 3 score for matching bases -x, --mismatch <int> default: -5 score for mismatching bases -g, --gap <int> default: -4 gap penalty (must be negative) -t, --threads <int> default: 1 number of threads --version prints the version number -h, --help prints the usage only available when built with CUDA: -c, --cudapoa-batches <int> default: 0 number of batches for CUDA accelerated polishing per GPU -b, --cuda-banded-alignment use banding approximation for polishing on GPU. Only applicable when -c is used. --cudaaligner-batches <int> default: 0 number of batches for CUDA accelerated alignment per GPU --cudaaligner-band-width <int> default: 0 Band width for cuda alignment. Must be >= 0. Non-zero allows user defined band width, whereas 0 implies auto band width determination. `racon_test` is run without any parameters. Usage of `racon_wrapper` equals the one of `racon` with two additional parameters: ... options: --split <int> split target sequences into chunks of desired size in bytes --subsample <int> <int> subsample sequences to desired coverage (2nd argument) given the reference length (1st argument) ... ## Contact information For additional information, help and bug reports please send an email to one of the following: ivan.sovic@irb.hr, robert.vaser@fer.hr, mile.sikic@fer.hr, nagarajann@gis.a-star.edu.sg ## Acknowledgment This work has been supported in part by Croatian Science Foundation under the project UIP-11-2013-7353. IS is supported in part by the Croatian Academy of Sciences and Arts under the project \"Methods for alignment and assembly of DNA sequences using nanopore sequencing data\". NN is supported by funding from A*STAR, Singapore.']```"}, {"role": "user", "content": "Content from https://github.com/isovic/racon:\n```\nRepository metadata:\n{'name': 'racon', 'label': ['racon'], 'description': [{'term': 'Ultrafast consensus module for raw de novo genome assembly of long uncorrected reads. http://genome.cshlp.org/content/early/2017/01/18/gr.214270.116 Note: This was the original repository which will no longer be officially maintained. Please use the new official repository here:', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://github.com/lbcb-sci/racon', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['1.4.3', '1.4.2', '1.4.0', '1.3.3', '1.3.2', '1.3.1', '1.3.0', '1.2.1', '1.2.0', '1.1.1', '1.1.0', '1.0.3', '1.0.2', '1.0.1', '1.0.0', '0.5.0'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/isovic/racon', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Robert Vaser', 'type': 'person', 'email': 'robert.vaser@fer.hr', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Joyjit Daw', 'type': 'person', 'email': 'jdaw@nvidia.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'rvaser', 'type': 'person', 'email': 'robert.vaser@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Ivan Sovic', 'type': 'person', 'email': 'isovic@users.noreply.github.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Ivan Sovic', 'type': 'person', 'email': 'ivan.sovic@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Steve Huang', 'type': 'person', 'email': 'shuang@broadinstitute.org', 'maintainer': False}, 'id': 5}, {'term': {'name': 'akkamesh', 'type': 'person', 'email': 'karumugam@nvidia.com', 'maintainer': False}, 'id': 6}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/isovic/racon/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/isovic/racon/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/isovic/racon/blob/main/meson_options.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"pizzly/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/pizzly/cmd/0.37.3\",\n    \"name\": \"pizzly\",\n    \"description\": [\n      \"Fast fusion detection using kallisto.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/pmelsted/pizzly\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/pmelsted/pizzly\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"BSD-2-Clause\",\n        \"url\": \"https://spdx.org/licenses/BSD-2-Clause.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/pizzly/cmd/0.37.3.1\",\n    \"name\": \"pizzly\",\n    \"description\": [\n      \"- fast fusion detection using kallisto.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fpizzly%2Fpizzly%2F0.37.3.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/pmelsted/pizzly:\n```\nREADME content:\n[\"![pizzly](logo.jpg) # pizzly Fast fusion detection using kallisto ### About pizzly is a program for detecting gene fusions from RNA-Seq data of cancer samples. It requires running [kallisto](https://pachterlab.github.io/kallisto) with the `--fusion` parameter (available in version `0.43.1` or later). ### Building pizzly uses the [SeqAn](http://www.seqan.de/) and requires a recent compiler with `c++14` enabled. This means `gcc 4.9` or later on linux or `clang 3.5` or later. Since these compilers are not always awailable on cluster systems we provide a precompiled binary under [releases](https://github.com/pmelsted/pizzly/releases). Compiling pizzly also requires `cmake 3.0.0` or later. In order to compile just run the following in the source directory ``` mkdir build cd build cmake .. make make install ``` ## Installing pizzly can be installed via the [Conda package manager](https://conda.io) from the [Bioconda channel](https://bioconda.github.io/). ```bash conda create pizzly --name pizzly -c bioconda ``` ### Ingredients Pizzly requires the reference transcriptome in FASTA format as well as a GTF file describing the transcriptome. We recommend using the [Ensembl](http://www.ensembl.org/index.html) transcriptomes. The example below assumes you have your transcriptome in FASTA format as `transcripts.fa.gz`, the GTF file `transcripts.gtf.gz` and your paired-end RNA-Seq data sets in `r1.fastq.gz` and `r2.fastq.gz` ### Running First we create the kallisto index ``` kallisto index -i index.idx -k 31 transcripts.fa.gz ``` Next we quantify using kallisto with fusion detection enabled ``` kallisto quant -i index.idx --fusion -o output r1.fastq.gz r2.fastq.gz ``` This creates the file `output/fusion.txt` which is used by pizzly, finally we run pizzly ``` pizzly -k 31 --gtf transcripts.gtf --cache index.cache.txt --align-score 2 \\\\ --insert-size 400 --fasta transcripts.fa.gz --output test output/fusion.txt ``` The parameters to set are * `--insert-size`, which should be the maximum insert size of the paired-end library (kallisto will estimate this from the reads, default used is 400) * `--align-score`, the number of mismatches allowed when aligning reads to a reference transcript (default used is 2) `--ignore-protein`, ignore any information about protein coding in the annotation, **warning** this will probably lead to an increase in the number of false positives reported. * `--cache`, if this file does not exist, pizzly will parse the GTF (which can take up to a minute or two) and store the required data in the cached file. If the cache file exists (if you've run pizzly previously on the same GTF file), pizzly will parse this file instead, which is much faster than parsing the GTF. A more sophisticated example is in the `test` directory which contains a `snakemake` workflow to index, quantify, call fusions and requantify using `kallisto` and `pizzly`. ### Output The `--output test` parameter is used as a prefix and two files are created `test.fusions.fasta` and `test.json`, this contains the filtered fusion calls. For unfiltered fusion calls, use the corresponding `.unfiltered` files. ### Scripts The `scripts` subfolder contains useful Python scripts - `get_fragment_length.py` examines an `abundance.h5` produced by `kallisto` and finds the 95th percentile of the fragment length distribution - `flatten_json.py` reads the `.json` output and converts to a simple gene table ### Annotations pizzly has been tested on [Ensembl](http://www.ensembl.org/) (versions 75+) and [Gencode](http://www.gencodegenes.org/) (version 19+) annotations. We recommend using the latest Ensembl annotations (version 87 [GTF](ftp://ftp.ensembl.org/pub/release-89/gtf/homo_sapiens/), [FASTA](ftp://ftp.ensembl.org/pub/release-89/fasta/homo_sapiens/cdna/)) for running with pizzly. Note that for gencode you will need to modify the FASTA file to remove pipe symbols (`|`) from the target names. The following should work (use `gzcat` on macosx) ``` zcat gencode.v26.transcripts.fa.gz | tr '|' ' ' | gzip -1 > gencode.v26.transcripts.fixed.fa.gz ``` The FASTA file used must be the same one that was used to build the kallisto index. ### License BSD-2\"]```"}, {"role": "user", "content": "Content from https://github.com/pmelsted/pizzly:\n```\nRepository metadata:\n{'name': 'pizzly', 'label': ['pizzly'], 'description': [{'term': 'Fast fusion detection using kallisto', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.37.3', 'v0.37.2', 'v0.37.1', 'v0.37.0'], 'license': [{'term': {'name': 'BSD 2-Clause \"Simplified\" License', 'url': 'http://choosealicense.com/licenses/bsd-2-clause/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/pmelsted/pizzly', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Pall Melsted', 'type': 'person', 'email': 'pmelsted@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Bj\u00f6rn Gr\u00fcning', 'type': 'person', 'email': 'bjoern@gruenings.eu', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/pmelsted/pizzly/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/pmelsted/pizzly/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"sparsedossa2/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconductor/sparsedossa2/lib/0.99.1\",\n    \"name\": \"sparsedossa2\",\n    \"description\": [\n      \"SparseDOSSA 2 is an R package for fitting to and the simulation of realistic microbial abundance observations. It provides functionlaities for: a) generation of synthetic microbial observations, based on either pre-fitted template that the package provides, or user-trained results.  b) spiking-in of associations with metadata variables or between feature pairs, for e.g. benchmarking or power analysis purposes.  c) fitting the SparseDOSSA 2 model to real-world microbial abundance observations.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://git.bioconductor.org/packages/SparseDOSSA2\",\n        \"kind\": \"bioconductor\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/SparseDOSSA2\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Siyuan Ma\",\n        \"email\": \"syma.research@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"bioconductor\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/sparsedossa2/lib/None\",\n    \"name\": \"sparsedossa2\",\n    \"description\": [\n      \"SparseDOSSA 2 is an upcoming software for the simulation of null and metadata-associated microbial abundance profiles. It has functionalities for a) fitting to user-provided microbial template datasets, b) synthesizing microbial abundances similar to either user-provided or pre-trained templates, and c) additionally simulating associations with sample metadata or among microbial features.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://huttenhower.sph.harvard.edu/sparsedossa2\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Curtis Huttenhower\",\n        \"email\": \"chuttenh@hsph.harvard.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8340110e2f3aa0b3bbbb2\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/SparseDOSSA2:\n```\nContent:\n['# Removed Packages A list of packages removed from Bioconductor along with their last-available landing pages. ## Packages deprecated in Bioconductor 3.21 (to be removed in 3.22) Software Packages: Experiment Data Packages: Annotation Data Packages: Workflow Packages: ## Packages removed with Bioconductor 3.21 release Software Packages: Note: [zlibbioc](/packages/3.19/bioc/html/zlibbioc.html) is deprecated but will be given an extra 6 months (removed in 3.22) to allow multiple dependencies to adapt. Experiment Data Packages: ## Packages removed with Bioconductor 3.20 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.19 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.18 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.17 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.16 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Packages: ## Packages removed with Bioconductor 3.15 release Software Packages: Experiment Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.14 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.13 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.12 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.11 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.10 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.9 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.8 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.7 release Software Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.6 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.5 release Software Packages: Experimental Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.4 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.3 release No packages were removed with this release. 17 packages were marked as deprecated, to be removed in the next release. ## Packages removed with Bioconductor 3.2 release No packages were removed with this release. ## Packages removed with Bioconductor 3.1 release ## Packages removed with Bioconductor 3.0 release ## Packages removed with Bioconductor 2.14 release ## Packages removed with Bioconductor 2.13 release ## Packages removed with Bioconductor 2.12 release ## Packages removed with Bioconductor 2.11 release ## Packages removed with Bioconductor 2.10 release ## Packages removed with Bioconductor 2.9 release ## Packages removed with Bioconductor 2.8 release ## Packages removed with Bioconductor 2.7 release ## Packages removed with Bioconductor 2.6 release ## Packages removed with Bioconductor 2.5 release ## Packages removed with Bioconductor 2.4 release ## Packages removed with Bioconductor 2.3 release ## Packages removed with Bioconductor 2.2 release ## Packages removed with Bioconductor 2.1 release ## Packages removed with Bioconductor 2.0 release ## Packages removed with Bioconductor 1.9 release ## Packages removed with Bioconductor 1.8 release ## Packages removed with Bioconductor 1.7 release']```"}, {"role": "user", "content": "Content from https://git.bioconductor.org/packages/SparseDOSSA2:\n```\nRepository metadata:\n{'url': 'https://git.bioconductor.org/packages/SparseDOSSA2'}```"}, {"role": "user", "content": "Content from http://huttenhower.sph.harvard.edu/sparsedossa2:\n```\nContent:\n['The most important functionality of SparseDOSSA2 is the simulation of realistic synthetic microbial observations. To this end, SparseDOSSA2 provides three pre-trained templates, \"Stool\" , \"Vaginal\" , and \"IBD\" , targeting continuous, discrete, and diseased population structures. Stool_simulation <- SparseDOSSA2(template = \"Stool\", n_sample = 100, n_feature = 100, verbose = TRUE) Vaginal_simulation <- SparseDOSSA2(template = \"Vaginal\", n_sample = 100, n_feature = 100, verbose = TRUE)']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bioconductor-crisprverse/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bioconductor-crisprverse/lib/1.4.0\",\n    \"name\": \"bioconductor-crisprverse\",\n    \"description\": [\n      \"The crisprVerse is a modular ecosystem of R packages developed for the design and manipulation of CRISPR guide RNAs (gRNAs). All packages share a common language and design principles. This package is designed to make it easy to install and load the crisprVerse packages in a single step. To learn more about the crisprVerse, visit <https://www.github.com/crisprVerse>.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/3.18/bioc/html/crisprVerse.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/crisprverse/lib/None\",\n    \"name\": \"crisprverse\",\n    \"description\": [\n      \"A comprehensive Bioconductor ecosystem for the design of CRISPR guide RNAs across nucleases and technologies.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/crisprVerse\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Jean-Philippe Fortin\",\n        \"email\": \"fortin946@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8371310e2f3aa0b3bc9ef\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/3.18/bioc/html/crisprVerse.html:\n```\nContent:\n['This package is for version 3.18 of Bioconductor; for the stable, up-to-date release version, see [crisprVerse](/packages/crisprVerse/) . ## Easily install and load the crisprVerse ecosystem for CRISPR gRNA design **Bioconductor version:** 3.18 The crisprVerse is a modular ecosystem of R packages developed for the design and manipulation of CRISPR guide RNAs (gRNAs). All packages share a common language and design principles. This package is designed to make it easy to install and load the crisprVerse packages in a single step. To learn more about the crisprVerse, visit . **Author:** Jean-Philippe Fortin [aut, cre] **Maintainer:** Jean-Philippe Fortin <fortin946 at gmail.com> **Citation (from within R, entercitation(\"crisprVerse\")):** ### Installation To install this package, start R (version \"4.3\") and enter: if (! require ( \"BiocManager\" , quietly = TRUE )) install. packages ( \"BiocManager\" ) BiocManager :: install ( \"crisprVerse\" ) For older versions of R, please refer to the appropriate [Bioconductor release](/about/release-announcements/) . ### Documentation To view documentation for the version of this package installed in your system, start R and enter: browseVignettes (\"crisprVerse\") ### Details biocViews [CRISPR](../../BiocViews.html#___CRISPR) , [FunctionalGenomics](../../BiocViews.html#___FunctionalGenomics) , [GeneTarget](../../BiocViews.html#___GeneTarget) , [Software](../../BiocViews.html#___Software) Version 1.4.0 In Bioconductor since BioC 3.16 (R-4.2) (1.5 years) License MIT + file [LICENSE](../licenses/crisprVerse/LICENSE) Depends R (>= 4.2.0) Imports BiocManager, cli, [crisprBase](../../bioc/html/crisprBase.html) , [crisprBowtie](../../bioc/html/crisprBowtie.html) , [crisprScore](../../bioc/html/crisprScore.html) , [crisprScoreData](../../data/experiment/html/crisprScoreData.html) , [crisprDesign](../../bioc/html/crisprDesign.html) , [crisprViz](../../bioc/html/crisprViz.html) , rlang, tools, utils System Requirements URL [https://github.com/crisprVerse/crisprVerse](https://github.com/crisprVerse/crisprVerse) Bug Reports [https://github.com/crisprVerse/crisprVerse/issues](https://github.com/crisprVerse/crisprVerse/issues) See More Suggests [BiocStyle](../../bioc/html/BiocStyle.html) , knitr, testthat Linking To Enhances Depends On Me Imports Me Suggests Me Links To Me Build Report [Build Report](https://bioconductor.org/checkResults/3.18/bioc-LATEST/crisprVerse/) ### Package Archives Follow [Installation](/install/#install-bioconductor-packages) instructions to use this package in your R session.']```"}, {"role": "user", "content": "Content from https://github.com/crisprVerse:\n```\nContent:\n['# Welcome to the crisprVerse, a Bioconductor ecosystem for designing CRISPR gRNAs <#welcome-to-the-crisprverse-a-bioconductor-ecosystem-for-designing-crispr-grnas> The crisprVerse is a Bioconductor ecosystem of R packages that enables efficient gRNA design and annotation for a multitude of CRISPR technologies, including CRISPR knockout (CRISPRko), CRISPR activation (CRISPRa), CRISPR interference (CRISPRi), CRISPR base editing (CRISPRbe), and CRISPR knockdown (CRISPRkd). The core package, crisprDesign, offers a comprehensive, user-friendly, and unified interface to add on- and off-target annotations via several alignment methods, rich gene and SNP annotations, and a dozen on- and off-target activity scores. These functionalities are enabled for any RNA- or DNA-targeting nucleases, including Cas9, Cas12, Cas13, and Csm. Both single and paired gRNA designs are enabled. For an overview of the ecosystem functionalities, see the [crisprDesign page](https://github.com/crisprVerse/crisprDesign) . For detailed examples, see our [tutorials](https://github.com/crisprVerse/Tutorials) . * Project lead: Jean-Philippe Fortin * Maintainers: Luke Hoberecht, Jean-Philippe Fortin * Contributors: Aaron Lun, Pirunthan Perampalam The crisprVerse is supported for macOS, Linux and Windows machines. It requires R version >=4.4. Some of the third-party functionalities are not available for Windows machines (BWA alignment, and some of the scoring functions). To download and install R, see the [R-project website](https://www.r-project.org/) . The crisprVerse is embedded within the Bioconductor ecosystem, therefore has 2 concurrent branches: release and devel . Currently (July 2024), the release branch is 3.19 , and the devel branch is 3.20 . Both branches requires R version 4.4 or higher. Release versions are created twice a year. See the [Bioconductor install page](https://www.bioconductor.org/install/) for more information regarding Bioconductor versions and to get an overview of how to install packages. ### Installing the core crisprVerse packages <#installing-the-core-crisprverse-packages> To install the packages from the release branch, type in the following commands in an R session: if ( ! requireNamespace( \" BiocManager \" , quietly = TRUE )) install.packages( \" BiocManager \" ) BiocManager :: install( version = \" release \" ) BiocManager :: install( \" crisprVerse \" ) To install the packages from the development branch, which is up to date with the lastest pushes to the GitHub repos, type in the following commands in an R session: if ( ! requireNamespace( \" BiocManager \" , quietly = TRUE )) install.packages( \" BiocManager \" ) BiocManager :: install( version = \" devel \" ) BiocManager :: install( \" crisprVerse \" ) The optional data package crisprDesignData can be installed from GitHub using the following commands: if ( ! requireNamespace( \" devtools \" , quietly = TRUE )) install.packages( \" devtools \" ) devtools :: install_github( \" crisprVerse/crisprDesignData \" ) For maxOS and Linux users, the [crisprBwa](https://github.com/crisprVerse/crisprBwa) can be installed from Bioconductor using the following: if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"crisprBwa\") We are providing ready-to-use Docker images that can be downloaded from the [Docker hub](https://hub.docker.com/r/fortin946/crisprverse) . See our [Docker page](https://github.com/crisprVerse/Docker) for more information. A comprehensive list of tutorials can be found [here](https://github.com/crisprVerse/Tutorials) . Tutorials are often updated to take into account the latest functionalities and changes of the ecosystem. ## Packages in the crisprVerse <#packages-in-the-crisprverse> Package BioC-release BioC-devel Description [crisprVerse](https://github.com/crisprVerse/crisprVerse) <http://bioconductor.org/checkResults/release/bioc-LATEST/crisprVerse/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/crisprVerse/> Easily install of the crisprVerse ecosystem [crisprDesign](https://github.com/crisprVerse/crisprDesign) <http://bioconductor.org/checkResults/release/bioc-LATEST/crisprDesign/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/crisprDesign/> Core gRNA design package across nucleases and applications [crisprBase](https://github.com/crisprVerse/crisprBase) <http://bioconductor.org/checkResults/release/bioc-LATEST/crisprBase/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/crisprBase/> Base functions and classes for CRISPR gRNA design [crisprBowtie](https://github.com/crisprVerse/crisprBowtie) <http://bioconductor.org/checkResults/release/bioc-LATEST/crisprBowtie/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/crisprBowtie/> Alignment of gRNA spacer sequences using bowtie [crisprBwa](https://github.com/crisprVerse/crisprBwa) <http://bioconductor.org/checkResults/release/bioc-LATEST/crisprBwa/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/crisprBwa/> Alignment of gRNA spacer sequences using BWA [crisprScore](https://github.com/crisprVerse/crisprScore) <http://bioconductor.org/checkResults/release/bioc-LATEST/crisprScore/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/crisprScore/> On-target and off-target scoring for CRISPR gRNAs [crisprScoreData](https://github.com/crisprVerse/crisprScoreData) <http://bioconductor.org/checkResults/release/data-experiment-LATEST/crisprScoreData/> <http://bioconductor.org/checkResults/devel/data-experiment-LATEST/crisprScoreData/> Pre-trained models for the crisprScore package [crisprViz](https://github.com/crisprVerse/crisprViz) <http://bioconductor.org/checkResults/release/bioc-LATEST/crisprViz/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/crisprViz/> Visualization of CRISPR gRNAs using genomic tracks [crisprShiny](https://github.com/crisprVerse/crisprShiny) <http://bioconductor.org/checkResults/release/bioc-LATEST/crisprShiny/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/crisprShiny/> Shiny interface for CRISPR gRNAs [screenCounter](https://github.com/crisprVerse/screenCounter) <http://bioconductor.org/checkResults/release/bioc-LATEST/screenCounter/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/screenCounter/> gRNA barcode sequencing reads alignment tool [Rbwa](https://github.com/crisprVerse/Rbwa) <http://bioconductor.org/checkResults/release/bioc-LATEST/Rbwa/> <http://bioconductor.org/checkResults/devel/bioc-LATEST/Rbwa/> R wrapper for BWA-backtrack and BWA-MEM aligners Package Devel Description [crisprDesignData](https://github.com/crisprVerse/crisprDesignData) <https://github.com/crisprVerse/crisprDesignData/actions/workflows/check-standard.yaml> Useful data for the crisprVerse ecosystem If using our software, please cite our Nature Communications paper [A comprehensive Bioconductor ecosystem for the design of CRISPR guide RNAs across nucleases and technologies](https://www.nature.com/articles/s41467-022-34320-7) . Here\\'s a bibtex citation: @article{hoberecht2022comprehensive, title={A comprehensive Bioconductor ecosystem for the design of CRISPR guide RNAs across nucleases and technologies}, author={Hoberecht, Luke and Perampalam, Pirunthan and Lun, Aaron and Fortin, Jean-Philippe}, journal={Nature Communications}, volume={13}, number={1}, pages={6568}, year={2022}, publisher={Nature Publishing Group UK London} } The ecosystem is released under the MIT license. Genentech Inc. owns the copyright.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"percolator/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/percolator/cmd/3.5\",\n    \"name\": \"percolator\",\n    \"description\": [\n      \"Semi-supervised learning for peptide identification from shotgun proteomics datasets.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/percolator/percolator\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/percolator/percolator\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Apache-2.0\",\n        \"url\": \"https://spdx.org/licenses/Apache-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dc36ac7ee6cb578912ba\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/percolator/cmd/None\",\n    \"name\": \"percolator\",\n    \"description\": [\n      \"Semi-supervised learning for peptide identification from MS/MS data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://noble.gs.washington.edu/proj/percolator/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"webmaster@ms-utils.org\",\n        \"maintainer\": false,\n        \"url\": \"http://ms-utils.org/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c829ed10e2f3aa0b3b8961\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/percolator/percolator:\n```\nREADME content:\n['## Percolator [![Build status](https://github.com/percolator/percolator/actions/workflows/build.yml/badge.svg)](https://github.com/percolator/percolator/actions/workflows/build.yml) [![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](code_of_conduct.md) This repository holds the source code for percolator and format converters software packages; a software for postprocessing of shotgun proteomics data. The source code is hosted at github, binary downloads can be found in the release repository: https://github.com/percolator/percolator/releases The building procedure is cmake-based. Helper scripts for building on different platforms can be found in: admin/builders/ The cleanest way to build is to use the vagrant virtual machine system, which can be setup using the script: admin/vagrant/manager.sh']```"}, {"role": "user", "content": "Content from https://github.com/percolator/percolator:\n```\nRepository metadata:\n{'name': 'percolator', 'label': ['percolator'], 'description': [{'term': 'Semi-supervised learning for peptide identification from shotgun proteomics datasets ', 'id': 0}], 'links': [], 'webpage': [{'term': 'http://percolator.ms', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['rel-3-07-01', 'rel-3-07', 'rel-3-06-05', 'rel-3-06-04', 'rel-3-06-03', 'rel-3-06-02', 'rel-3-06-01', 'rel-3-06', 'rel-3-05', 'rel-3-04', 'rel-3-03', 'rel-3-02-01', 'rel-3-02', 'rel-3-01-02', 'rel-3-01-01', 'rel-3-01', 'rel-3-00', 'rel-2-10', 'rel-2-09', 'rel-2-08-01', 'rel-2-08', 'rel-2-07-02', 'rel-2-07-01', 'rel-2-07', 'rel-2-06', 'rel-2-05'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/percolator/percolator', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'percolator', 'type': 'person', 'email': 'lukas.kall@scilifelab.se', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Lukas Ka\u0308ll', 'type': 'person', 'email': 'lukas.kall@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Frank Acquaye', 'type': 'person', 'email': 'acquayefrank@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Matthew The', 'type': 'person', 'email': 'matthew.the@tum.de', 'maintainer': False}, 'id': 3}, {'term': {'name': 'MatthewThe', 'type': 'person', 'email': 'MatthewThe@users.noreply.github.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Attila -- Kertesz-Farkas', 'type': 'person', 'email': 'kfattila@yandex.com', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Markus Ekvall', 'type': 'person', 'email': 'marekv@kth.se', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Brian Raiter', 'type': 'person', 'email': 'breadbox@muppetlabs.com', 'maintainer': False}, 'id': 7}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/percolator/percolator/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/percolator/percolator/blob/main/CPack.txt'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/percolator/percolator/blob/main/CommonCMake.txt'}, 'id': 2}, {'term': {'type': 'readme', 'url': 'https://github.com/percolator/percolator/blob/main/ReadMe.md'}, 'id': 3}, {'term': {'type': 'readme', 'url': 'https://github.com/percolator/percolator/blob/main/ReadMe.txt'}, 'id': 4}, {'term': {'type': 'code_of_conduct', 'url': 'https://github.com/percolator/percolator/blob/main/code_of_conduct.md'}, 'id': 5}, {'term': {'type': 'license', 'url': 'https://github.com/percolator/percolator/blob/main/license.txt'}, 'id': 6}, {'term': {'type': 'docs', 'url': 'https://github.com/percolator/percolator/blob/main/docs/CodeReviews.md'}, 'id': 7}, {'term': {'type': 'docs', 'url': 'https://github.com/percolator/percolator/blob/main/docs/ReleaseChecklist.md'}, 'id': 8}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://noble.gs.washington.edu/proj/percolator/:\n```\nContent:\n['## Semi-supervised learning for peptide identification from shotgun proteomics datasets ### Lukas K\u00e4ll, Jesse D. Canterbury, Jason Weston, William Stafford Noble and Michael J. MacCoss _Nature Methods_ 4:923 - 925, November 2007 ### Abstract Shotgun proteomics uses liquid chromatography-tandem mass spectrometry to identify proteins in complex biological samples. We describe an algorithm, called Percolator, for improving the rate of peptide identifications from a collection of tandem mass spectra. Percolator uses semi-supervised machine learning to discriminate between correct and decoy spectrum identifications, correctly assigning peptides to 17% more spectra from a tryptic dataset and up to 77% more spectra from non-tryptic digests, relative to a fully supervised approach. * Download the Percolator software[here](http://percolator.ms). For the experiments reported in the paper, Percolator version 1.01 was used. * Supplementary data: * For the \"Yeast 1\" spectra, we generated[10 sets of decoy PSMs](data/shuffled.sqt.tar.gz)using 10 decoy databases ([1](data/decoy1.fasta.gz),[2](data/decoy2.fasta.gz),[3](data/decoy3.fasta.gz),[4](data/decoy4.fasta.gz),[5](data/decoy5.fasta.gz),[6](data/decoy6.fasta.gz),[7](data/decoy7.fasta.gz),[8](data/decoy8.fasta.gz),[9](data/decoy9.fasta.gz),[10](data/decoy0.fasta.gz)) . * The yeast-01 data is available in[tab delimited format](data/yeast-01.tab.gz). * We used the following SEQUEST[parameter file](data/yeast.params)and[target database](data/yeast.fasta.gz)for the yeast data. * We used the following SEQUEST[parameter file](data/worm.params)and[target database](data/worm.fasta.gz)for the worm data. The ms2 and sqt file formats are described in [McDonaldet al.(2004)](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&cmd=Retrieve&list_uids=15317041) .']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"ncbi-fcs-gx/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/ncbi-fcs-gx/cmd/0.5.0\",\n    \"name\": \"ncbi-fcs-gx\",\n    \"description\": [\n      \"The NCBI Foreign Contamination Screen. Genomic cross-species aligner, for contamination detection.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/ncbi/fcs\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/ncbi/fcs\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"NCBI-PD\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/fcs/undefined/None\",\n    \"name\": \"fcs\",\n    \"description\": [\n      \"A calculator of conservation and allele frequency for variant pathogenic prediction.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://bioinfo.cnic.es/FCS\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Jos\u00e9 Luis Cabrera Alarc\u00f3n\",\n        \"email\": \"jlcabreraa@cnic.es\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c832cb10e2f3aa0b3bb480\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/ncbi/fcs:\n```\nREADME content:\n['# FCS The NCBI Foreign Contamination Screen (FCS) is a tool suite for identifying and removing contaminant sequences in genome assemblies. Contaminants are defined as sequences in a dataset that do not originate from the biological source organism and can arise from a variety of environmental and laboratory sources. FCS will help you remove contaminants from genomes before submission to GenBank. ## FCS-adaptor FCS-adaptor detects adaptor and vector contamination in genome sequences. FCS-adaptor is a high-throughput implementation of [NCBI VecScreen](https://www.ncbi.nlm.nih.gov/tools/vecscreen/about/). The FCS-adaptor executable retrieves a Docker or Singularity container and runs a pipeline to screen input sequences against a database of adaptors and vectors using stringent BLAST searches. Please read the [wiki](https://github.com/ncbi/fcs/wiki/FCS-adaptor-quickstart) for instructions on how to run FCS-adaptor. ## FCS-GX FCS-GX detects contamination from foreign organisms in genome sequences using the genome cross-species aligner (GX). The FCS-GX executable retrieves a Docker or Singularity container and runs a pipeline to align sequences to a large database of NCBI genomes through modified k-mer seeds and assign a most likely taxonomic division. FCS-GX classifies sequences as contaminant when their taxonomic assignment is different from the user-provided taxonomic identifier. Please read the [wiki](https://github.com/ncbi/fcs/wiki/FCS-GX-quickstart) for instructions on how to run FCS-GX. ## NEWS :exclamation: FCS is live on on [Galaxy](https://usegalaxy.org/)! Tutorial [here](https://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/ncbi-fcs/tutorial.html). ## REFERENCES ### FCS-GX Astashyn A, Tvedte ES, Sweeney D, Sapojnikov V, Bouk N, Joukov V, Mozes E, Strope PK, Sylla PM, Wagner L, Bidwell SL, Brown LC, Clark K, Davis EW, Smith-White B, Hlavina W, Pruitt KD, Schneider VA, Murphy TD. Rapid and sensitive detection of genome contamination at scale with FCS-GX. Genome Biol. 2024 Feb 26;25(1):60. doi: 10.1186/s13059-024-03198-7. PMID: 38409096; PMCID: PMC10898089. [Read the FCS-GX paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-024-03198-7) ## LICENSING The NCBI FCS tool suite software authored by NCBI is a \"United States Government Work\" under the terms of the United States Copyright Act. It was written as part of the authors\\' official duties as United States Government employees and thus cannot be copyrighted. This software is freely available to the public for use. The National Library of Medicine and the U.S. Government have not placed any restriction on its use or reproduction. Although all reasonable efforts have been taken to ensure the accuracy and reliability of the software and data, the NLM and the U.S. Government do not and cannot warrant the performance or results that may be obtained by using this software or data. The NLM and the U.S. Government disclaim all warranties, express or implied, including warranties of performance, merchantability or fitness for any particular purpose. Please cite NCBI in any work or product based on this material. ## FUNDING This work was supported by the National Center for Biotechnology Information of the National Library of Medicine (NLM), National Institutes of Health. FCS is part of the [NIH Comparative Genomics Resource (CGR)](https://www.ncbi.nlm.nih.gov/comparative-genomics-resource/), an NLM project to establish an ecosystem to facilitate reliable comparative genomics analyses for all eukaryotic organisms.']```"}, {"role": "user", "content": "Content from https://github.com/ncbi/fcs:\n```\nRepository metadata:\n{'name': 'fcs', 'label': ['fcs'], 'description': [{'term': 'Foreign Contamination Screening caller scripts and documentation', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.5.5', 'v0.5.4', 'v0.5.0', 'v0.4.0', 'v0.3.0-beta', 'v0.2.3-beta', 'v0.2.2-beta', 'v0.2.1-beta'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/ncbi/fcs', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Eric Tvedte', 'type': 'person', 'email': 'eric.tvedte@nih.gov', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Eric Tvedte', 'type': 'person', 'email': 'tvedtees@ncbi.nlm.nih.gov', 'maintainer': False}, 'id': 1}, {'term': {'name': 'etvedte', 'type': 'person', 'email': 'etvedte@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Strope, Pooja', 'type': 'person', 'email': 'pooja.strope@nih.gov', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Pooja Strope', 'type': 'person', 'email': 'pstrope@users.noreply.github.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Pape Sylla', 'type': 'person', 'email': 'syllapm@ncbi.nlm.nih.gov', 'maintainer': False}, 'id': 5}, {'term': {'name': 'deaconjs', 'type': 'person', 'email': 'deacon.sweeney@gmail.com', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Brad Holmes', 'type': 'person', 'email': 'holmesbr@ncbi.nlm.nih.gov', 'maintainer': False}, 'id': 7}, {'term': {'name': 'Deacon Sweeney', 'type': 'person', 'email': 'sweeneydj@nih.gov', 'maintainer': False}, 'id': 8}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/ncbi/fcs/blob/main/LICENSE.txt'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/ncbi/fcs/blob/main/PRIVACY.md'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/ncbi/fcs/blob/main/README.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://bioinfo.cnic.es/FCS:\n```\nContent:\n['# Instructions: #### Jose Luis Cabrera Alarcon ###### 2019-03-16 This is a shiny app for FCS/FCSM scores calculation, to aid in variant prioritization in nuclear and mitochondrial single nucleotide variants. 1. Click on \u201cFCS\u201d or \u201cFCSM\u201d to select the score to calculate. 2. Click on \u201cbrowser\" to upload variants data. Uploaded data must be in tab separated values with \".tsv\" or \".txt\" extesion. This file must contain the following variables under these exact headers: Locus_variability Phastcons phyloP GranthamScore MODIFIER MODERATE HIGH LOW OPTIONAL: You can change the purpose cutoff value to clasify variants in benign or pathogenic: OPTIONAL: Hit \u201cDownload data\u201d to save the results table to a csv file: # Authors: Jos\u00e9 Luis Cabrera Alarc\u00f3n (jlcabreraa@cnic.es) #### Powered by R Shiny.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"banner/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/banner/cmd/0.0.2\",\n    \"name\": \"banner\",\n    \"description\": [\n      \"BANNER is a tool that lives inside HULK and aims to make sense of hulk histosketches.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/will-rowe/banner\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://www.github.com/will-rowe/banner\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"sourceforge/banner/None/None\",\n    \"name\": \"banner\",\n    \"description\": [\n      \"BANNER is a named entity recognition system intended primarily for biomedical text. It uses conditional random fields as the primary recognition engine and includes a wide survey of the best techniques described in recent literature.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://sourceforge.net/projects/banner\",\n        \"kind\": \"sourceforge\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://banner.sourceforge.net/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"sourceforge\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/will-rowe/banner:\n```\nREADME content:\n['<div align=\"center\"> <img src=\"https://raw.githubusercontent.com/will-rowe/banner/master/misc/logo/banner-logo-with-text.png\" alt=\"banner-logo\" width=\"250\"> <hr> <a href=\"https://travis-ci.org/will-rowe/banner\"><img src=\"https://travis-ci.org/will-rowe/banner.svg?branch=master\" alt=\"travis\"></a> <a href=\\'http://hulk.readthedocs.io/en/latest/?badge=latest\\'><img src=\\'https://readthedocs.org/projects/hulk/badge/?version=latest\\' alt=\\'Documentation Status\\' /></a> <a href=\"https://github.com/will-rowe/banner/blob/master/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-orange.svg\" alt=\"License\"></a> <a href=\"https://zenodo.org/badge/latestdoi/144629592\"><img src=\"https://zenodo.org/badge/144629592.svg\" alt=\"DOI\"></a> </div> *** ``` BANNER is still under development - features and improvements are being added, so please check back soon. ``` *** ## Overview `BANNER` is a tool that lives inside [HULK](https://github.com/will-rowe/hulk) and aims to make sense of **hulk sketches**. At the moment, it trains a [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) using a set of labelled **hulk sketches**. It can then use this model to predict the label of microbiomes as they are sketches by ``HULK``. For example, you could train `BANNER` using a set of microbiomes from patients that either have or haven\\'t received antibiotic treatment. You can then use `BANNER` to predict whether a new microbiome sample exhibits signs of antibiotic dysbiosis. I will post more information and examples soon... ## Installation ### Bioconda ``` conda install banner ``` > note: if using Conda make sure you have added the [Bioconda](https://bioconda.github.io/) channel first #### Pip ``` pip install banner ``` ## Quick Start `BANNER` is called by typing **banner**, followed by the subcommand you wish to run. There are two main subcommands: **train** and **predict**. This quick start will show you how to get things running but it is recommended to follow the [HULK documentation](http://hulk-documentation.readthedocs.io/en/latest/?badge=latest). ```bash # Train a random forest classifier banner train -m hulk-banner-matrix.csv -o banner.rfc # Predict the label for a hulk sketch hulk sketch -f mystery-sample.fastq --stream -p 8 | banner predict -m banner.rfc ``` ## Notes * only supports 2 labels at the moment * there is very limited checking and not many unit tests...']```"}, {"role": "user", "content": "Content from https://github.com/will-rowe/banner:\n```\nRepository metadata:\n{'name': 'banner', 'label': ['banner'], 'description': [{'term': 'BANNER is a tool that lives inside HULK and aims to make sense of HULK sketches', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['0.0.2', '0.0.1'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/will-rowe/banner', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Will Rowe', 'type': 'person', 'email': 'will.rowe@stfc.ac.uk', 'maintainer': False}, 'id': 0}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/will-rowe/banner/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'requirements', 'url': 'https://github.com/will-rowe/banner/blob/main/requirements.txt'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://www.github.com/will-rowe/banner:\n```\nREADME content:\n['<div align=\"center\"> <img src=\"https://raw.githubusercontent.com/will-rowe/banner/master/misc/logo/banner-logo-with-text.png\" alt=\"banner-logo\" width=\"250\"> <hr> <a href=\"https://travis-ci.org/will-rowe/banner\"><img src=\"https://travis-ci.org/will-rowe/banner.svg?branch=master\" alt=\"travis\"></a> <a href=\\'http://hulk.readthedocs.io/en/latest/?badge=latest\\'><img src=\\'https://readthedocs.org/projects/hulk/badge/?version=latest\\' alt=\\'Documentation Status\\' /></a> <a href=\"https://github.com/will-rowe/banner/blob/master/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-orange.svg\" alt=\"License\"></a> <a href=\"https://zenodo.org/badge/latestdoi/144629592\"><img src=\"https://zenodo.org/badge/144629592.svg\" alt=\"DOI\"></a> </div> *** ``` BANNER is still under development - features and improvements are being added, so please check back soon. ``` *** ## Overview `BANNER` is a tool that lives inside [HULK](https://github.com/will-rowe/hulk) and aims to make sense of **hulk sketches**. At the moment, it trains a [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) using a set of labelled **hulk sketches**. It can then use this model to predict the label of microbiomes as they are sketches by ``HULK``. For example, you could train `BANNER` using a set of microbiomes from patients that either have or haven\\'t received antibiotic treatment. You can then use `BANNER` to predict whether a new microbiome sample exhibits signs of antibiotic dysbiosis. I will post more information and examples soon... ## Installation ### Bioconda ``` conda install banner ``` > note: if using Conda make sure you have added the [Bioconda](https://bioconda.github.io/) channel first #### Pip ``` pip install banner ``` ## Quick Start `BANNER` is called by typing **banner**, followed by the subcommand you wish to run. There are two main subcommands: **train** and **predict**. This quick start will show you how to get things running but it is recommended to follow the [HULK documentation](http://hulk-documentation.readthedocs.io/en/latest/?badge=latest). ```bash # Train a random forest classifier banner train -m hulk-banner-matrix.csv -o banner.rfc # Predict the label for a hulk sketch hulk sketch -f mystery-sample.fastq --stream -p 8 | banner predict -m banner.rfc ``` ## Notes * only supports 2 labels at the moment * there is very limited checking and not many unit tests...']```"}, {"role": "user", "content": "Content from https://www.github.com/will-rowe/banner:\n```\nRepository metadata:\n{'name': 'banner', 'label': ['banner'], 'description': [{'term': 'BANNER is a tool that lives inside HULK and aims to make sense of HULK sketches', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['0.0.2', '0.0.1'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/will-rowe/banner', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Will Rowe', 'type': 'person', 'email': 'will.rowe@stfc.ac.uk', 'maintainer': False}, 'id': 0}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/will-rowe/banner/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'requirements', 'url': 'https://github.com/will-rowe/banner/blob/main/requirements.txt'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://banner.sourceforge.net/:\n```\nContent:\n['# BANNER Named Entity Recognition System BANNER is a named entity recognition system, primarily intended for biomedical text. It is a machine-learning system based on conditional random fields and contains a wide survey of the best features in recent literature on biomedical named entity recognition (NER). BANNER is portable and is designed to maximize domain independence by not employing semantic features or rule-based processing steps. It is therefore useful to developers as an extensible NER implementation, to researchers as a standard for comparing innovative techniques, and to biologists requiring the ability to find novel entities in large amounts of text. BANNER is released under the [Common Public License 1.0](http://www.opensource.org/licenses/cpl1.0.php) and can be [tested online](http://cbioc.eas.asu.edu/banner/) or [downloaded](http://sourceforge.net/projects/banner/) from the releases page, though the [CVS repository](http://banner.cvs.sourceforge.net/banner/) contains more recent updates. BANNER is constantly being improved, so you may wish to subscribe to the [updates mailing list](http://lists.sourceforge.net/mailman/listinfo/banner-updates) to be notified of new releases and bug fixes. BANNER was written and is maintained by [Bob Leaman](http://www.fulton.asu.edu/~bioai/people.html#Bob) and is advised by [Dr. Graciela Gonzalez](http://www.fulton.asu.edu/~bioai/people.html#Gonzalez) , both members of the [BioAI lab](http://www.fulton.asu.edu/~bioai/) at Arizona State University. We have compared BANNER to the official results of the Second BioCreative Challenge Evaluation. Following the protocol for the BioCreative 2 gene mention task, BANNER was trained for this evaluation on the entire BioCreative 2 gene mention task training set and tested on the test set, both of which can be downloaded from the [BioCreative 2 datasets](http://sourceforge.net/project/showfiles.php?group_id=156383&package_id=194069&release_id=428496) . A difference of 1.23 or more f-measure is statistically significant, a difference of 0.35 or less is not (p < 0.05). Both the version of BANNER as published and the current version of BANNER, which includes a dictionary feature and is available in the CVS repository, are listed. System or author Rank at BioCreative 2 Precision (%) Recall (%) F-Measure Ando 1 88.48 85.97 87.21 BANNER (current) - 88.66 84.32 86.43 BANNER (as published) - 87.18 82.78 84.92 Vlachos 9 86.28 79.66 82.84 Baumgartner et. al. 11 (median) 85.54 76.83 80.95 NERBio 13 92.67 68.91 79.05 We have also compared BANNER to [ABNER](http://pages.cs.wisc.edu/~bsettles/abner/) and [LingPipe](http://www.alias-i.com/lingpipe/) (existing freely-available software), on two different corpora, using 5x2 cross-validation. Corpus: BioCreative 2 gene mention task (Training) BioText disease/treatment (diseases only) System Precision (%) Recall (%) F-Measure Precision (%) Recall (%) F-Measure BANNER 85.09 79.06 81.96 68.89 45.55 54.84 ABNER 83.21 73.94 78.30 66.08 44.86 53.44 LingPipe 60.34 70.32 64.95 55.41 47.50 51.15 BANNER is the subject of the following paper: Leaman, R. & Gonzalez G. (2008) _[BANNER: An executable survey of advances in biomedical named entity recognition](http://psb.stanford.edu/psb-online/proceedings/psb08/leaman.pdf)_ . Pacific Symposium on Biocomputing 13:652-663(2008) Thanks to SourceForge for providing hosting: <http://sourceforge.net>']```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/banner:\n```\nProject metadata:\n{'description': 'BANNER is a named entity recognition system intended primarily for biomedical text. It uses conditional random fields as the primary recognition engine and includes a wide survey of the best techniques described in recent literature.', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['https://sourceforge.net/auth/?return_to=%2Fprojects%2Fbanner%2F', '/p/banner/activity/feed', '/p/banner/activity/']}, {'text': 'Categories\\nBio-Informatics\\n,\\nLinguistics\\nLicense\\nCommon Public License 1.0\\nCategories\\nBio-Informatics\\n,\\nLinguistics\\nLicense\\nCommon Public License 1.0', 'hrefs': ['/directory/linguistics/', '/directory/bio-informatics/']}, {'text': 'Follow BANNER Named Entity Recognition System\\nBANNER Named Entity Recognition System Web Site\\nFollow BANNER Named Entity Recognition System\\nBANNER Named Entity Recognition System Web Site', 'hrefs': ['http://banner.sourceforge.net']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof BANNER Named Entity Recognition System!\\nUser Reviews\\nBe the first to\\npost a review\\nof BANNER Named Entity Recognition System!', 'hrefs': ['/projects/banner/reviews/new']}, {'text': 'Additional Project Details\\nLanguages\\nEnglish\\nIntended Audience\\nAdvanced End Users, Developers\\nProgramming Language\\nJava\\nRelated Categories\\nJava Bio-Informatics Software\\n,\\nJava Linguistics Software\\nRegistered\\n2007-09-14\\nAdditional Project Details\\nLanguages\\nEnglish\\nIntended Audience\\nAdvanced End Users, Developers\\nProgramming Language\\nJava\\nRelated Categories\\nJava Bio-Informatics Software\\n,\\nJava Linguistics Software\\nRegistered\\n2007-09-14', 'hrefs': ['/directory/java/', '/directory/linguistics/java/', '/directory/bio-informatics/java/']}]}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"escher/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconductor/escher/lib/1.7.0\",\n    \"name\": \"escher\",\n    \"description\": [\n      \"The creation of effective visualizations is a fundamental component of data analysis. In biomedical research, new challenges are emerging to visualize multi-dimensional data in a 2D space, but current data visualization tools have limited capabilities. To address this problem, we leverage Gestalt principles to improve the design and interpretability of multi-dimensional data in 2D data visualizations, layering aesthetics to display multiple variables. The proposed visualization can be applied to spatially-resolved transcriptomics data, but also broadly to data visualized in 2D space, such as embedding visualizations. We provide this open source R package escheR, which is built off of the state-of-the-art ggplot2 visualization framework and can be seamlessly integrated into genomics toolboxes and workflows.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://git.bioconductor.org/packages/escheR\",\n        \"kind\": \"bioconductor\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      },\n      {\n        \"url\": \"https://github.com/boyiguo1/escheR\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/boyiguo1/escheR\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Boyi Guo\",\n        \"email\": \"boyi.guo.work@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": \"https://orcid.org/0000-0003-2950-2349\"\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Stephanie C. Hicks\",\n        \"email\": \"shicks19@jhu.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": \"https://orcid.org/0000-0002-7858-0231\"\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Erik D. Nelson\",\n        \"email\": \"erik.nelson116@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": \"https://orcid.org/0000-0001-8477-0982\"\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"bioconductor\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/escher/lib/1.7.3\",\n    \"name\": \"escher\",\n    \"description\": [\n      \"Escher: A Web Application for Building, Sharing, and Embedding Data-Rich Visualizations of Metabolic Pathways.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://escher.github.io/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://escher.github.io/:\n```\nContent:\n['ESCHER Build, share, and embed visualizations of metabolic pathways ### Filter by organism All Escherichia coli Homo sapiens Saccharomyces cerevisiae ### Map None Core metabolism (e_coli_core) Central metabolism (iJO1366) Fatty acid beta-oxidation (iJO1366) Fatty acid biosynthesis (saturated) (iJO1366) Nucleotide and histidine biosynthesis (iJO1366) Nucleotide metabolism (iJO1366) Amino acid metabolism (partial) (RECON1) Carbohydrate metabolism (RECON1) Glycolysis TCA PPP (RECON1) Inositol retinol metabolism (RECON1) Tryptophan metabolism (RECON1) Central carbon metabolism (iMM904) ### Model (Optional) None e_coli_core iJO1366 RECON1 iMM904 ### Tool Builder Viewer ## Escher for Python Use Escher in Python and Jupyter import escher escher.Builder(reaction_data=fluxes) [Learn more and install](https://escher.readthedocs.io/en/latest/escher-python.html) ## EscherConverter EscherConverter is a standalone program that reads files created with Escher and converts them to files in community standard formats. [Learn more and download](https://escher.readthedocs.org/en/stable/escherconverter.html) ## FAQ * #### What is Escher?Escher is a web-based tool for building, viewing, and sharing visualizations of biological pathways. These pathway maps are a great way to contextualize data about metabolism. To get started, load a map by clicking the**Load Map**button above, or visit the[documentation](https://escher.readthedocs.io)to learn more. * #### How do I cite Escher?You can help support Escher by citing our publication when you use Escher or EscherConverter:Zachary A. King, Andreas Dr\u00e4ger, Ali Ebrahim, Nikolaus Sonnenschein, Nathan E. Lewis, and Bernhard O. Palsson (2015)_Escher: A web application for building, sharing, and embedding data-rich visualizations of biological pathways_, PLOS Computational Biology 11(8): e1004321. doi:[10.1371/journal.pcbi.1004321](http://dx.doi.org/10.1371/journal.pcbi.1004321) * #### How is Escher licensed?Escher and EscherConverter are distributed under the[MIT license](https://github.com/zakandrewking/escher/blob/master/LICENSE). * #### What browsers can I use?We recommend using Google Chrome for optimal performance, but Escher will also run in the latest versions of Firefox, MS Edge, and Safari (including mobile Chrome & Safari). * #### I have more questions. Who do I ask?Visit the[documentation](https://escher.readthedocs.io)to get started with Escher and explore the API, or ask questions in the[Gitter chat room](https://gitter.im/zakandrewking/escher). If you find bugs or would like to contribute to the project, feel free to submit an issue and or a pull request on[Github](https://github.com/zakandrewking/escher). Funding provided by the Novo Nordisk Foundation through the Center for Biosustainability at the Technical University of Denmark (NNF10CC1016517).']```"}, {"role": "user", "content": "Content from https://github.com/boyiguo1/escheR:\n```\nREADME content:\n['# escheR <img src=\"man/figures/logo.png\" align=\"right\" height=\"139\" alt=\"\" /> <!-- badges: start --> R Package Status: [![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental) [![R build status](https://github.com/boyiguo1/escheR/workflows/R-CMD-check-bioc/badge.svg)](https://github.com/boyiguo1/escheR/actions) Bioc Status: [![Bioc release status](http://www.bioconductor.org/shields/build/release/bioc/escheR.svg)](https://bioconductor.org/checkResults/release/bioc-LATEST/escheR) [![Bioc devel status](http://www.bioconductor.org/shields/build/devel/bioc/escheR.svg)](https://bioconductor.org/checkResults/devel/bioc-LATEST/escheR) [![Bioc history](https://bioconductor.org/shields/years-in-bioc/escheR.svg)](https://bioconductor.org/packages/release/bioc/html/escheR.html#since) [![Bioc last commit](https://bioconductor.org/shields/lastcommit/devel/bioc/escheR.svg)](http://bioconductor.org/checkResults/devel/bioc-LATEST/escheR/) <!-- badges: end --> The goal of `escheR` is to create an unified multi-dimensional spatial visualizations for spatially-resolved transcriptomics data following Gestalt principles. [Gestalt principles](http://www.scholarpedia.org/article/Gestalt_principles) refer to a set of rules describing how humans perceive and interpret visual information and are commonly applied in art and designs. Our implementation is based on the state-of-art data visualization framework [`ggplot2`](https://ggplot2.tidyverse.org/) to map individual variables to different aesthetics to simultaneously display disparate variables, showing their relationship in a spatial context. ![](man/figures/insitu.jpg) Our manuscript describing the innovative visualization is published in [_Bioinformatics Advances_](https://doi.org/10.1093/bioadv/vbad179). # Installation You can install the latest release version of `escheR` from Bioconductor. with using the following code will install version of the `nnSVG` package from Bioconductor. Additional details are shown on the [Bioconductor](https://bioconductor.org/packages/escheR) page. ```{r} if (!require(\"BiocManager\", quietly = TRUE)) { install.packages(\"BiocManager\") } BiocManager::install(\"escheR\") ``` The latest development version can also be installed from the `devel` version of Bioconductor or from [GitHub](https://github.com/boyiguo1/escheR) following ```{r} if (!require(\"devtools\")) install.packages(\"devtools\") devtools::install_github(\"boyiguo1/escheR\") ``` If you have R version before v4.3 and would like to install `escheR`, you can follow ```{r} if (!require(\"devtools\")) install.packages(\"devtools\") devtools::install_github(\"boyiguo1/escheR@pre-R4.3\") ``` > WARNING: The purpose of having the branch `pre-R4.3` is to allow users to use `escheR` before the formal release of R 4.3 and during the early stage of R 4.3 release. This branch will not update with any further development passing [`escheR v0.99.6`](https://github.com/boyiguo1/escheR/blob/devel/NEWS.md). We recommend users to update their R versions up to date. # Tutorial Please find an end-to-end tutorial at [https://boyiguo1.github.io/escheR/](https://boyiguo1.github.io/escheR/articles/SRT_eg.html) ## Syntax ![](man/figures/syntax.png) # Frequently Asked Questions * Can I use `escheR` on `SingleCellExperiment` object? Yes. Starting from Version 1.2.0, `escheR` supports the visualization of dimensionality reduced embeddings of a `SingleCellExperiment` object by providing values to the argument `dimred`. For more information, please see [Beyond Sequence-based Spatially-Resolved Data](https://boyi-guo.com/escheR/articles/more_than_visium.html#visualized-dimensionality-reduced-embedding-with-singlecellexperiment). * How can I make plots to visualize dimensional reduced embeddings? Yes, you can provide a string value to the`dimred` argument. The string value should be included in `reducedDimNames(object)`. For more information, please see [Beyond Sequence-based Spatially-Resolved Data](https://boyi-guo.com/escheR/articles/more_than_visium.html#visualized-dimensionality-reduced-embedding-with-singlecellexperiment). * Can I highlight only a subset of groups/levels for a categorical variables? Yes, please find the example in [Adjusting aesthetics](https://boyi-guo.com/escheR/articles/SRT_eg.html#adjusting-aesthetics). * Can `escheR` be directly applied to `Seurat` or `Giotto` objects? Unfortunately, `Seurat` or `Giotto` objects can not be directly pipe into `make_escheR` function. Instead, we provide a generic function that works with a `data.frame` object as input. Please find example via [Beyond Bioconductor Eco-system](https://boyi-guo.com/escheR/articles/more_than_visium.html#beyond-bioconductor-eco-system). * How can I make a paneled figure containing multiple `escheR` plots? You can compile and arrange individual plots by `escheR` via functions, e.g. [`cowplot::plot_grid`](https://wilkelab.org/cowplot/articles/plot_grid.html), [`patchwork`](https://patchwork.data-imaginist.com/index.html). Please find the example via [Plot Multiple Samples](https://boyi-guo.com/escheR/articles/SRT_eg.html#plot-multiple-samples). * How can I choose color palettes for `add_fill` and `add_ground` to minimize interference? Interested user can explore the topic on bivariate color palette. The [blog post](https://jakubnowosad.com/posts/2020-08-25-cbc-bp2/) by Jakub Nowosad and R package [`biscale`](https://cran.r-project.org/web/packages/biscale/vignettes/bivariate_palettes.html) could be helpful. * Can I apply `escheR` to plot image-based spatially resolved data? In principle, `escheR` can be used to visualize image-based spatially-resolved (SR) data. See examples via [Image-based SpatialExperiment Object](https://boyi-guo.com/escheR/articles/more_than_visium.html#image-based-spatialexperiment-object). Nevertheless, we are still optimizing the design principles for image-based SR data due to the difference in throughput compared to the sequence-based spatially resolved data. Optimization for image-based SR data visualization will be the main focus of our upcoming updates. We are open to working with new ideas and seek collaboration. * Can I plot polygons as each data points to show cell morphology? Unfortunately, not now. We acknowledge it would be a very important feature to consider, and we are motivated to implement it in our future updates. # Citation Our manuscript describing `escheR` is available via _Bioinformatics Advances_: * Guo B, Huuki-Myers LA, Grant-Peters M, Collado-Torres L, Hicks SC (2023). \u201cescheR: Unified multi-dimensional visualizations with Gestalt principles.\u201d Volume 3, Issue 1, 2023, vbad179, https://doi.org/10.1093/bioadv/vbad179 # Acknowledgement ## The importance of visual analytics ![](man/figures/visual_analytics.png) The panel of relational plots is from the book _the visual display of quantitative information_ by [Edward Tufte](https://www.edwardtufte.com/tufte/books_vdqi). This book inspired me to think more about the meaning of visual analytics in the era of computational science, and motivates me to think about statistical graphics design and its meaning. ## Development tools - Continuous code testing is possible thanks to [GitHub actions](https://www.tidyverse.org/blog/2020/04/usethis-1-6-0/) through *[biocthis](https://lcolladotor.github.io/biocthis/)*, - The [documentation website](https://boyiguo1.github.io/escheR/) is automatically updated thanks to *[pkgdown](https://CRAN.R-project.org/package=pkgdown)*. - The code is styled automatically thanks to *[styler](https://CRAN.R-project.org/package=styler)*. - The documentation is formatted thanks to *[devtools](https://CRAN.R-project.org/package=devtools)* and *[roxygen2](https://CRAN.R-project.org/package=roxygen2)*.']```"}, {"role": "user", "content": "Content from https://github.com/boyiguo1/escheR:\n```\nRepository metadata:\n{'name': 'escheR', 'label': ['escheR'], 'description': [{'term': 'R package to visualize colocalization for single cell & spatially-resolved genomics data', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://boyi-guo.com/escheR/', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.0.0', 'v0.99.6-alpha'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/boyiguo1/escheR', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/multidimensional', 'term': 'multidimensional', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/rstats', 'term': 'rstats', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/single-cell', 'term': 'single-cell', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/spatial-omics', 'term': 'spatial-omics', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/visualization', 'term': 'visualization', 'vocabulary': ''}, 'id': 4}], 'operations': [], 'authors': [{'term': {'name': 'Boyi Guo', 'type': 'person', 'email': 'boyi.guo.work@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'J Wokaty', 'type': 'person', 'email': 'jennifer.wokaty@sph.cuny.edu', 'maintainer': False}, 'id': 1}, {'term': {'name': 'J Wokaty', 'type': 'person', 'email': 'jwokaty@users.noreply.github.com', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/boyiguo1/escheR/blob/main/LICENSE.md'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/boyiguo1/escheR/blob/main/NEWS.md'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/boyiguo1/escheR/blob/main/README.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://git.bioconductor.org/packages/escheR:\n```\nRepository metadata:\n{'url': 'https://git.bioconductor.org/packages/escheR'}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"roddy/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/roddy/cmd/3+_versions\",\n    \"name\": \"roddy\",\n    \"description\": [\n      \"Roddy is a workflow management system for large scale NGS processing pipelines on Petabyte scale. It is used for Pan-Cancer Analysis of Whole Genomes (PCAWG) project and has a focus on high-throughput computing infrastructures.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/TheRoddyWMS/Roddy\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/TheRoddyWMS/Roddy\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Organization\",\n        \"name\": \"German Cancer Research Center (DKFZ)\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Philip R. Kensche\",\n        \"email\": \"p.kensche@dkfz.de\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/roddy/workbench/unreleased\",\n    \"name\": \"roddy\",\n    \"description\": [\n      \"Framework aiding scientists and developers to write portable and standardized workflows. It has support for different cluster backends and is designed to be as lightweight as possible. It features XML based multi-level configuration files, support for various workflow and scripting languages and gives them detailed information to reproduce workflow runs.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://ibios.dkfz.de/tbi/index.php/computational-oncology\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Organization\",\n        \"name\": \"German Cancer Research Center (DKFZ)\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"EilsLabs\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"hdhub\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Michael Heinold\",\n        \"email\": \"m.heinold@dkfz-heidelberg.de\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/TheRoddyWMS/Roddy:\n```\nREADME content:\n['[![CircleCI](https://circleci.com/gh/TheRoddyWMS/Roddy/tree/master.svg?style=svg)](https://circleci.com/gh/TheRoddyWMS/Roddy/tree/master) [![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FTheRoddyWMS%2FRoddy.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2FTheRoddyWMS%2FRoddy?ref=badge_shield) # What is Roddy? Roddy is a framework for development and management of workflows on a batch processing cluster. It has been developed at the German Cancer Research Center (DKFZ) in Heidelberg in the eilslabs group and is used by a number of in-house workflows such as the [PanCancer Alignment Workflow](https://github.com/DKFZ-ODCF/AlignmentAndQCWorkflows) and the [ACEseq workflow](https://github.com/eilslabs/ACEseqWorkflow). The development is now continued in the Omics IT and Data Management Core Facility (ODCF) at the DKFZ. > <table><tr><td><a href=\"https://www.denbi.de/\"><img src=\"docs/images/denbi.png\" alt=\"de.NBI logo\" width=\"300\" align=\"left\"></a></td><td><strong>Your opinion matters!</strong> The development of Roddy is supported by the <a href=\"https://www.denbi.de/\">German Network for Bioinformatic Infrastructure (de.NBI)</a>. By completing <a href=\"https://www.surveymonkey.de/r/denbi-service?sc=hd-hub&tool=roddy\">this very short survey</a> you support our efforts to improve this tool.</td></tr></table> > No new features will be implemented for Roddy! We will continue to fix bugs occurring with currently existing workflows. On the long run, existing workflows should be migrated to other workflow management systems. # Documentation You can find the documentation at [Read the Docs](http://roddy-documentation.readthedocs.io), including [detailed installation instructions](https://roddy-documentation.readthedocs.io/en/latest/installationGuide.html). # Workflows The following workflows have been developed at the DKFZ based on Roddy as workflow management system: * [Alignment and QC workflows](https://github.com/DKFZ-ODCF/AlignmentAndQCWorkflows) * [SNV-Calling workflow](https://github.com/DKFZ-ODCF/SNVCallingWorkflow) * [ACEseq workflow](https://github.com/DKFZ-ODCF/ACEseqWorkflow) for copy-number variation calling * [InDel-Calling workflow](https://github.com/DKFZ-ODCF/IndelCallingWorkflow) workflow * [Sophia workflow](https://github.com/DKFZ-ODCF/SophiaWorkflow) for structural variation calling * [RNA-seq workflow](https://github.com/DKFZ-ODCF/RNAseqWorkflow) * CNVkit for copy-number variation calling on exome data (to be published) * Leaf-Cutter workflow * [Bam-to-FASTQ](https://github.com/TheRoddyWMS/BamToFastqPlugin) plugin The following plugins are available as support for the workflows: * [COWorkflowBasePlugin](https://github.com/DKFZ-ODCF/COWorkflowsBasePlugin) with basic control code for many of the workflows * [PluginBase](https://github.com/TheRoddyWMS/Roddy-Base-Plugin) just the plugin base-class from which other plugins are derived * [DefaultPlugin](https://github.com/TheRoddyWMS/Roddy-Default-Plugin) with the `wrapInScript.sh` that wraps all Roddy cluster jobs']```"}, {"role": "user", "content": "Content from https://github.com/TheRoddyWMS/Roddy:\n```\nRepository metadata:\n{'name': 'Roddy', 'label': ['Roddy'], 'description': [{'term': 'The Roddy workflow development and management system.', 'id': 0}], 'links': [], 'webpage': [{'term': 'http://roddy-documentation.readthedocs.io', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['3.7.3', '3.5.11', '3.7.1', '3.7.0', '3.6.1', '3.6.0', '3.5.9', '3.5.8', '3.5.7', '3.5.6', '3.5.5', '3.5.4', '3.5.3', '3.5.2', '3.3.4', '3.5.1', '3.5.0', '3.4.2', '3.4.1', '3.4.0', '3.3.3', '3.3.2', '3.0.11', '3.3.1', '3.0.10', '3.0.9', '3.3.0', '3.2.3', '3.2.2', '3.2.1', '3.2.0', '3.1.0', '3.0.8', '3.0.7', '3.0.0'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/TheRoddyWMS/Roddy', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/workflow-management', 'term': 'workflow-management', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/hpc-applications', 'term': 'hpc-applications', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/workflow-engine', 'term': 'workflow-engine', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/scientific-workflows', 'term': 'scientific-workflows', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/pipeline', 'term': 'pipeline', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/pbs', 'term': 'pbs', 'vocabulary': ''}, 'id': 6}, {'term': {'uri': 'https://github.com/topics/lsf', 'term': 'lsf', 'vocabulary': ''}, 'id': 7}, {'term': {'uri': 'https://github.com/topics/grid-engine', 'term': 'grid-engine', 'vocabulary': ''}, 'id': 8}, {'term': {'uri': 'https://github.com/topics/workflow', 'term': 'workflow', 'vocabulary': ''}, 'id': 9}, {'term': {'uri': 'https://github.com/topics/roddy', 'term': 'roddy', 'vocabulary': ''}, 'id': 10}], 'operations': [], 'authors': [{'term': {'name': 'Philip Reiner Kensche', 'type': 'person', 'email': 'vinjana@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Philip R. Kensche', 'type': 'person', 'email': 'p.kensche@dkfz.de', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Philip R. Kensche', 'type': 'person', 'email': 'p.kensche@dkfz-heidelberg.de', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/TheRoddyWMS/Roddy/blob/main/CHANGELIST.md'}, 'id': 0}, {'term': {'type': 'contributing', 'url': 'https://github.com/TheRoddyWMS/Roddy/blob/main/CONTRIBUTING.md'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/TheRoddyWMS/Roddy/blob/main/CONTRIBUTORS.md'}, 'id': 2}, {'term': {'type': 'readme', 'url': 'https://github.com/TheRoddyWMS/Roddy/blob/main/README.md'}, 'id': 3}, {'term': {'type': 'requirements', 'url': 'https://github.com/TheRoddyWMS/Roddy/blob/main/requirements.txt'}, 'id': 4}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"gplas/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/gplas/cmd/None\",\n    \"name\": \"gplas\",\n    \"description\": [\n      \"A comprehensive tool for plasmid analysis using short-read graphs.\\n\\ngplas: Binning plasmid-predicted contigs using short-read graphs.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://gitlab.com/sirarredondo/gplas.git\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82ca210e2f3aa0b3b97a0\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/gplas/cmd/0.7.0\",\n    \"name\": \"gplas\",\n    \"description\": [\n      \"Gplas is a tool to bin plasmid-predicted contigs based on sequence composition, coverage and assembly graph information. It extends the possibility of accurately binning predicted plasmid contigs into several discrete plasmid components.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://gitlab.com/sirarredondo/gplas\",\n        \"kind\": \"gitlab\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://gitlab.com/sirarredondo/gplas\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://gitlab.com/sirarredondo/gplas.git:\n```\nREADME content:\n['--- title: \"gplas: binning plasmid-predicted contigs\" output: github_document fontsize: 16pt editor_options: chunk_output_type: inline --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ``` # Disclaimer **Note that this is the repo for gplas (version 1).** **If you are looking for gplas2, go to [gplas2 on gitlab](https://gitlab.com/mmb-umcu/gplas2)** # Introduction gplas is a tool to bin plasmid-predicted contigs based on sequence composition, coverage and assembly graph information. Gplas is a new tool that extends the possibility of accurately binning predicted plasmid contigs into several discrete plasmid components. ```{r, echo = FALSE} knitr::include_graphics(\"figures/logo.png\") ``` # Installation ```{bash, eval = FALSE} git clone https://gitlab.com/sirarredondo/gplas.git cd gplas ./gplas.sh -i test/faecium_graph.gfa -c mlplasmids -s \\'Enterococcus faecium\\' -n \\'installation\\' ``` First-time installation can take some time depending on your internet speed (~20 minutes). The good news is that you do not have to install any dependencies. The snakemake pipeline and different conda environments integrate all the dependencies required to run gplas. After the first-time installation, you will get the prediction of gplas in a few minutes and using a single thread! Gplas first checks if the following tools are present on your system: 1. [Conda](https://bioconda.github.io/) 2. [Snakemake](https://snakemake.readthedocs.io/en/stable/) version 5.5.4 After this, gplas will start the snakemake pipeline and will install different conda environments with the following R packages: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [igraph](https://cran.r-project.org/web/packages/igraph/index.html) version 1.2.4.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [ggraph](https://cran.r-project.org/web/packages/ggraph/index.html) version 1.0.2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [Biostrings](https://www.bioconductor.org/packages/release/bioc/html/Biostrings.html) version 2.50.2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [seqinr](https://cran.r-project.org/web/packages/seqinr/index.html) version 3.4-5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [tidyverse](https://www.tidyverse.org/) version 1.2.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [spatstat](https://cran.r-project.org/web/packages/spatstat/index.html) version 1.59-0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [ggrepel](https://cran.r-project.org/web/packages/ggrepel/index.html) version 0.8.0 Following this, it will install the tools that we use to predict plasmid-derived contigs. 4. [mlplasmids](https://gitlab.com/sirarredondo/mlplasmids) version 1.0.0 5. [plasflow](https://anaconda.org/bioconda/plasflow) version 1.1 # Usage ## Quick usage ### Running gplas with an assembly graph Gplas only requires a single argument **\\'-i\\'** corresponding to an assembly graph in gfa format. Such an assembly graph can be obtained with [SPAdes genome assembler](https://github.com/ablab/spades). You need to specify which classifier gplas is going to use, mlplasmids or plasflow, with the argument **\\'-c\\'** If you choose mlplasmids for the prediction, there is an additional mandatory argument **\\'-s\\'** in which you need to list any of the following bacterial species: - \\'Enterococcus faecium\\' - \\'Klebsiella pneumoniae\\' - \\'Acinetobacter baumannii\\' - \\'Escherichia coli\\' ```{bash, eval = FALSE} ./gplas.sh -i test/faecium_graph.gfa -c mlplasmids -s \\'Enterococcus faecium\\' -n \\'my_isolate\\' ``` You can use plasflow as a classifier if you have a different bacterial species. ```{bash, eval = FALSE} ./gplas.sh -i test/faecium_graph.gfa -c plasflow -s \\'Other species\\' -n \\'my_isolate\\' ``` ### New model for A. baumannii Thanks to the brilliant work from Alessia Carrara and Julian Paganini, we developed a new model for mlplasmids and integrated it into gplas. To use the model, please run: ```{bash, eval = FALSE} ./gplas.sh -i test/abaumannii_graph.gfa -c mlplasmids -s \\'Acinetobacter baumannii\\' -n \\'ab_test\\' -t 0.7 ``` ## Main output files Gplas will create a folder called \\'results\\' with the following files: ```{bash} ls results/my_isolate* ``` ### results/*results.tab Tab delimited file containing the prediction given by mlplasmids or plasflow together with the bin prediction by gplas. The file contains the following information: contig number, probability of being chromosome-derived, probability of being plasmid-derived, class prediction, contig name, k-mer coverage, length, bin assigned. ```{r, echo = FALSE} results <- read.table(file = \\'results/my_isolate_results.tab\\', header = TRUE) knitr::kable(results) ``` ### results/*components.tab Tab delimited file containing the bin prediction reported by gplas with the following information: contig number, bin assignation ```{r, echo = FALSE} components <- read.table(file = \\'results/my_isolate_bins.tab\\', header = TRUE) knitr::kable(components) ``` ### results/*plasmidome_network.png Png file of the plasmidome network generated by gplas after creating an undirected graph from edges between plasmid unitigs co-existing in the walks created by gplas. ```{r, echo=FALSE} knitr::include_graphics(\"results/my_isolate_plasmidome_network.png\") ``` ### results/*components.fasta Fasta files with the nodes belonging to each predicted component. ```{bash} grep \\'>\\' results/my_isolate*.fasta ``` ### walks/*solutions.csv gplas generates plasmid-like walks per each plasmid starting node. These paths are used later to generate the edges from the plasmidome network but they can also be useful to observe all the different walks starting from a single node (plasmid unitig). These walk can be directly given to Bandage to visualize and manually inspect a walk. In this case, we find different possible plasmid walks starting from the node 18+. These paths may contain inversions and rearrangements since repeats units such as transposases which can be present several times in the same plasmid sequence. In these cases, gplas can traverse the sequence in different ways generating different plasmid-like paths. ```{bash} head -n 10 walks/my_isolate_solutions.csv ``` For example, we can inspect in Bandage the path: 18+,76-,52+,94+,57-,77+,18+ This path forms a circular sequence since there is overlap between the initial and end node of the path. ```{r, echo=FALSE} knitr::include_graphics(\"figures/bandage_path.jpg\") ``` ## Complete usage Gplas can take the following arguments: Mandatory arguments: - **-i**: Path to the graph file in *.gfa format used to extract nodes and links. Gfa file format - **-c**: Classifier used to predict the contigs extracted from the input graph. String value: \\'plasflow\\' or \\'mlplasmids\\' - **-s**: Only applicable if mlplasmids is chosen. Bacterial species from the graph file. If you have specified mlplasmids as classifier you need to provide one of the following bacterial species: \\'Enterococcus faecium\\',\\'Klebsiella pneumoniae\\', \\'Acinetobacter baumannii\\' or \\'Escherichia coli\\' Optional arguments: - **-n**: Project name given to gplas. Default: \\'unnamed\\' - **-t**: Threshold to predict plasmid-derived sequences. Integer value ranging from 0 to 1. Default mlplasmids threshold: 0.5 Default plasflow threshold: 0.7 - **-x**: Number of times gplas finds plasmid paths per each plasmid starting node. Integer value ranging from 1 to infinite. Default: 20 - **-f**: Gplas filtering threshold score to reject possible outcoming edges. Integer value ranging from 0 to 1. Default: 0.1 - **-q**: Modularity threshold to split components present in the plasmidome network. Integer value ranging from 0 to 1. Default: 0.2 For benchmarking purposes you can pass a complete genome to gplas and will generate a precision and completeness. Using this you can assess the performance of gplas on a small set of genomes in which perhaps you have generated long-reads. - **-r**: Path to the complete reference genome corresponding to the graph given. For optimal results using this benchmarking flag, please name the reference genomes using the Unicycler scheme: e.g. \\'1 length=4123456\\' \\'2 length=10000\\' \\'3 length=2000\\' for your chromosome and plasmids. Fasta file format Fasta file format # Help page ```{bash} ./gplas.sh -h ``` # Issues/Bugs You can report any issues or bugs that you find while installing/running gplas using the [issue tracker](https://gitlab.com/sirarredondo/gplas/issues)']```"}, {"role": "user", "content": "Content from https://gitlab.com/sirarredondo/gplas.git:\n```\nRepository metadata:\n{'id': 13239498, 'description': 'gplas: Binning plasmid-predicted contigs using short-read graphs. ', 'name': 'gplas', 'name_with_namespace': 'Sergio_Arredondo / gplas', 'path': 'gplas', 'path_with_namespace': 'sirarredondo/gplas', 'created_at': '2019-07-09T15:13:04.298Z', 'default_branch': 'master', 'tag_list': [], 'topics': [], 'ssh_url_to_repo': 'git@gitlab.com:sirarredondo/gplas.git', 'http_url_to_repo': 'https://gitlab.com/sirarredondo/gplas.git', 'web_url': 'https://gitlab.com/sirarredondo/gplas', 'readme_url': 'https://gitlab.com/sirarredondo/gplas/-/blob/master/README.Rmd', 'forks_count': 4, 'avatar_url': 'https://gitlab.com/uploads/-/system/project/avatar/13239498/logo.png', 'star_count': 4, 'last_activity_at': '2023-05-02T09:15:20.580Z', 'namespace': {'id': 665405, 'name': 'Sergio_Arredondo', 'path': 'sirarredondo', 'kind': 'user', 'full_path': 'sirarredondo', 'parent_id': None, 'avatar_url': '/uploads/-/system/user/avatar/561404/avatar.png', 'web_url': 'https://gitlab.com/sirarredondo'}, 'container_registry_image_prefix': 'registry.gitlab.com/sirarredondo/gplas', '_links': {'self': 'https://gitlab.com/api/v4/projects/13239498', 'issues': 'https://gitlab.com/api/v4/projects/13239498/issues', 'merge_requests': 'https://gitlab.com/api/v4/projects/13239498/merge_requests', 'repo_branches': 'https://gitlab.com/api/v4/projects/13239498/repository/branches', 'labels': 'https://gitlab.com/api/v4/projects/13239498/labels', 'events': 'https://gitlab.com/api/v4/projects/13239498/events', 'members': 'https://gitlab.com/api/v4/projects/13239498/members', 'cluster_agents': 'https://gitlab.com/api/v4/projects/13239498/cluster_agents'}, 'packages_enabled': True, 'empty_repo': False, 'archived': False, 'visibility': 'public', 'owner': {'id': 561404, 'username': 'sirarredondo', 'name': 'Sergio_Arredondo', 'state': 'active', 'locked': False, 'avatar_url': 'https://gitlab.com/uploads/-/system/user/avatar/561404/avatar.png', 'web_url': 'https://gitlab.com/sirarredondo'}, 'resolve_outdated_diff_discussions': False, 'repository_object_format': 'sha1', 'issues_enabled': True, 'merge_requests_enabled': True, 'wiki_enabled': True, 'jobs_enabled': True, 'snippets_enabled': True, 'container_registry_enabled': True, 'service_desk_enabled': True, 'can_create_merge_request_in': True, 'issues_access_level': 'enabled', 'repository_access_level': 'enabled', 'merge_requests_access_level': 'enabled', 'forking_access_level': 'enabled', 'wiki_access_level': 'enabled', 'builds_access_level': 'enabled', 'snippets_access_level': 'enabled', 'pages_access_level': 'private', 'analytics_access_level': 'enabled', 'container_registry_access_level': 'enabled', 'security_and_compliance_access_level': 'private', 'releases_access_level': 'enabled', 'environments_access_level': 'enabled', 'feature_flags_access_level': 'enabled', 'infrastructure_access_level': 'enabled', 'monitor_access_level': 'enabled', 'model_experiments_access_level': 'enabled', 'model_registry_access_level': 'enabled', 'emails_disabled': False, 'emails_enabled': True, 'shared_runners_enabled': True, 'lfs_enabled': True, 'creator_id': 561404, 'import_status': 'none', 'open_issues_count': 6, 'description_html': '<p data-sourcepos=\"1:1-1:66\" dir=\"auto\">gplas: Binning plasmid-predicted contigs using short-read graphs.</p>', 'updated_at': '2024-01-09T18:08:17.317Z', 'ci_config_path': None, 'public_jobs': True, 'shared_with_groups': [{'group_id': 3732727, 'group_name': 'mmb-umcu', 'group_full_path': 'mmb-umcu', 'group_access_level': 30, 'expires_at': None}], 'only_allow_merge_if_pipeline_succeeds': False, 'allow_merge_on_skipped_pipeline': None, 'request_access_enabled': False, 'only_allow_merge_if_all_discussions_are_resolved': False, 'remove_source_branch_after_merge': None, 'printing_merge_request_link_enabled': True, 'merge_method': 'merge', 'squash_option': 'default_off', 'enforce_auth_checks_on_uploads': True, 'suggestion_commit_message': None, 'merge_commit_template': None, 'squash_commit_template': None, 'issue_branch_template': None, 'warn_about_potentially_unwanted_characters': True, 'autoclose_referenced_issues': True, 'max_artifacts_size': None, 'external_authorization_classification_label': '', 'requirements_enabled': False, 'requirements_access_level': 'enabled', 'security_and_compliance_enabled': False, 'compliance_frameworks': [], 'permissions': {'project_access': None, 'group_access': None}}```"}, {"role": "user", "content": "Content from https://gitlab.com/sirarredondo/gplas:\n```\nREADME content:\n['--- title: \"gplas: binning plasmid-predicted contigs\" output: github_document fontsize: 16pt editor_options: chunk_output_type: inline --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ``` # Disclaimer **Note that this is the repo for gplas (version 1).** **If you are looking for gplas2, go to [gplas2 on gitlab](https://gitlab.com/mmb-umcu/gplas2)** # Introduction gplas is a tool to bin plasmid-predicted contigs based on sequence composition, coverage and assembly graph information. Gplas is a new tool that extends the possibility of accurately binning predicted plasmid contigs into several discrete plasmid components. ```{r, echo = FALSE} knitr::include_graphics(\"figures/logo.png\") ``` # Installation ```{bash, eval = FALSE} git clone https://gitlab.com/sirarredondo/gplas.git cd gplas ./gplas.sh -i test/faecium_graph.gfa -c mlplasmids -s \\'Enterococcus faecium\\' -n \\'installation\\' ``` First-time installation can take some time depending on your internet speed (~20 minutes). The good news is that you do not have to install any dependencies. The snakemake pipeline and different conda environments integrate all the dependencies required to run gplas. After the first-time installation, you will get the prediction of gplas in a few minutes and using a single thread! Gplas first checks if the following tools are present on your system: 1. [Conda](https://bioconda.github.io/) 2. [Snakemake](https://snakemake.readthedocs.io/en/stable/) version 5.5.4 After this, gplas will start the snakemake pipeline and will install different conda environments with the following R packages: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [igraph](https://cran.r-project.org/web/packages/igraph/index.html) version 1.2.4.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [ggraph](https://cran.r-project.org/web/packages/ggraph/index.html) version 1.0.2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [Biostrings](https://www.bioconductor.org/packages/release/bioc/html/Biostrings.html) version 2.50.2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [seqinr](https://cran.r-project.org/web/packages/seqinr/index.html) version 3.4-5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [tidyverse](https://www.tidyverse.org/) version 1.2.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [spatstat](https://cran.r-project.org/web/packages/spatstat/index.html) version 1.59-0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [ggrepel](https://cran.r-project.org/web/packages/ggrepel/index.html) version 0.8.0 Following this, it will install the tools that we use to predict plasmid-derived contigs. 4. [mlplasmids](https://gitlab.com/sirarredondo/mlplasmids) version 1.0.0 5. [plasflow](https://anaconda.org/bioconda/plasflow) version 1.1 # Usage ## Quick usage ### Running gplas with an assembly graph Gplas only requires a single argument **\\'-i\\'** corresponding to an assembly graph in gfa format. Such an assembly graph can be obtained with [SPAdes genome assembler](https://github.com/ablab/spades). You need to specify which classifier gplas is going to use, mlplasmids or plasflow, with the argument **\\'-c\\'** If you choose mlplasmids for the prediction, there is an additional mandatory argument **\\'-s\\'** in which you need to list any of the following bacterial species: - \\'Enterococcus faecium\\' - \\'Klebsiella pneumoniae\\' - \\'Acinetobacter baumannii\\' - \\'Escherichia coli\\' ```{bash, eval = FALSE} ./gplas.sh -i test/faecium_graph.gfa -c mlplasmids -s \\'Enterococcus faecium\\' -n \\'my_isolate\\' ``` You can use plasflow as a classifier if you have a different bacterial species. ```{bash, eval = FALSE} ./gplas.sh -i test/faecium_graph.gfa -c plasflow -s \\'Other species\\' -n \\'my_isolate\\' ``` ### New model for A. baumannii Thanks to the brilliant work from Alessia Carrara and Julian Paganini, we developed a new model for mlplasmids and integrated it into gplas. To use the model, please run: ```{bash, eval = FALSE} ./gplas.sh -i test/abaumannii_graph.gfa -c mlplasmids -s \\'Acinetobacter baumannii\\' -n \\'ab_test\\' -t 0.7 ``` ## Main output files Gplas will create a folder called \\'results\\' with the following files: ```{bash} ls results/my_isolate* ``` ### results/*results.tab Tab delimited file containing the prediction given by mlplasmids or plasflow together with the bin prediction by gplas. The file contains the following information: contig number, probability of being chromosome-derived, probability of being plasmid-derived, class prediction, contig name, k-mer coverage, length, bin assigned. ```{r, echo = FALSE} results <- read.table(file = \\'results/my_isolate_results.tab\\', header = TRUE) knitr::kable(results) ``` ### results/*components.tab Tab delimited file containing the bin prediction reported by gplas with the following information: contig number, bin assignation ```{r, echo = FALSE} components <- read.table(file = \\'results/my_isolate_bins.tab\\', header = TRUE) knitr::kable(components) ``` ### results/*plasmidome_network.png Png file of the plasmidome network generated by gplas after creating an undirected graph from edges between plasmid unitigs co-existing in the walks created by gplas. ```{r, echo=FALSE} knitr::include_graphics(\"results/my_isolate_plasmidome_network.png\") ``` ### results/*components.fasta Fasta files with the nodes belonging to each predicted component. ```{bash} grep \\'>\\' results/my_isolate*.fasta ``` ### walks/*solutions.csv gplas generates plasmid-like walks per each plasmid starting node. These paths are used later to generate the edges from the plasmidome network but they can also be useful to observe all the different walks starting from a single node (plasmid unitig). These walk can be directly given to Bandage to visualize and manually inspect a walk. In this case, we find different possible plasmid walks starting from the node 18+. These paths may contain inversions and rearrangements since repeats units such as transposases which can be present several times in the same plasmid sequence. In these cases, gplas can traverse the sequence in different ways generating different plasmid-like paths. ```{bash} head -n 10 walks/my_isolate_solutions.csv ``` For example, we can inspect in Bandage the path: 18+,76-,52+,94+,57-,77+,18+ This path forms a circular sequence since there is overlap between the initial and end node of the path. ```{r, echo=FALSE} knitr::include_graphics(\"figures/bandage_path.jpg\") ``` ## Complete usage Gplas can take the following arguments: Mandatory arguments: - **-i**: Path to the graph file in *.gfa format used to extract nodes and links. Gfa file format - **-c**: Classifier used to predict the contigs extracted from the input graph. String value: \\'plasflow\\' or \\'mlplasmids\\' - **-s**: Only applicable if mlplasmids is chosen. Bacterial species from the graph file. If you have specified mlplasmids as classifier you need to provide one of the following bacterial species: \\'Enterococcus faecium\\',\\'Klebsiella pneumoniae\\', \\'Acinetobacter baumannii\\' or \\'Escherichia coli\\' Optional arguments: - **-n**: Project name given to gplas. Default: \\'unnamed\\' - **-t**: Threshold to predict plasmid-derived sequences. Integer value ranging from 0 to 1. Default mlplasmids threshold: 0.5 Default plasflow threshold: 0.7 - **-x**: Number of times gplas finds plasmid paths per each plasmid starting node. Integer value ranging from 1 to infinite. Default: 20 - **-f**: Gplas filtering threshold score to reject possible outcoming edges. Integer value ranging from 0 to 1. Default: 0.1 - **-q**: Modularity threshold to split components present in the plasmidome network. Integer value ranging from 0 to 1. Default: 0.2 For benchmarking purposes you can pass a complete genome to gplas and will generate a precision and completeness. Using this you can assess the performance of gplas on a small set of genomes in which perhaps you have generated long-reads. - **-r**: Path to the complete reference genome corresponding to the graph given. For optimal results using this benchmarking flag, please name the reference genomes using the Unicycler scheme: e.g. \\'1 length=4123456\\' \\'2 length=10000\\' \\'3 length=2000\\' for your chromosome and plasmids. Fasta file format Fasta file format # Help page ```{bash} ./gplas.sh -h ``` # Issues/Bugs You can report any issues or bugs that you find while installing/running gplas using the [issue tracker](https://gitlab.com/sirarredondo/gplas/issues)']```"}, {"role": "user", "content": "Content from https://gitlab.com/sirarredondo/gplas:\n```\nRepository metadata:\n{'id': 13239498, 'description': 'gplas: Binning plasmid-predicted contigs using short-read graphs. ', 'name': 'gplas', 'name_with_namespace': 'Sergio_Arredondo / gplas', 'path': 'gplas', 'path_with_namespace': 'sirarredondo/gplas', 'created_at': '2019-07-09T15:13:04.298Z', 'default_branch': 'master', 'tag_list': [], 'topics': [], 'ssh_url_to_repo': 'git@gitlab.com:sirarredondo/gplas.git', 'http_url_to_repo': 'https://gitlab.com/sirarredondo/gplas.git', 'web_url': 'https://gitlab.com/sirarredondo/gplas', 'readme_url': 'https://gitlab.com/sirarredondo/gplas/-/blob/master/README.Rmd', 'forks_count': 4, 'avatar_url': 'https://gitlab.com/uploads/-/system/project/avatar/13239498/logo.png', 'star_count': 4, 'last_activity_at': '2023-05-02T09:15:20.580Z', 'namespace': {'id': 665405, 'name': 'Sergio_Arredondo', 'path': 'sirarredondo', 'kind': 'user', 'full_path': 'sirarredondo', 'parent_id': None, 'avatar_url': '/uploads/-/system/user/avatar/561404/avatar.png', 'web_url': 'https://gitlab.com/sirarredondo'}, 'container_registry_image_prefix': 'registry.gitlab.com/sirarredondo/gplas', '_links': {'self': 'https://gitlab.com/api/v4/projects/13239498', 'issues': 'https://gitlab.com/api/v4/projects/13239498/issues', 'merge_requests': 'https://gitlab.com/api/v4/projects/13239498/merge_requests', 'repo_branches': 'https://gitlab.com/api/v4/projects/13239498/repository/branches', 'labels': 'https://gitlab.com/api/v4/projects/13239498/labels', 'events': 'https://gitlab.com/api/v4/projects/13239498/events', 'members': 'https://gitlab.com/api/v4/projects/13239498/members', 'cluster_agents': 'https://gitlab.com/api/v4/projects/13239498/cluster_agents'}, 'packages_enabled': True, 'empty_repo': False, 'archived': False, 'visibility': 'public', 'owner': {'id': 561404, 'username': 'sirarredondo', 'name': 'Sergio_Arredondo', 'state': 'active', 'locked': False, 'avatar_url': 'https://gitlab.com/uploads/-/system/user/avatar/561404/avatar.png', 'web_url': 'https://gitlab.com/sirarredondo'}, 'resolve_outdated_diff_discussions': False, 'repository_object_format': 'sha1', 'issues_enabled': True, 'merge_requests_enabled': True, 'wiki_enabled': True, 'jobs_enabled': True, 'snippets_enabled': True, 'container_registry_enabled': True, 'service_desk_enabled': True, 'can_create_merge_request_in': True, 'issues_access_level': 'enabled', 'repository_access_level': 'enabled', 'merge_requests_access_level': 'enabled', 'forking_access_level': 'enabled', 'wiki_access_level': 'enabled', 'builds_access_level': 'enabled', 'snippets_access_level': 'enabled', 'pages_access_level': 'private', 'analytics_access_level': 'enabled', 'container_registry_access_level': 'enabled', 'security_and_compliance_access_level': 'private', 'releases_access_level': 'enabled', 'environments_access_level': 'enabled', 'feature_flags_access_level': 'enabled', 'infrastructure_access_level': 'enabled', 'monitor_access_level': 'enabled', 'model_experiments_access_level': 'enabled', 'model_registry_access_level': 'enabled', 'emails_disabled': False, 'emails_enabled': True, 'shared_runners_enabled': True, 'lfs_enabled': True, 'creator_id': 561404, 'import_status': 'none', 'open_issues_count': 6, 'description_html': '<p data-sourcepos=\"1:1-1:66\" dir=\"auto\">gplas: Binning plasmid-predicted contigs using short-read graphs.</p>', 'updated_at': '2024-01-09T18:08:17.317Z', 'ci_config_path': None, 'public_jobs': True, 'shared_with_groups': [{'group_id': 3732727, 'group_name': 'mmb-umcu', 'group_full_path': 'mmb-umcu', 'group_access_level': 30, 'expires_at': None}], 'only_allow_merge_if_pipeline_succeeds': False, 'allow_merge_on_skipped_pipeline': None, 'request_access_enabled': False, 'only_allow_merge_if_all_discussions_are_resolved': False, 'remove_source_branch_after_merge': None, 'printing_merge_request_link_enabled': True, 'merge_method': 'merge', 'squash_option': 'default_off', 'enforce_auth_checks_on_uploads': True, 'suggestion_commit_message': None, 'merge_commit_template': None, 'squash_commit_template': None, 'issue_branch_template': None, 'warn_about_potentially_unwanted_characters': True, 'autoclose_referenced_issues': True, 'max_artifacts_size': None, 'external_authorization_classification_label': '', 'requirements_enabled': False, 'requirements_access_level': 'enabled', 'security_and_compliance_enabled': False, 'compliance_frameworks': [], 'permissions': {'project_access': None, 'group_access': None}}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"nebulosa/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconductor/nebulosa/lib/1.17.0\",\n    \"name\": \"nebulosa\",\n    \"description\": [\n      \"This package provides a enhanced visualization of single-cell data based on gene-weighted density estimation. Nebulosa recovers the signal from dropped-out features and allows the inspection of the joint expression from multiple features (e.g. genes). Seurat and SingleCellExperiment objects can be used within Nebulosa.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://git.bioconductor.org/packages/Nebulosa\",\n        \"kind\": \"bioconductor\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      },\n      {\n        \"url\": \"https://github.com/powellgenomicslab/Nebulosa\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/powellgenomicslab/Nebulosa\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Jose Alquicira-Hernandez\",\n        \"email\": \"alquicirajose@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": \"https://orcid.org/0000-0002-9049-7780\"\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"bioconductor\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/nebulosa/lib/None\",\n    \"name\": \"nebulosa\",\n    \"description\": [\n      \"Nebulosa recovers single cell gene expression signals by kernel density estimation.\\n\\nDue to the sparsity observed in single-cell data (e.g. RNA-seq, ATAC-seq), the visualization of cell features (e.g. gene, peak) is frequently affected and unclear, especially when it is overlaid with clustering to annotate cell types. Nebulosa is an R package to visualize data from single cells based on kernel density estimation. It aims to recover the signal from dropped-out features by incorporating the similarity between cells allowing a \u201cconvolution\u201d of the cell features.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.github.com/powellgenomicslab/Nebulosa\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Joseph E. Powell\",\n        \"email\": \"j.powell@garvan.org.au\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82e5f10e2f3aa0b3ba480\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/powellgenomicslab/Nebulosa:\n```\nREADME content:\n['# Nebulosa <img src=\"man/figure/logo.png\" align=\"right\" height=\"280\"/> [![](https://img.shields.io/badge/download-130/total-blue.svg)](https://bioconductor.org/packages/stats/bioc/Nebulosa) [![Build Status](https://travis-ci.org/powellgenomicslab/Nebulosa.svg?branch=master)](https://travis-ci.org/powellgenomicslab/Nebulosa) ![https://www.tidyverse.org/lifecycle/#maturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg) ## Motivation Due to the sparsity observed in single-cell data (e.g. RNA-seq, ATAC-seq), the visualization of cell features (e.g. gene, peak) is frequently affected and unclear, especially when it is overlaid with clustering to annotate cell types. Nebulosa is an R package to visualize data from single cells based on kernel density estimation. It aims to recover the signal from dropped-out features by incorporating the similarity between cells allowing a \u201cconvolution\u201d of the cell features. ## Installation `Nebulosa` is available on `Bioconductor` and can be installed as follows: ```R if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"Nebulosa\") ``` See [Nebulosa](https://bioconductor.org/packages/devel/bioc/html/Nebulosa.html) for more details. You can install the developing version of `Nebulosa` from github via `devtools`: ```R devtools::install_github(\"powellgenomicslab/Nebulosa\") ``` ## Vignettes Nebulosa can use `Seurat` and `SingleCellExperiment` objects. See the corresponding vignette: - [Seurat](https://bioconductor.org/packages/devel/bioc/vignettes/Nebulosa/inst/doc/nebulosa_seurat.html) - [OSCA-Bioconductor](https://bioconductor.org/packages/devel/bioc/vignettes/Nebulosa/inst/doc/introduction.html)']```"}, {"role": "user", "content": "Content from https://github.com/powellgenomicslab/Nebulosa:\n```\nRepository metadata:\n{'name': 'Nebulosa', 'label': ['Nebulosa'], 'description': [{'term': 'R package to visualize gene expression data based on weighted kernel density estimation', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['0.99.92'], 'license': [], 'repository': [{'term': 'https://github.com/powellgenomicslab/Nebulosa', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/single-cell', 'term': 'single-cell', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/single-cell-rna-seq', 'term': 'single-cell-rna-seq', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/single-cell-analysis', 'term': 'single-cell-analysis', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/single-cell-multiomics', 'term': 'single-cell-multiomics', 'vocabulary': ''}, 'id': 3}], 'operations': [], 'authors': [{'term': {'name': 'Jose Alquicira Hernandez', 'type': 'person', 'email': '7775770+joseah@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Katherine Beigel', 'type': 'person', 'email': '123108611+beigelk@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'joseah', 'type': 'person', 'email': 'alquicirajose@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Paul Hoffman', 'type': 'person', 'email': 'mojaveazure@users.noreply.github.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'G\u00f6k\u00e7en Eraslan', 'type': 'person', 'email': 'gokcen.eraslan@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Nitesh Turaga', 'type': 'person', 'email': 'nitesh.turaga@gmail.com', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/powellgenomicslab/Nebulosa/blob/main/NEWS.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/powellgenomicslab/Nebulosa/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://git.bioconductor.org/packages/Nebulosa:\n```\nRepository metadata:\n{'url': 'https://git.bioconductor.org/packages/Nebulosa'}```"}, {"role": "user", "content": "Content from http://www.github.com/powellgenomicslab/Nebulosa:\n```\nREADME content:\n['# Nebulosa <img src=\"man/figure/logo.png\" align=\"right\" height=\"280\"/> [![](https://img.shields.io/badge/download-130/total-blue.svg)](https://bioconductor.org/packages/stats/bioc/Nebulosa) [![Build Status](https://travis-ci.org/powellgenomicslab/Nebulosa.svg?branch=master)](https://travis-ci.org/powellgenomicslab/Nebulosa) ![https://www.tidyverse.org/lifecycle/#maturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg) ## Motivation Due to the sparsity observed in single-cell data (e.g. RNA-seq, ATAC-seq), the visualization of cell features (e.g. gene, peak) is frequently affected and unclear, especially when it is overlaid with clustering to annotate cell types. Nebulosa is an R package to visualize data from single cells based on kernel density estimation. It aims to recover the signal from dropped-out features by incorporating the similarity between cells allowing a \u201cconvolution\u201d of the cell features. ## Installation `Nebulosa` is available on `Bioconductor` and can be installed as follows: ```R if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"Nebulosa\") ``` See [Nebulosa](https://bioconductor.org/packages/devel/bioc/html/Nebulosa.html) for more details. You can install the developing version of `Nebulosa` from github via `devtools`: ```R devtools::install_github(\"powellgenomicslab/Nebulosa\") ``` ## Vignettes Nebulosa can use `Seurat` and `SingleCellExperiment` objects. See the corresponding vignette: - [Seurat](https://bioconductor.org/packages/devel/bioc/vignettes/Nebulosa/inst/doc/nebulosa_seurat.html) - [OSCA-Bioconductor](https://bioconductor.org/packages/devel/bioc/vignettes/Nebulosa/inst/doc/introduction.html)']```"}, {"role": "user", "content": "Content from http://www.github.com/powellgenomicslab/Nebulosa:\n```\nRepository metadata:\n{'name': 'Nebulosa', 'label': ['Nebulosa'], 'description': [{'term': 'R package to visualize gene expression data based on weighted kernel density estimation', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['0.99.92'], 'license': [], 'repository': [{'term': 'https://github.com/powellgenomicslab/Nebulosa', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/single-cell', 'term': 'single-cell', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/single-cell-rna-seq', 'term': 'single-cell-rna-seq', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/single-cell-analysis', 'term': 'single-cell-analysis', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/single-cell-multiomics', 'term': 'single-cell-multiomics', 'vocabulary': ''}, 'id': 3}], 'operations': [], 'authors': [{'term': {'name': 'Jose Alquicira Hernandez', 'type': 'person', 'email': '7775770+joseah@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Katherine Beigel', 'type': 'person', 'email': '123108611+beigelk@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'joseah', 'type': 'person', 'email': 'alquicirajose@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Paul Hoffman', 'type': 'person', 'email': 'mojaveazure@users.noreply.github.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'G\u00f6k\u00e7en Eraslan', 'type': 'person', 'email': 'gokcen.eraslan@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Nitesh Turaga', 'type': 'person', 'email': 'nitesh.turaga@gmail.com', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/powellgenomicslab/Nebulosa/blob/main/NEWS.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/powellgenomicslab/Nebulosa/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"crossicc/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconductor/crossicc/lib/1.7.0\",\n    \"name\": \"crossicc\",\n    \"description\": [\n      \"CrossICC utilizes an iterative strategy to derive the optimal gene set and cluster number from consensus similarity matrix generated by consensus clustering and it is able to deal with multiple cross platform datasets so that requires no between-dataset normalizations. This package also provides abundant functions for visualization and identifying subtypes of cancer. Specially, many cancer-related analysis methods are embedded to facilitate the clinical translation of the identified cancer subtypes.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://git.bioconductor.org/packages/CrossICC\",\n        \"kind\": \"bioconductor\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/CrossICC\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Yu Sun\",\n        \"email\": \"suny226@mail2.sysu.edu.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": \"https://orcid.org/0000-0003-4269-7187\"\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Qi Zhao\",\n        \"email\": \"zhaoqi@sysucc.org.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": \"https://orcid.org/0000-0002-8683-6145\"\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"bioconductor\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/crossicc/lib/None\",\n    \"name\": \"crossicc\",\n    \"description\": [\n      \"CrossICC is a R package for iterative consensus clustering of cross-platform gene expression data without adjusting batch effect. CrossICC utilizes an iterative strategy to derive the optimal gene set and cluster number from consensus similarity matrix generated by consensus clustering and it is able to deal with multiple cross platform datasets so that requires no between-dataset normalizations. This package also provides abundant functions for visualization and identifying subtypes of cancer. Specially, many cancer-related analysis methods are embedded to facilitate the clinical translation of the identified cancer subtypes.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/bioinformatist/CrossICC\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Ze-Xian Liu\",\n        \"email\": \"liuzx@sysucc.org.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Jian Ren\",\n        \"email\": \"renjian@sysucc.org.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Zhixiang Zuo\",\n        \"email\": \"zuozhx@sysucc.org.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82dff10e2f3aa0b3ba1a2\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://git.bioconductor.org/packages/CrossICC:\n```\nRepository metadata:\n{'url': 'https://git.bioconductor.org/packages/CrossICC'}```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/CrossICC:\n```\nContent:\n['# Removed Packages A list of packages removed from Bioconductor along with their last-available landing pages. ## Packages deprecated in Bioconductor 3.21 (to be removed in 3.22) Software Packages: Experiment Data Packages: Annotation Data Packages: Workflow Packages: ## Packages removed with Bioconductor 3.21 release Software Packages: Note: [zlibbioc](/packages/3.19/bioc/html/zlibbioc.html) is deprecated but will be given an extra 6 months (removed in 3.22) to allow multiple dependencies to adapt. Experiment Data Packages: ## Packages removed with Bioconductor 3.20 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.19 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.18 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.17 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.16 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Packages: ## Packages removed with Bioconductor 3.15 release Software Packages: Experiment Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.14 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.13 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.12 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.11 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.10 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.9 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.8 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.7 release Software Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.6 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.5 release Software Packages: Experimental Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.4 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.3 release No packages were removed with this release. 17 packages were marked as deprecated, to be removed in the next release. ## Packages removed with Bioconductor 3.2 release No packages were removed with this release. ## Packages removed with Bioconductor 3.1 release ## Packages removed with Bioconductor 3.0 release ## Packages removed with Bioconductor 2.14 release ## Packages removed with Bioconductor 2.13 release ## Packages removed with Bioconductor 2.12 release ## Packages removed with Bioconductor 2.11 release ## Packages removed with Bioconductor 2.10 release ## Packages removed with Bioconductor 2.9 release ## Packages removed with Bioconductor 2.8 release ## Packages removed with Bioconductor 2.7 release ## Packages removed with Bioconductor 2.6 release ## Packages removed with Bioconductor 2.5 release ## Packages removed with Bioconductor 2.4 release ## Packages removed with Bioconductor 2.3 release ## Packages removed with Bioconductor 2.2 release ## Packages removed with Bioconductor 2.1 release ## Packages removed with Bioconductor 2.0 release ## Packages removed with Bioconductor 1.9 release ## Packages removed with Bioconductor 1.8 release ## Packages removed with Bioconductor 1.7 release']```"}, {"role": "user", "content": "Content from https://github.com/bioinformatist/CrossICC:\n```\nREADME content:\n['--- output: github_document --- <!-- README.md is generated from README.Rmd. Please edit that file --> ```{r, echo = FALSE} knitr::opts_chunk$set( collapse = TRUE, comment = \"#>\", fig.path = \"README-\", eval = FALSE ) ``` # CrossICC ## Table of the content - [Overview](#overview) - [Installation](#installation) * [Via GitHub (latest)](#via-github-latest) - [Usage](#usage) - [FAQ](#faq) - [Contact](#contact) ## Overview Unsupervised clustering of high-throughput molecular profiling data is widely adopted for discovering cancer subtypes. However, cancer subtypes derived from a single dataset are not usually applicable across multiple datasets from different platforms. We previously published an iterative clustering algorithm to address the issue (see this [paper](http://clincancerres.aacrjournals.org/content/21/4/870.long)), but its use was hampered due to lack of implementation. In this work, we presented CrossICC that was an R package implementation of this method. Moreover, many new features were added to improve the performance of the algorithm. Briefly, [CrossICC](https://github.com/bioinformatist/CrossICC) utilizes an iterative strategy to derive the optimal gene set and cluster number from consensus similarity matrix generated by consensus clustering. [CrossICC](https://github.com/bioinformatist/CrossICC) is able to deal with multiple cross platform datasets so that requires no between-dataset normalizations. This package also provides abundant functions to help users visualize the identified subtypes and evaluate the subtyping performance. Specially, many cancer-related analysis methods are embedded to facilitate the clinical translation of the identified cancer subtypes. There are two modes for the integration of clusters derived cross-platform datasets: `cluster` mode and `sample` mode. For `cluster` mode, samples from each platform are clustered separately and centroids of each sub cluster derived from [ConsensusClusterPlus](https://bioconductor.org/packages/release/bioc/html/ConsensusClusterPlus.htm) were further clustered to generate super cluster. This process avoided removing batch effect across platforms. The details step by step illustration of this algorithm can be found in our previous published [paper](http://clincancerres.aacrjournals.org/content/early/2014/12/09/1078-0432.ccr-14-2481) and our recent submitted paper[coming soon]. For `sample` mode, sub clusters were firstly derived from [ConsensusClusterPlus](https://bioconductor.org/packages/release/bioc/html/ConsensusClusterPlus.htm) in each platform. We then calculated correlation coefficient between samples and centroids of clusters to get a new feature vector of each samples. Based on this new matrix, samples were divided into new clusters. ## Installation ### Via GitHub (latest) ```{r} install.packages(\"devtools\") devtools::install_github(\"bioinformatist/CrossICC\") ``` ## Usage CrossICC has the ability to automatically process **arbitrary numbers** of expression datasets, **no matter which platform they came from** (Even you can **use sequencing and microarray data together**). What you **only** need is a list of matrices in R, without any type of pre-processing (never need manipulation like filtering or normalization). ```{r} library(CrossICC) CrossICC.obj <- CrossICC(demo.platforms, skip.mfs = TRUE, max.iter = 100, cross = \"cluster\", fdr.cutoff = 0.1, ebayes.cutoff = 0.1, filter.cutoff = 0.1) ``` CrossICC will automatically iterate your data until it reaches convergence. By default, CrossICC will generate an `.rds` formatted object in your home path (`~/`, a.k.a `$HOME` in Linux), followed by an shiny app as shown below that is opened in your default browser, which provides you a very intuitive way to view the results. ## Shiny app Our package also comes with a shiny app. To run it: ```r pkg.suggested <- c(\\'ggalluvial\\', \\'rmarkdown\\', \\'knitr\\', \\'shiny\\', \\'shinydashboard\\', \\'shinyWidgets\\', \"shinycssloaders\", \\'DT\\', \\'ggthemes\\', \\'ggplot2\\', \\'pheatmap\\', \\'RColorBrewer\\', \\'tibble\\') checkPackages <- function(pkg){ if (!requireNamespace(pkg, quietly = TRUE)) { stop(\"Package pkg needed for shiny app. Please install it.\", call. = FALSE) } } lapply(pkg.suggested, checkPackages) shiny::runApp(system.file(\"shiny\", package = \"CrossICC\")) ``` ![](inst/imgs/readMe_home.jpg) ## FAQ - Question 1: NA values involved in our data set, how to go through them? > A: Users may encounter unexpected errors due to NA values in raw dataset. Therefore, we strongly recommanded that users checked the NA valus in their data set before loading it into `CrossICC`. To check the completed cases in matrix, `completed.cases` can be a good option to do that. Here, we also present an example for users to impute there data in case they don\\'t want to remove case in the dataset. The imputation method shown here are KNN method from impute package. ```{r} # for a individual matrix, plz do imputation using the following r code tempdata.impute=impute.knn(as.matrix(tempdata) ,k = 10, rowmax = 0.5, colmax = 0.8) normalize.Data=as.data.frame(tempdata.impute$data) ``` ## Contribution Qi Zhao [\\\\@likelet](https://github.com/likelet) and Yu Sun [\\\\@bioinformatist](https://github.com/bioinformatist) implemented the packages. Zhixiang Zuo [\\\\@zhixiang](https://scholar.google.com/citations?user=Ln_bw9AAAAAJ&hl=zh-CN) supervise the project. Zekun Liu performed the test and helped run an example of the package. For more information or questions, plz contact either of the authors above.']```"}, {"role": "user", "content": "Content from https://github.com/bioinformatist/CrossICC:\n```\nRepository metadata:\n{'name': 'CrossICC', 'label': ['CrossICC'], 'description': [{'term': 'An Interactive Consensus Clustering framework for Cross-platform data analysis ', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/bioinformatist/CrossICC', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Qi ZHAO', 'type': 'person', 'email': 'zhaoqi@sysucc.org.cn', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Qi ZHAO', 'type': 'person', 'email': 'likelet@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'likelet', 'type': 'person', 'email': 'zhaoqi699@sina.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Yu Sun', 'type': 'person', 'email': 'icannotendure666@gmail.com', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/bioinformatist/CrossICC/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"instrain/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"github/inStrain/None/v1.3.1\",\n    \"name\": \"inStrain\",\n    \"description\": [\n      \"Bioinformatics program inStrain.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/MrOlm/inStrain\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"github\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/instrain/undefined/None\",\n    \"name\": \"instrain\",\n    \"description\": [\n      \"InStrain is a tool for analysis of co-occurring genome populations from metagenomes that allows highly accurate genome comparisons, analysis of coverage, microdiversity, and linkage, and sensitive SNP detection with gene localization and synonymous non-synonymous identification.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://instrain.readthedocs.io/en/latest/#\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/MrOlm/inStrain:\n```\nREADME content:\n['# inStrain [![Downloads](https://pepy.tech/badge/instrain)](https://pepy.tech/project/instrain) [![Downloads](https://pepy.tech/badge/instrain/week)](https://pepy.tech/project/instrain) inStrain is a python program for analysis of co-occurring genome populations from metagenomes that allows highly accurate genome comparisons, analysis of coverage, microdiversity, and linkage, and sensitive SNP detection with gene localization and synonymous non-synonymous identification. Manual, installation instructions, and expected output are at available at [ReadTheDocs](https://instrain.readthedocs.io/en/latest/) Publication is available in [Nature Biotechnology](https://doi.org/10.1038/s41587-020-00797-0) and on [bioRxiv](https://www.biorxiv.org/content/10.1101/2020.01.22.915579v1) ## Installation options ### pip ``` $ pip install instrain ``` ### bioconda ``` $ conda install -c conda-forge -c bioconda -c defaults instrain ``` ### Docker Docker image is available on Docker Hub at [mattolm/instrain](https://hub.docker.com/repository/docker/mattolm/instrain). See [docker/](docker/) for use instructions. ## Quick start ### Show program help and modules: ``` $ inStrain -h ``` ### Microdiversity and SNP-calling pipeline: ``` $ inStrain profile mapping.bam genome_file.fasta -o inStrain_profile1 ``` ### Detailed strain-level comparison: ``` $ inStrain compare inStrain_profile1 inStrain_profile2 ```']```"}, {"role": "user", "content": "Content from https://github.com/MrOlm/inStrain:\n```\nRepository metadata:\n{'name': 'inStrain', 'label': ['inStrain'], 'description': [{'term': 'Bioinformatics program inStrain', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.3.1'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/MrOlm/inStrain', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Matt Olm', 'type': 'person', 'email': 'mattolm@Matts-MacBook-Pro-2.local', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Matt Olm', 'type': 'person', 'email': 'mattolm@saba.stanford.edu', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Jing Wang', 'type': 'person', 'email': 'zhenjiaofenjie@sjtu.edu.cn', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Matt Olm', 'type': 'person', 'email': 'mattolm@berkeley.edu', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Alex Crits-Christoph', 'type': 'person', 'email': 'acritsc1@jhu.edu', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Vini Salazar', 'type': 'person', 'email': '17276653+vinisalazar@users.noreply.github.com', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Matt Olm', 'type': 'person', 'email': 'mattolm@sr22-ac376befe7.stanford.edu', 'maintainer': False}, 'id': 6}, {'term': {'name': 'szimmerman92', 'type': 'person', 'email': 'samuel.e.zimmerman@gmail.com', 'maintainer': False}, 'id': 7}, {'term': {'name': 'Matt Olm', 'type': 'person', 'email': 'mattolm@DN52e6ik.SUNet', 'maintainer': False}, 'id': 8}, {'term': {'name': 'Matt Olm', 'type': 'person', 'email': 'mattolm@udon.stanford.edu', 'maintainer': False}, 'id': 9}, {'term': {'name': 'Matt Olm', 'type': 'person', 'email': 'mattolm@Matts-MacBook-Pro.local', 'maintainer': False}, 'id': 10}, {'term': {'name': 'Francisco Zorrilla', 'type': 'person', 'email': 'fz274@cam.ac.uk', 'maintainer': False}, 'id': 11}, {'term': {'name': 'rusher321', 'type': 'person', 'email': 'renhuahui@genomics.cn', 'maintainer': False}, 'id': 12}, {'term': {'name': 'Matt Olm', 'type': 'person', 'email': 'mattolm@gmail.com', 'maintainer': False}, 'id': 13}, {'term': {'name': 'Mike Lee', 'type': 'person', 'email': 'michael.lee0517@gmail.com', 'maintainer': False}, 'id': 14}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'changelog', 'url': 'https://github.com/MrOlm/inStrain/blob/main/CHANGELOG.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/MrOlm/inStrain/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'requirements', 'url': 'https://github.com/MrOlm/inStrain/blob/main/docs/requirements.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://instrain.readthedocs.io/en/latest/#:\n```\nContent:\n['\u00a9 Copyright 2019, Matt Olm and Alex Crits-Christoph. Built with']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"pydockweb/web": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/pydockweb/web/3.5.1\",\n    \"name\": \"pydockweb\",\n    \"description\": [\n      \"Web server for the structural prediction of protein-protein interactions. Given the 3D coordinates of two interacting proteins, it returns the best rigid-body docking orientations generated by FTDock (Gabb HA. et al. J Mol Biol 1997;272:106-120) and evaluated by pyDock scoring function (Cheng et al. 2007 Proteins 68, 503-515), which includes electrostatics, desolvation energy and limited van der Waals contribution.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://life.bsc.es/pid/pydockweb\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Brian Jim\u00e9nez-Garc\u00ed\u00ada\",\n        \"email\": \"b.jimenezgarcia@uu.nl\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"Barcelona Supercomputing Center\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"https://www.bsc.es/\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": null,\n        \"email\": \"pydock.services@gmail.com\",\n        \"maintainer\": false,\n        \"url\": \"https://life.bsc.es/pid/pydockweb/info/contactus\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Luis \u00c1ngel Rodriguez Lumbreras\",\n        \"email\": \"luis.rodriguez@bsc.es\",\n        \"maintainer\": false,\n        \"url\": \"https://www.bsc.es/rodriguez-lumbreras-luis-angel\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82a0f10e2f3aa0b3b8a77\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/pydockweb/web/1\",\n    \"name\": \"pydockweb\",\n    \"description\": [\n      \"Web server for the structural prediction of protein-protein interactions. Given the 3D coordinates of two interacting proteins, it returns the best rigid-body docking orientations generated by FTDock (Gabb HA. et al. J Mol Biol 1997;272:106-120) and evaluated by pyDock scoring function (Cheng et al. 2007 Proteins 68, 503-515), which includes electrostatics, desolvation energy and limited van der Waals contribution.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://life.bsc.es/servlet/pydock/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Brian Jim\u00e9nez-Garc\u00ed\u00ada\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"Barcelona Supercomputing Center\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"bsc.es\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"bjimenez@bsc.es\",\n        \"maintainer\": false,\n        \"url\": \"https://life.bsc.es/servlet/pydock/contact/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82a0f10e2f3aa0b3b8a77\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://life.bsc.es/pid/pydockweb:\n```\nContent:\n['**pyDockWEB is a web server for the structural prediction of protein-protein interactions.** Given the 3D coordinates of two interacting proteins, pyDockWEB returns the best rigid-body docking orientations generated by [FTDock](https://www.ncbi.nlm.nih.gov/pubmed/9299341) and evaluated by [pyDock](https://www.ncbi.nlm.nih.gov/pubmed/17444519) scoring function, which includes electrostatics, desolvation energy and limited van der Waals contribution. Please cite this reference if you use pyDockWEB in your research: Jimenez-Garcia B., Pons C. and Fernandez-Recio J. [\"pyDockWEB: a web server for rigid-body protein-protein docking using electrostatics and desolvation scoring\".](https://www.ncbi.nlm.nih.gov/pubmed/23661696) _Bioinformatics_ (2013) 29(13):1698-1699. A 32-bit Linux version of pyDock is available for download [here](https://life.bsc.es/pid/pydock/get_pydock.html) .']```"}, {"role": "user", "content": "Content from https://life.bsc.es/servlet/pydock/:\n```\nContent:\n['**pyDockWEB is a web server for the structural prediction of protein-protein interactions.** Given the 3D coordinates of two interacting proteins, pyDockWEB returns the best rigid-body docking orientations generated by [FTDock](https://www.ncbi.nlm.nih.gov/pubmed/9299341) and evaluated by [pyDock](https://www.ncbi.nlm.nih.gov/pubmed/17444519) scoring function, which includes electrostatics, desolvation energy and limited van der Waals contribution. Please cite this reference if you use pyDockWEB in your research: Jimenez-Garcia B., Pons C. and Fernandez-Recio J. [\"pyDockWEB: a web server for rigid-body protein-protein docking using electrostatics and desolvation scoring\".](https://www.ncbi.nlm.nih.gov/pubmed/23661696) _Bioinformatics_ (2013) 29(13):1698-1699. A 32-bit Linux version of pyDock is available for download [here](https://life.bsc.es/pid/pydock/get_pydock.html) .']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bioconductor-mixomics/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bioconductor-mixomics/lib/6.26.0\",\n    \"name\": \"bioconductor-mixomics\",\n    \"description\": [\n      \"Multivariate methods are well suited to large omics data sets where the number of variables (e.g. genes, proteins, metabolites) is much larger than the number of samples (patients, cells, mice). They have the appealing properties of reducing the dimension of the data by using instrumental variables (components), which are defined as combinations of all variables. Those components are then used to produce useful graphical outputs that enable better understanding of the relationships and correlation structures between the different data sets that are integrated. mixOmics offers a wide range of multivariate methods for the exploration and integration of biological datasets with a particular focus on variable selection. The package proposes several sparse multivariate models we have developed to identify the key variables that are highly correlated, and/or explain the biological outcome of interest. The data that can be analysed with mixOmics may come from high throughput sequencing technologies, such as omics data (transcriptomics, metabolomics, proteomics, metagenomics etc) but also beyond the realm of omics (e.g. spectral imaging). The methods implemented in mixOmics can also handle missing values without having to delete entire rows with missing data. A non exhaustive list of methods include variants of generalised Canonical Correlation Analysis, sparse Partial Least Squares and sparse Discriminant Analysis. Recently we implemented integrative methods to combine multiple data sets: N-integration with variants of Generalised Canonical Correlation Analysis and P-integration with variants of multi-group Partial Least Squares.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/3.18/bioc/html/mixOmics.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-2.0-or-later\",\n        \"url\": \"https://spdx.org/licenses/GPL-2.0-or-later.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/mixomics/lib/None\",\n    \"name\": \"mixomics\",\n    \"description\": [\n      \"The tool offers a wide range of multivariate methods for the exploration and integration of biological datasets with a particular focus on variable selection.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://mixomics.org/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Organization\",\n        \"name\": \"unimelb.edu.au\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Australia\",\n        \"email\": \"mixomics@math.univ-toulouse.fr\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8293510e2f3aa0b3b8333\"\n      },\n      {\n        \"$oid\": \"67c8293010e2f3aa0b3b8307\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/3.18/bioc/html/mixOmics.html:\n```\nContent:\n['This package is for version 3.18 of Bioconductor; for the stable, up-to-date release version, see [mixOmics](/packages/mixOmics/) . **Bioconductor version:** 3.18 Multivariate methods are well suited to large omics data sets where the number of variables (e.g. genes, proteins, metabolites) is much larger than the number of samples (patients, cells, mice). They have the appealing properties of reducing the dimension of the data by using instrumental variables (components), which are defined as combinations of all variables. Those components are then used to produce useful graphical outputs that enable better understanding of the relationships and correlation structures between the different data sets that are integrated. mixOmics offers a wide range of multivariate methods for the exploration and integration of biological datasets with a particular focus on variable selection. The package proposes several sparse multivariate models we have developed to identify the key variables that are highly correlated, and/or explain the biological outcome of interest. The data that can be analysed with mixOmics may come from high throughput sequencing technologies, such as omics data (transcriptomics, metabolomics, proteomics, metagenomics etc) but also beyond the realm of omics (e.g. spectral imaging). The methods implemented in mixOmics can also handle missing values without having to delete entire rows with missing data. A non exhaustive list of methods include variants of generalised Canonical Correlation Analysis, sparse Partial Least Squares and sparse Discriminant Analysis. Recently we implemented integrative methods to combine multiple data sets: N-integration with variants of Generalised Canonical Correlation Analysis and P-integration with variants of multi-group Partial Least Squares. **Author:** Kim-Anh Le Cao [aut], Florian Rohart [aut], Ignacio Gonzalez [aut], Sebastien Dejean [aut], Al J Abadi [ctb, cre], Max Bladen [ctb], Benoit Gautier [ctb], Francois Bartolo [ctb], Pierre Monget [ctb], Jeff Coquery [ctb], FangZou Yao [ctb], Benoit Liquet [ctb] **Maintainer:** Max Bladen <mbladen19 at gmail.com> **Citation (from within R, entercitation(\"mixOmics\")):** To view documentation for the version of this package installed in your system, start R and enter:']```"}, {"role": "user", "content": "Content from http://mixomics.org/:\n```\nContent:\n['mixOmics is an R package for exploring and integrating omics data, including transcriptomics, proteomics, lipidomics, microbiome, metagenomics and beyond. The mixOmics package includes tools for data integration, biomarker discovery, and data visualisation, using advanced multivariate methods to reduce data dimensionality and uncover relationships within and across datasets. **mixOmics offers a wide range of multivariate methods for the exploration and integration of biological datasets, with a specific focus on variable selection** . ###### Multivariate methods for Omics data **Multivariate methods** are well suited for large omics datasets with many variables (e.g., genes, proteins) and few samples (e.g., patients, cells). They reduce data dimensionality by creating components\u2014combinations of variables\u2014that reveal patterns and relationships between omics datasets. These components can be used to produce clear visualisations, uncover key variables linked to biological outcomes and integrate multi-omics datasets. Our **sparse** multivariate models can be used to identify the key variables that are highly correlated or explain the biological outcome of interest (e.g. disease status). mixOmics can be used to analyse a single omics dataset, or to integrate two or more datasets across samples (N-integration) or across variables (P-integration) ###### The mixOmics project The mixOmics R toolkit aims to bridge the gap between complex statistical methods and practical applications in biological and biomedical research, making advanced data analysis accessible to a diverse range of users, from statisticians to biologists. The [mixOmics team](https://mixomics.org/a-propos/the-team/) is led by [Prof. Kim-Anh L\u00ea Cao](https://lecao-lab.science.unimelb.edu.au) at the University of Melbourne and includes contributors from around the world. If you use mixOmics in your research, do not forget to [cite us](https://mixomics.org/cite-us/) ! Consider also supporting the ongoing development of mixOmics by purchasing our [book](https://mixomics.org/book/) , or joining our [workshops](https://mixomics.org/workshops/) !']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"mustang/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/mustang/cmd/None\",\n    \"name\": \"mustang\",\n    \"description\": [\n      \"Softwear for the alignment of multiple protein structures.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.csse.monash.edu.au/~karun/Site/mustang.html\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Arun Konagurthu\",\n        \"email\": \"arun.konagurthu@monash.edu\",\n        \"maintainer\": false,\n        \"url\": \"http://www.csse.monash.edu.au/~karun/Site/Home.html\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a27cd3b8acd1a2f9b525\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/mustang/cmd/3.2.3\",\n    \"name\": \"mustang\",\n    \"description\": [\n      \"Mustang is a program that implements an algorithm for structural alignment of multiple protein structures.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://lcb.infotech.monash.edu.au/mustang/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a27cd3b8acd1a2f9b525\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://www.csse.monash.edu.au/~karun/Site/mustang.html:\n```\nContent:\n['# This site can\u2019t be reached The connection was reset. ERR_CONNECTION_RESET Check any cables and reboot any routers, modems or other network devices you may be using. If it is already listed as a program allowed to access the network, try removing it from the list and adding it again. Go to Applications > System Preferences > Network > Advanced > Proxies and deselect any proxies that have been selected. The connection was reset.']```"}, {"role": "user", "content": "Content from http://lcb.infotech.monash.edu.au/mustang/:\n```\nContent:\n['##### Instructions for building MUSTANG Dependencies: GNUMake or equivalent. A modern C++ compiler. MUSTANG is known to build with G++ >=4.1.2. If these dependencies are met, follow these instructions: * Download the source code from the[link](mustang_v3.2.4.tgz)above. * Extract the archive with:tar -zxvf mustang_v3.2.4.tgz * Type:cd MUSTANG_v3.2.4 * Build MUSTANG with:make * The built binary will appear in thebin/subdirectory. * Test the binary by running fromMUSTANG_v3.2.4/directory:./bin/mustang-3.2.3 -f data/test/test_zf-CCHH * This should produce the following files:results.htmlresults.html ##### Copyright license Copyright (c) 2005-, Arun Konagurthu, Monash University. arun DOT konagurthu AT monash DOT edu http://lcb.infotech.monash.edu.au/mustang All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the University of Melbourne nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESSINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"consent/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/consent/cmd/2.2.2\",\n    \"name\": \"consent\",\n    \"description\": [\n      \"Scalable long read self-correction and assembly polishing with multiple sequence alignment.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/morispi/CONSENT\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/morispi/CONSENT\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"AGPL-3.0\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/consent/cmd/None\",\n    \"name\": \"consent\",\n    \"description\": [\n      \"Ligand-based virtual screening with consensus queries.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/UnixJunkie/consent\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Francois Berenger\",\n        \"email\": \"berenger@bioreg.kyushu-u.ac.jp\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7aac5d3b8acd1a2f9bd03\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/morispi/CONSENT:\n```\nREADME content:\n[\"# CONSENT CONSENT (Scalable long read self-correction and assembly polishing with multiple sequence alignment) is a self-correction method for long reads. It works by, first, computing overlaps between the long reads, in order to define an alignment pile (i.e. a set of overlapping reads used for correction) for each read. Each read's alignment pile is then further divided into smaller windows, that are corrected idependently. First, a multiple alignment strategy is used in order to compute consensus. Then, this consensus is further polished with a local de Bruijn graph, in order to get rid of the remaining errors. Additionally to error correction, CONSENT can also perform assembly polishing. Requirements -------------- - A Linux based operating system. - Python3. - g++, minimum version 5.5.0. - CMake, minimum version 2.8.2. - Minimap2 available through you path. Installation -------------- Clone the CONSENT repository, along with its submodules with: ```bash git clone --recursive https://github.com/morispi/CONSENT ``` Then run the install.sh script: ```bash ./install.sh ``` If you do not already have minimap2 available through your path, you can then run: ```bash export PATH=$PWD/minimap2:$PATH ``` CONSENT should then be able to run. Getting started -------------- An example dataset (10x of simulated PacBio reads, raw assembly, and reference genome) is provided in the `example` folder. Please run the following commands to try out CONSENT on this example. ### Self-correction To perform self-correction on the example dataset, run the following command: `./CONSENT-correct --in example/reads.fasta --out example/correctedReads.fasta --type PB` This should take about 2 min and use up to 750 MB of RAM, using 4 cores. ### Polishing To perform assembly polishing on the example dataset, run the following command: `./CONSENT-polish --contigs example/rawAssembly.fasta --reads example/reads.fasta --out example/polishedAssembly.fasta` This should take about 15 sec and use at most 150 MB of RAM, using 4 cores. Running CONSENT -------------- ### Self-correction To run CONSENT for long reads self-correction, run the following command: `./CONSENT-correct --in longReads.fast[a|q] --out result.fasta --type readsTechnology` - longReads.fast[a|q]: fasta or fastq file of long reads to . - result.fasta: fasta file where to output the corrected long reads. - readsTechnology: Indicate whether the long reads are from PacBio (--type PB) or Oxford Nanopore (--type ONT) ### Polishing To run CONSENT for assembly polishing, run the followning command: `./CONSENT-polish --contigs contigs.fast[a|q] --reads longReads.fast[a|q] --out result.fasta` - contigs.fast[a|q]: fasta or fastq file of contigs to polish. - longReads.fast[a|q]: fasta or fastq file of long reads to use for polishing. - result.fasta: fasta file where to output the polished contigs. ### Options --windowSize INT, -l INT: Size of the windows to process. (default: 500) --minSupport INT, -s INT: Minimum support to consider a window for correction. (default: 4) --maxSupport INT, -S INT: Maximum number of overlaps to include in a pile. (default: 150) --maxMSA INT, -M: Maximum number of sequences to include into the MSA. (default: 150) --merSize INT, -k INT: k-mer size for chaining and polishing. (default: 9) --solid INT, -f INT: Minimum number of occurrences to consider a k-mer as solid during polishing. (default: 4) --anchorSupport INT, -c INT: Minimum number of sequences supporting (Ai) - (Ai+1) to keep the two anchors in the chaining. (default: 8) --minAnchors INT, -a INT: Minimum number of anchors in a window to allow consensus computation. (default: 2) --windowOverlap INT, -o INT: Overlap size between consecutive windows. (default: 50) --nproc INT, -j INT: Number of processes to run in parallel (default: number of cores). --minimapIndex INT, -m INT: Split minimap2 index every INT input bases (default: 500M). --tmpdir STRING, -t STRING: Path where to store the temporary overlaps file (default: working directory, as Alignments_dateTimeStamp.paf). --help, -h: Print this help message. Notes -------------- CONSENT has been developed and tested on x86-64 GNU/Linux. Support for any other platform has not been tested. Authors -------------- Pierre Morisse, Camille Marchet, Antoine Limasset, Arnaud Lefebvre and Thierry Lecroq. Reference -------------- Morisse, P., Marchet, C., Limasset, A. et al. Scalable long read self-correction and assembly polishing with multiple sequence alignment. Sci Rep 11, 761 (2021). https://doi.org/10.1038/s41598-020-80757-5 Contact -------------- You can report problems and bugs to pierre[dot]morisse[at]inria[dot]fr\"]```"}, {"role": "user", "content": "Content from https://github.com/morispi/CONSENT:\n```\nRepository metadata:\n{'name': 'CONSENT', 'label': ['CONSENT'], 'description': [{'term': 'Scalable long read self-correction and assembly polishing with multiple sequence alignment', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://doi.org/10.1038/s41598-020-80757-5', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v2.2.2', 'v2.2.1', 'v2.2', 'v2.1', 'v2.0', 'v1.2', 'v1.1.2', 'v1.1.1', 'v1.1', 'v1.0'], 'license': [{'term': {'name': 'GNU Affero General Public License v3.0', 'url': 'http://choosealicense.com/licenses/agpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/morispi/CONSENT', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/ngs', 'term': 'ngs', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/correction', 'term': 'correction', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/long-reads', 'term': 'long-reads', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/long', 'term': 'long', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/reads', 'term': 'reads', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/self-correction', 'term': 'self-correction', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/self', 'term': 'self', 'vocabulary': ''}, 'id': 6}, {'term': {'uri': 'https://github.com/topics/msa', 'term': 'msa', 'vocabulary': ''}, 'id': 7}, {'term': {'uri': 'https://github.com/topics/multiple-sequence-alignment', 'term': 'multiple-sequence-alignment', 'vocabulary': ''}, 'id': 8}, {'term': {'uri': 'https://github.com/topics/consensus', 'term': 'consensus', 'vocabulary': ''}, 'id': 9}, {'term': {'uri': 'https://github.com/topics/contigs', 'term': 'contigs', 'vocabulary': ''}, 'id': 10}, {'term': {'uri': 'https://github.com/topics/assembly', 'term': 'assembly', 'vocabulary': ''}, 'id': 11}, {'term': {'uri': 'https://github.com/topics/polishing', 'term': 'polishing', 'vocabulary': ''}, 'id': 12}], 'operations': [], 'authors': [{'term': {'name': 'Pierre Morisse', 'type': 'person', 'email': 'contact@pierremorisse.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Pierre-Morisse', 'type': 'person', 'email': 'pierre.morisse2@univ-rouen.fr', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/morispi/CONSENT/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/UnixJunkie/consent:\n```\nREADME content:\n['# Consent Chemoinformatics software for Ligand-Based Virtual Screening (LBVS) using consensus queries. Cf. the INSTALL file for instructions on how to install consent. # I) Command line help lbvs_consent -s {sing|oppo|opti|real|know} -q queries.{sdf|mol2|csv|ecfp4} -db candidates.{sdf|mol2|csv|ecfp4} -s <pol> consensus policy {sing|oppo|opti|real|know} (mandatory) -q <filename> queries file (known actives; mandatory) -db <filename> database to rank order (mandatory) -o <filename> where to write scores (can be combined with -top) -n <int> consensus size; #known actives used to create query (optional; default=all molecules in query file) -top <int> how many top scoring molecules to write out (optional; default=all; must be combined with -o) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1006728.svg)](https://doi.org/10.5281/zenodo.1006728) # II) Usage recommendation Please cite the corresponding paper (https://doi.org/10.1186/s13321-017-0248-5) in case you use this software and publish about your results (Consensus queries in ligand-based virtual screening experiments. F. Berenger, O. Vu and J., Meiler. Journal of Cheminformatics, November 2017). The opportunist consensus policy (-s oppo) is recommended. It works well with any fingerprint and is usually the best performing method. However, if you really need to go faster, here are some recommendations: - MACCS fingerprint (166 bits): use the realistic policy (-s real); it will average the MACCS fingerprints of your known actives. - ECFP4 fingerprint (2048 bits; folded; uncounted): use the optimist policy (-s opti); it will do a logical union of the fingerprints of your known actives. - UMOP2D (unfolded MOLPRINT2D; uncounted): same as for ECFP4, use -s opti. # III) How to encode your molecules First, we need some SDF and MOL2 files. The obabel command is provided by the Open Babel package (cf. http://openbabel.org). obabel data/ARm_actives.smi -O data/ARm_actives.sdf obabel data/ARm_inactives.smi -O data/ARm_inactives.sdf obabel data/ARm_actives.smi -O data/ARm_actives.mol2 obabel data/ARm_inactives.smi -O data/ARm_inactives.mol2 cat data/ARm_actives.mol2 data/ARm_inactives.mol2 > data/ARm_database.mol2 ## With the MACCS fingerprint lbvs_consent_ob_maccs data/ARm_actives.sdf > data/ARm_actives.maccs lbvs_consent_ob_maccs data/ARm_inactives.sdf > data/ARm_inactives.maccs cat data/ARm_actives.maccs data/ARm_inactives.maccs > data/ARm_database.maccs ## With the ECFP4 fingerprint lbvs_consent_ecfp4.py data/ARm_actives.sdf > data/ARm_actives.ecfp4 lbvs_consent_ecfp4.py data/ARm_inactives.sdf > data/ARm_inactives.ecfp4 cat data/ARm_actives.ecfp4 data/ARm_inactives.ecfp4 > data/ARm_database.ecfp4 ## With the UMOP2D fingerprint lbvs_consent_mop2di -i data/ARm_database.mol2 > data/ARm_database.idx lbvs_consent_mop2de -idx data/ARm_database.idx -i data/ARm_database.mol2 -o data/ARm_database.mop2d # IV) How to query with a consensus query and a consensus policy # example with ECFP4 fingerprints and 20 actives head -20 data/ARm_actives.ecfp4 > data/ARm_query_20.ecfp4 # recommended way; AUC ~= 0.60 lbvs_consent -s oppo -q data/ARm_query_20.ecfp4 -db data/ARm_database.ecfp4 -o scores.txt # faster, but still with good performance in many cases; AUC ~= 0.61 lbvs_consent -s opti -q data/ARm_query_20.ecfp4 -db data/ARm_database.ecfp4 -o scores.txt']```"}, {"role": "user", "content": "Content from https://github.com/UnixJunkie/consent:\n```\nRepository metadata:\n{'name': 'consent', 'label': ['consent'], 'description': [{'term': 'ligand-based virtual screening with consensus queries', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.0.1'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/UnixJunkie/consent', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/chemoinformatics', 'term': 'chemoinformatics', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/ligand', 'term': 'ligand', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/lbvs', 'term': 'lbvs', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/consensus', 'term': 'consensus', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/chemical', 'term': 'chemical', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/fingerprint', 'term': 'fingerprint', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/maccs', 'term': 'maccs', 'vocabulary': ''}, 'id': 6}, {'term': {'uri': 'https://github.com/topics/ecfp4', 'term': 'ecfp4', 'vocabulary': ''}, 'id': 7}, {'term': {'uri': 'https://github.com/topics/ocaml', 'term': 'ocaml', 'vocabulary': ''}, 'id': 8}, {'term': {'uri': 'https://github.com/topics/molprint2d', 'term': 'molprint2d', 'vocabulary': ''}, 'id': 9}], 'operations': [], 'authors': [{'term': {'name': 'Francois Berenger', 'type': 'person', 'email': 'unixjunkie@sdf.org', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Jan Midtgaard', 'type': 'person', 'email': 'mail@janmidtgaard.dk', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Francois BERENGER', 'type': 'person', 'email': 'ligand@free.fr', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Francois BERENGER', 'type': 'person', 'email': 'francois.berenger@inria.fr', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/UnixJunkie/consent/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"mendelscan/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"github/mendelscan/None/1.2.2\",\n    \"name\": \"mendelscan\",\n    \"description\": [\n      \"Analyze exome data for Mendelian disorders. Still in alpha-testing.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/genome/mendelscan\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://gmt.genome.wustl.edu/mendelscan/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"github\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"sourceforge/mendelscan/None/None\",\n    \"name\": \"mendelscan\",\n    \"description\": [\n      \"MendelScan is a tool for prioritizing candidate variants in family-based studies of inherited disease.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://sourceforge.net/projects/mendelscan\",\n        \"kind\": \"sourceforge\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://mendelscan.sourceforge.io/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"sourceforge\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://mendelscan.sourceforge.io/:\n```\nProject metadata:\n{'description': 'MendelScan is a tool for prioritizing candidate variants in family-based studies of inherited disease.', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['https://sourceforge.net/auth/?return_to=%2Fprojects%2Fmendelscan%2F', '/p/mendelscan/activity/', '/p/mendelscan/activity/feed']}, {'text': 'Categories\\nBio-Informatics\\nLicense\\nCreative Commons Attribution Non-Commercial License V2.0\\nCategories\\nBio-Informatics\\nLicense\\nCreative Commons Attribution Non-Commercial License V2.0', 'hrefs': ['/directory/bio-informatics/']}, {'text': 'Follow MendelScan\\nMendelScan Web Site\\nFollow MendelScan\\nMendelScan Web Site', 'hrefs': ['https://mendelscan.sourceforge.io']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof MendelScan!\\nUser Reviews\\nBe the first to\\npost a review\\nof MendelScan!', 'hrefs': ['/projects/mendelscan/reviews/new']}, {'text': 'Additional Project Details\\nIntended Audience\\nScience/Research\\nUser Interface\\nCommand-line\\nProgramming Language\\nJava\\nRelated Categories\\nJava Bio-Informatics Software\\nRegistered\\n2013-08-12\\nAdditional Project Details\\nIntended Audience\\nScience/Research\\nUser Interface\\nCommand-line\\nProgramming Language\\nJava\\nRelated Categories\\nJava Bio-Informatics Software\\nRegistered\\n2013-08-12', 'hrefs': ['/directory/java/', '/directory/bio-informatics/java/']}]}```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/mendelscan:\n```\nProject metadata:\n{'description': 'MendelScan is a tool for prioritizing candidate variants in family-based studies of inherited disease.', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['https://sourceforge.net/auth/?return_to=%2Fprojects%2Fmendelscan%2F', '/p/mendelscan/activity/', '/p/mendelscan/activity/feed']}, {'text': 'Categories\\nBio-Informatics\\nLicense\\nCreative Commons Attribution Non-Commercial License V2.0\\nCategories\\nBio-Informatics\\nLicense\\nCreative Commons Attribution Non-Commercial License V2.0', 'hrefs': ['/directory/bio-informatics/']}, {'text': 'Follow MendelScan\\nMendelScan Web Site\\nFollow MendelScan\\nMendelScan Web Site', 'hrefs': ['https://mendelscan.sourceforge.io']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof MendelScan!\\nUser Reviews\\nBe the first to\\npost a review\\nof MendelScan!', 'hrefs': ['/projects/mendelscan/reviews/new']}, {'text': 'Additional Project Details\\nIntended Audience\\nScience/Research\\nUser Interface\\nCommand-line\\nProgramming Language\\nJava\\nRelated Categories\\nJava Bio-Informatics Software\\nRegistered\\n2013-08-12\\nAdditional Project Details\\nIntended Audience\\nScience/Research\\nUser Interface\\nCommand-line\\nProgramming Language\\nJava\\nRelated Categories\\nJava Bio-Informatics Software\\nRegistered\\n2013-08-12', 'hrefs': ['/directory/java/', '/directory/bio-informatics/java/']}]}```"}, {"role": "user", "content": "Content from https://github.com/genome/mendelscan:\n```\nREADME content:\n['# MendelScan ## Overview Please see the official site for MendelScan, on [GMT][], for detailed information. [GMT]: http://gmt.genome.wustl.edu/mendelscan/ ## Releases Please see the [releases][] page for official releases. ## Build You can build the Java source by running `ant compile` or a JAR file by running `ant dist`. [releases]: https://github.com/genome/mendelscan/releases ## Run You can run the JAR file on the commandline like any Java JAR file, e.g. `java -jar ./MendelScan.jar`. ## Source Documentation Javadoc is published on the GitHub [page][gh-pages]. [gh-pages]: http://genome.github.io/mendelscan/ ## Example Included in the repository is an example data set using [1000 Genomes][] data. tar -zxvf example_data.tar.gz cd example_data java -jar MendelScan.jar score variants.vcf \\\\ --vep-file annotation.vep --ped-file family.ped --gene-file gene-expression.txt \\\\ --output-file mendelscan.tsv --output-vcf mendelscan.vcf [1000 Genomes]: http://www.1000genomes.org']```"}, {"role": "user", "content": "Content from https://github.com/genome/mendelscan:\n```\nRepository metadata:\n{'name': 'mendelscan', 'label': ['mendelscan'], 'description': [{'term': 'Analyze exome data for Mendelian disorders. Still in alpha-testing.', 'id': 0}], 'links': [], 'webpage': [{'term': 'http://gmt.genome.wustl.edu/mendelscan/', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['1.2.2', 'v1.2.1', 'v1.2.0', 'v1.1.1.0'], 'license': [], 'repository': [{'term': 'https://github.com/genome/mendelscan', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'dkoboldt', 'type': 'person', 'email': 'dkoboldt@genome.wustl.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Dan Koboldt', 'type': 'person', 'email': 'dkoboldt@watson.wustl.edu', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Jared Tobin', 'type': 'person', 'email': 'jared@jtobin.ca', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Adam Coffman', 'type': 'person', 'email': 'acoffman@genome.wustl.edu', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Nathan Nutter', 'type': 'person', 'email': 'iam@nnutter.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Nathan Nutter', 'type': 'person', 'email': 'nnutter@genome.wustl.edu', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/genome/mendelscan/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://gmt.genome.wustl.edu/mendelscan/:\n```\nContent:\n['This is the initial release of MendelScan, a software package for analyzing variants identified by family-based sequencing of rare genetic diseases. There are three main functions included with this release: java -jar MendelScan.jar score \u2013 prioritize variants in a VCF based upon segregation, annotation, population frequency, and gene expression java -jar MendelScan.jar rhro \u2013 apply rare heterozygote rule out to map dominant disease genes java -jar MendelScan.jar sibd \u2013 apply shared IBD analysis to map disease genes A manuscript describing this tool has been submitted; in the interim, please cite MendelScan by noting the version number and citing its home URL: http://gmt.genome.wustl.edu/mendelscan. You can [download this release on Github.](https://github.com/genome/mendelscan/releases/tag/v1.1.1.0)']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"snakemake/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/snakemake/cmd/8.5.3\",\n    \"name\": \"snakemake\",\n    \"description\": [\n      \"Snakemake is a workflow management system that aims to reduce the complexity of creating \\nworkflows by providing a fast and comfortable execution environment, together with a clean \\nand modern specification language in python style. Snakemake workflows are essentially Python \\nscripts extended by declarative code to define rules. Rules describe how to create output \\nfiles from input files.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://snakemake.github.io/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a48ed3b8acd1a2f9b58b\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/snakemake/cmd/None\",\n    \"name\": \"snakemake\",\n    \"description\": [\n      \"Workflow engine and language. It aims to reduce the complexity of creating workflows by providing a fast and comfortable execution environment, together with a clean and modern domain specific specification language (DSL) in python style.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://snakemake.readthedocs.io/en/stable/index.html\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"http://johanneskoester.bitbucket.org/\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"https://groups.google.com/forum/#!forum/snakemake\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a48ed3b8acd1a2f9b58b\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://snakemake.readthedocs.io/en/stable/index.html:\n```\nContent:\n['<https://gitpod.io/#https://github.com/snakemake/snakemake> <https://bioconda.github.io/recipes/snakemake/README.html> <https://www.python.org> <https://pypi.python.org/pypi/snakemake> <https://hub.docker.com/r/snakemake/snakemake> <https://github.com/snakemake/snakemake/actions?query=branch%3Amain+workflow%3ACI> <https://stackoverflow.com/questions/tagged/snakemake> <https://discord.gg/NUdMtmr> <https://bsky.app/profile/johanneskoester.bsky.social> <https://fosstodon.org/@johanneskoester> <https://github.com/snakemake/snakemake/stargazers> The Snakemake workflow management system is a tool to create **reproducible and scalable** data analyses. Workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition. Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment. Snakemake is **highly popular** , with [>10 new citations per week](https://badge.dimensions.ai/details/id/pub.1018944052) . For an introduction, please visit [https://snakemake.github.io](https://snakemake.github.io) . When using Snakemake, please cite our \u201crolling\u201d paper [M\u00f6lder, F., Jablonski, K.P., Letcher, B., Hall, M.B., Tomkins-Tinch, C.H., Sochat, V., Forster, J., Lee, S., Twardziok, S.O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., K\u00f6ster, J., 2021. Sustainable data analysis with Snakemake. F1000Res 10, 33.](https://doi.org/10.12688/f1000research.29032.1) This paper will also be regularly updated when Snakemake receives new features. See [Citations](project_info/citations.html) for more information. The Snakemake maintainers are: [Snakemake Wrappers Repository](https://snakemake-wrappers.readthedocs.org) The Snakemake Wrapper Repository is a collection of reusable wrappers that allow to quickly use popular tools from Snakemake rules and workflows. [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog) An automatically scraped catalog of publicly available Snakemake workflows for any kind of data analysis. [Snakemake Workflows Project](https://github.com/snakemake-workflows/docs) This project provides a collection of high quality modularized and reusable workflows. The provided code should also serve as a best-practices of how to build production ready workflows with Snakemake. Everybody is invited to contribute. [Snakemake Profiles Project](https://github.com/snakemake-profiles/doc) This project provides Snakemake configuration profiles for various execution environments. Please consider contributing your own if it is still missing. [Snakemake API documentation](https://snakemake-api.readthedocs.io) The documentation of the Snakemake API for programmatic access and development on Snakemake. [Conda-Forge](https://conda-forge.org) Conda-Forge is a community driven distribution of Conda packages that can be used from Snakemake for creating completely reproducible workflows by defining the used software versions and providing binaries. [Bioconda](https://bioconda.github.io/) Bioconda, a partner project of conda-forge, is a community driven distribution of bioinformatics-related Conda packages that can be used from Snakemake for creating completely reproducible workflows by defining the used software versions and providing binaries.']```"}, {"role": "user", "content": "Content from https://snakemake.github.io/:\n```\nContent:\n['Snakemake has a powerful [plugin system](https://snakemake.github.io/snakemake-plugin-catalog/index.html) that allows to extend various functionalities with alternative implementations. Via stable and well-defined interfaces, plugins can evolve independently of Snakemake, and mutual update requirements are minimized. Currently, execution backends and remote storage support is implemented via plugins. In the future, we will extend this to other areas, such as workflow scheduling, reporting, software deployment, and more.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"sina/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/sina/cmd/1.7.2\",\n    \"name\": \"sina\",\n    \"description\": [\n      \"SINA allows incorporating additional sequences into an existing\\nmultiple sequence alignment (MSA) without modifying the original\\nalignment. While adding sequences to an MSA with SINA is usually\\nfaster than re-computing the entire MSA from an augmented set of\\nunaligned sequences, the primary benefit lies in protecting\\ninvestments made into the original MSA such as manual curation of the\\nalignment, compute intensive phylogenetic tree reconstruction and\\ntaxonomic annotation of the resulting phylogeny.\\n\\nAdditionally, SINA includes a homology search which uses the\\npreviously computed alignment to determine the most similar\\nsequences. Based on the search results, a LCA based classification of\\nthe query sequence can be computed using taxonomic classifications\\nassigned to the sequences comprising the reference MSA.\\n\\nSINA is used to compute the small and large subunit ribosomal RNA\\nalignments provided by the SILVA_ project and is able to use the ARB_\\nformat reference databases released by the project.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/epruesse/SINA\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/epruesse/SINA\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a4a8d3b8acd1a2f9b594\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/sina/web/1.3.1\",\n    \"name\": \"sina\",\n    \"description\": [\n      \"Aligns and optionally taxonomically classifies your rRNA gene sequences.\\n\\nReference based multiple sequence alignment.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.arb-silva.de/aligner\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"contact@arb-silva.de\",\n        \"maintainer\": false,\n        \"url\": \"http://www.arb-silva.de/contact/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a4a8d3b8acd1a2f9b594\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://www.arb-silva.de/aligner:\n```\nContent:\n['### **SINA Aligner** SINA (v1.2.12) will align your rRNA gene sequences accoding to the global SILVA alignment for rRNA genes. The results can be combined with any other sequences aligned by SINA or taken from the SILVA databases by concatenation of FASTA files or using the ARB MERGE tool. ### SINA Search and Classify Enabling \"Search and classify\" will force SINA to additionally classify your sequences with the least common ancestor (LCA) method based on the taxonomies hosted by SILVA. ### Compute Tree with RAxML or FastTree Enabling \"Compute tree\" will reconstruct a phylogenetic tree based on your aligned sequences, plus optionally their neighbours, using either [FastTree](http://www.microbesonline.org/fasttree/) or [RAxML](https://sco.h-its.org/exelixis/web/software/raxml/) . ### Find and download related sequences (nearest neighbours) Activating \"Search and Classify\" will return a set of sequences that have a certain identity with your submitted sequence in the SSU/LSU datasets. By adding these sequenced to the Cart they can be downloaded. ### Documentation [SILVA ACT Tutorial](https://www.arb-silva.de/documentation/act-tutorial/) ### Note The SINA aligner and classification system is currently limited to 1000 sequences of at most 6000 bases each. If your requirements exceed this limitation, get [SINA for local installation](https://www.arb-silva.de/aligner/sina-download/) . When reconstructing a tree, there is a limit of 500 sequences for RAxML and 1000 sequences for FastTree. These limits include neighbour sequences, so please ensure that the number of your input sequences times the maximum number of neighbours does not exceed this limit. **Pruesse, E., Peplies, J. and Gl\u00f6ckner, F.O.** (2012) SINA: accurate high-throughput multiple sequence alignment of ribosomal RNA genes. [Bioinformatics, 28, 1823-1829](http://bioinformatics.oxfordjournals.org/content/28/14/1823.long)']```"}, {"role": "user", "content": "Content from https://github.com/epruesse/SINA:\n```\nREADME content:\n[\"SINA - reference based multiple sequence alignment ================================================== |latest| |Bioconda| |downloads| |TravisCI| |CircleCI| |Read the Docs| |Codecov| .. |latest| image:: https://img.shields.io/github/release/epruesse/SINA/all.svg?label=latest .. |release| image:: https://img.shields.io/github/release/epruesse/SINA.svg .. |Bioconda| image:: https://img.shields.io/conda/vn/Bioconda/sina.svg :target: https://bioconda.github.io/recipes/sina/README.html .. |TravisCI| image:: https://img.shields.io/travis/epruesse/SINA.svg?label=build%20(TravisCI) :target: https://travis-ci.org/epruesse/SINA .. |CircleCI| image:: https://img.shields.io/circleci/project/github/epruesse/SINA.svg?label=build%20(CircleCI) :target: https://circleci.com/gh/epruesse/SINA .. |Codecov| image:: https://img.shields.io/codecov/c/github/epruesse/sina.svg :target: https://codecov.io/gh/epruesse/SINA .. |Read the Docs| image:: https://img.shields.io/readthedocs/sina/latest.svg :target: https://readthedocs.org/projects/sina/builds .. |downloads| image:: https://img.shields.io/conda/dn/bioconda/sina.svg?style=flat SINA aligns nucleotide sequences to match a pre-existing MSA using a graph based alignment algorithm similar to PoA. The graph approach allows SINA to incorporate information from many reference sequences building without blurring highly variable regions. While pure NAST implementations depend highly on finding a good match in the reference database, SINA is able to align sequences relatively distant to references with good quality and will yield a robust result for query sequences with many close reference. Features -------- - Speed. Aligning 100,000 full length rRNA against the SILVA NR takes 40 minutes on a mid-sized 2018 desktop computer. Aligning 1,000,000 V4 amplicons takes about 60 minutes. - Accuracy. SINA is used to build the SILVA_ SSU and LSU rRNA databases. - Classification. SINA includes an LCA based classification module. - ARB. SINA is able to directly read and write ARB_ format files such as distributed by the SILVA_ project. .. _SILVA: https://www.arb-silva.de .. _ARB: https://www.arb-home.de Online Version -------------- An online version for submitting small batches of sequences is made available by the SILVA_ project as part of their `ACT: Alignment, Classification and Tree Service <https://www.arb-silva.de/aligner>`_. In addition to SINA's alignment and classification stages, ACT allows directly building phylogenetic trees with RAxML or FastTree from your sequences and (optionally) additional sequences chosen using SINA's add-neighbors feature. Installing SINA --------------- The preferred way to install SINA locally is via `Bioconda <https://bioconda.github.io>`_. If you have a working Bioconda installation, just run:: conda create -n sina sina conda activate sina Alternatively, self-contained images are available at https://github.com/epruesse/SINA/releases. Choose the most recent ``tar.gz`` appropriate for your operating system and unpack:: tar xf sina-1.7.3-dev-dev-linux.tar.gz cd sina-1.7.3-dev-dev ./sina Documentation ------------- The full documentation is available at https://sina.readthedocs.io. The algorithm is explained in the paper: Elmar Pruesse, J\u00f6rg Peplies, Frank Oliver Gl\u00f6ckner; *SINA: Accurate high-throughput multiple sequence alignment of ribosomal RNA genes.* Bioinformatics 2012; 28 (14): 1823-1829. `doi:10.1093/bioinformatics/bts252 <https://doi.org/10.1093/bioinformatics/bts252>`_\"]```"}, {"role": "user", "content": "Content from https://github.com/epruesse/SINA:\n```\nRepository metadata:\n{'name': 'SINA', 'label': ['SINA'], 'description': [{'term': 'SINA - Reference based multiple sequence alignment', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://sina.readthedocs.io', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.7.2', 'v1.7.1', 'v1.7.0', 'v1.6.1', 'v1.6.0', 'v1.6.0-rc.1', 'v1.5.0', 'v1.4.0', 'v1.4.0-rc', 'v1.3.5', 'v1.3.4', 'v1.3.3', 'v1.3.2', 'v1.3.1', 'v1.3.1-a5'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/epruesse/SINA', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/rrna', 'term': 'rrna', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/alignment', 'term': 'alignment', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/sequence', 'term': 'sequence', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/16s', 'term': '16s', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/silva', 'term': 'silva', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 5}], 'operations': [], 'authors': [{'term': {'name': 'Elmar Pruesse', 'type': 'person', 'email': 'elmar.pruesse@ucdenver.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Elmar Pruesse', 'type': 'person', 'email': 'epruesse@users.noreply.github.com', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/epruesse/SINA/blob/main/arb_LICENSE.txt'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/epruesse/SINA/blob/main/arb_PUBLIB_LICENSE.txt'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"metabuli/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/metabuli/cmd/1.0.3\",\n    \"name\": \"metabuli\",\n    \"description\": [\n      \"Metabuli: specific and sensitive metagenomic classification via joint analysis of DNA and amino acid.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/steineggerlab/Metabuli\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/steineggerlab/Metabuli\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/metabuli/cmd/None\",\n    \"name\": \"metabuli\",\n    \"description\": [\n      \"Metabuli: specific and sensitive metagenomic classification via joint analysis of DNA and amino acid.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://metabuli.steineggerlab.com/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c837c110e2f3aa0b3bcce4\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://metabuli.steineggerlab.com/:\n```\nREADME content:\n['[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/metabuli/README.html) ![Platform](https://img.shields.io/badge/platform-Mac%20%7C%20Windows%20%7C%20Linux-brightgreen) # Metabuli ***Metabuli*** classifies metagenomic reads by comparing them to reference genomes. You can use Metabuli to profile the taxonomic composition of your samples or to detect specific (pathogenic) species. ***Sensitive and Specific.*** Metabuli uses a novel k-mer structure, called *metamer*, to analyze both amino acid (AA) and DNA sequences. It leverages AA conservation for sensitive homology detection and DNA mutations for specific differentiation between closely related taxa. ***A laptop is enough.*** Metabuli operates within user-specified RAM limits, allowing it to search any database that fits in storage. A PC with 8 GiB of RAM is sufficient for most analyses. ***A few clicks are enough.*** Metabuli App is now available [here](https://github.com/steineggerlab/Metabuli-App). With just a few clicks, you can run Metabuli and browse the results with Sankey and Krona plots on your PC. ***Short reads, long reads, and contigs.*** Metabuli can classify all types of sequences. --- For more details, please see [Nature Methods](https://www.nature.com/articles/s41592-024-02273-y), [PDF](https://www.nature.com/articles/s41592-024-02273-y.epdf?sharing_token=je_2D5Su0-xVOSjuKSAXF9RgN0jAjWel9jnR3ZoTv0M7gE7NDF_xi_3sW8QdRiwfSJNwqaXItSoeCvr7cvcoQxKLt0oROgWc6urmki9tP80cXEuHPN0D7b4y9y3i8Yv7sZw8MxxhAj7W6p9eZE2zaK3eozdOkXvwADVfso9cXIM%3D), [bioRxiv](https://www.biorxiv.org/content/10.1101/2023.05.31.543018v2), or [ISMB 2023 talk](https://www.youtube.com/watch?v=vz2fuRcVwyk). Please cite: [Kim J, Steinegger M. Metabuli: sensitive and specific metagenomic classification via joint analysis of amino acid and DNA. Nature Methods (2024).](https://doi.org/10.1038/s41592-024-02273-y) --- ### \ud83d\udda5\ufe0f [Metabuli App](https://github.com/steineggerlab/Metabuli-App) for Windows, MacOS, and Linux are now available! > Run taxonomic profiling in just a few clicks and explore results with Sankey and Krona plots. > Download the app for your OS [here](https://github.com/steineggerlab/Metabuli-App/releases)\u2014no separate Metabuli installation needed. <p align=\"center\"><img src=\"https://raw.githubusercontent.com/jaebeom-kim/Metabuli/master/.github/metabuli.jpg\" style=\"max-height: 500px; width: auto;\" /></p> --- ### Update in v1.1.0 - Fix errors in v1.0.9 - Custom DB creation became easier - Improve `updateDB` command ### Update in v1.0.9 - DB creation process improved - `updateDB` module to add new sequences to an existing database. - Users can provide CDS information to skip Prodigal\\'s gene prediction. - `--max-ram` parameter added to `build` module. - Compatibility with taxdump files generated using [taxonkit](https://bioinf.shenwei.me/taxonkit/). - Please check release note for details. ### Update in v1.0.8 - Added `extract` module to extract reads classified into a certain taxon. ### Update in v1.0.7 - **Metabuli became faster \ud83d\ude80** - Windows: *8.3* times faster - MacOS: *1.7* times faster - Linux: *1.3* times faster - Test details are in release note. - Fixed a bug in score calculation that could affect classification results. --- ## Table of Contents - [Installation](#installation) - [Precompiled binaries](#precompiled-binaries) - [Compile from source code](#compile-from-source-code) - [Pre-built databases](#pre-built-databases) - [Classification](#classification) - [Extract](#extract) - [GTDB-based custom database](#gtdb-based-custom-database) - [Creat a new database](#creat-a-new-database) - [Add new sequences to an existing database](#add-new-sequences-to-an-existing-database) - [NCBI or custom taxonomy based database](#ncbi-or-custom-taxonomy-based-database) - [Creat a new database](#creat-a-new-database-1) - [Add new sequences to an existing database](#add-new-sequences-to-an-existing-database-1) - [Example](#example) ## Installation ### Precompiled binaries ``` # install via conda conda install -c conda-forge -c bioconda metabuli # Linux AVX2 build (fast, recommended for most Linux system # check using: cat /proc/cpuinfo | grep avx2) wget https://mmseqs.com/metabuli/metabuli-linux-avx2.tar.gz; tar xvzf metabuli-linux-avx2.tar.gz; export PATH=$(pwd)/metabuli/bin/:$PATH # Linux SSE2 build (slower, for old systems) wget https://mmseqs.com/metabuli/metabuli-linux-sse2.tar.gz; tar xvzf metabuli-linux-sse2.tar.gz; export PATH=$(pwd)/metabuli/bin/:$PATH # MacOS (Universal, works on Apple Silicon and Intel Macs) wget https://mmseqs.com/metabuli/metabuli-osx-universal.tar.gz; tar xvzf metabuli-osx-universal.tar.gz; export PATH=$(pwd)/metabuli/bin/:$PATH ``` Metabuli also works on Linux ARM64 and Windows systems. Please check [https://mmseqs.com/metabuli](https://mmseqs.com/metabuli) for static builds for other architectures. ### Compile from source code ``` git clone https://github.com/steineggerlab/Metabuli.git cd Metabuli mkdir build && cd build cmake -DCMAKE_BUILD_TYPE=Release .. make -j 16 ``` The built binary can be found in `./build/src`. --- ## Pre-built databases You can download [pre-built databases](https://metabuli.steineggerlab.workers.dev/) using `databases` workflow. NOTE: The `databases` workflow may not work if you don\\'t use the latest version of Metabuli. In that case, please manually download databases from this [link](https://metabuli.steineggerlab.workers.dev/). ``` Usage: metabuli databases DB_NAME OUTDIR tmp # NOTE - A human genome (T2T-CHM13v2.0) is included in all databases below. 1. RefSeq Virus (8.1 GiB) - NCBI RefSeq release 223 virus genomes - Database will be in OUT_DIR/refseq_virus metabuli databases RefSeq_virus OUT_DIR tmp 2. RefSeq Prokaryote and Virus (115.6 GiB) - RefSeq prokaryote genomes (Complete Genome/Chromosome, 2024-03-26) + RefSeq Virus above. - Database will be in OUT_DIR/refseq_prokaryote_virus metabuli databases RefSeq OUTDIR tmp 3. GTDB (101 GiB) - GTDB 214.1 (Complete Genome/Chromosome, CheckM completeness > 90 and contamination < 5). - Database will be in OUT_DIR/gtdb metabuli databases GTDB OUTDIR tmp 4. RefSeq Releases 224 (619 GiB) - Viral and prokaryotic genomes of RefSeq release 224. metabuli databases RefSeq_release OUTDIR tmp ``` Downloaded files are stored in `OUTDIR/DB_NAME` directory, which can be provided for `classify` module as `DBDIR`. --- ## Classification ``` metabuli classify <i:FASTA/Q> <i:DBDIR> <o:OUTDIR> <Job ID> [options] - INPUT : FASTA/Q file of reads you want to classify. (gzip supported) - DBDIR : The directory of reference DB. - OUTDIR : The directory where the result files will be generated. - Job ID: It will be the prefix of result files. # Paired-end metabuli classify read_1.fna read_2.fna dbdir outdir jobid # Single-end metabuli classify --seq-mode 1 read.fna dbdir outdir jobid # Long-read metabuli classify --seq-mode 3 read.fna dbdir outdir jobid * Important parameters: --threads : The number of threads used (all by default) --max-ram : The maximum RAM usage. (128 GiB by default) --min-score : The minimum score to be classified --min-sp-score : The minimum score to be classified at or below species rank. --taxonomy-path: Directory where the taxonomy dump files are stored. (DBDIR/taxonomy by default) --accession-level : Set 1 to use accession level classification (0 by default). It is available when the DB is also built with accession level taxonomy. ``` - Paratemers for precision mode (Metabuli-P) - Illumina short reads: `--min-score 0.15 --min-sp-score 0.5` - PacBio HiFi reads: `--min-score 0.07 --min-sp-score 0.3` - PacBio Sequel II reads: `--min-score 0.005` - ONT reads: `--min-score 0.008` This will generate three result files: `JobID_classifications.tsv`, `JobID_report.tsv`, and `JobID_krona.html`. > Sankey diagram is available in the [GUI app](https://github.com/steineggerlab/Metabuli-App). #### JobID_classifications.tsv 1. Classified or not 2. Read ID 3. Taxonomy identifier 4. Effective read length 5. DNA level identity score 6. Classification Rank 7. List of \"taxID : k-mer match count\" ``` 1 read_1 2688 294 0.627551 subspecies 2688:65 1 read_2 2688 294 0.816327 subspecies 2688:78 0 read_3 0 294 0 no rank ``` #### JobID_report.tsv The proportion of reads that are assigned to each taxon. ``` 33.73 77571 77571 0 no rank unclassified 66.27 152429 132 1 no rank root 64.05 147319 2021 8034 superkingdom d__Bacteria 22.22 51102 3 22784 phylum p__Firmicutes 22.07 50752 361 22785 class c__Bacilli 17.12 39382 57 123658 order o__Bacillales 15.81 36359 3 126766 family f__Bacillaceae 15.79 36312 26613 126767 genus g__Bacillus 2.47 5677 4115 170517 species s__Bacillus amyloliquefaciens 0.38 883 883 170531 subspecies RS_GCF_001705195.1 0.16 360 360 170523 subspecies RS_GCF_003868675.1 ``` #### JobID_krona.html It is for an interactive taxonomy report (Krona). You can use any modern web browser to open `JobID_krona.html`. <p align=\"left\"><img src=\"https://raw.githubusercontent.com/steineggerlab/Metabuli/master/.github/image.png\" style=\"max-height: 350px; width: auto;\" /></p> ### Resource requirements Metabuli can classify reads against a database of any size as long as the database is fits in the hard disk, regardless of the machine\\'s RAM size. We tested it with a MacBook Air (2020, M1, 8 GiB), where we classified about 15 M paired-end 150 bp reads (~5 GiB in size) against a database built with ~23K prokaryotic genomes (~69 GiB in size). --- ## Extract After running the `classify` command, you can extract reads that are classified under a specific taxon. This requires the FASTA/Q files used in the `classify` step and the `JobID_classifications.tsv` file, which is generated as one of the output files. ``` metabuli extract <i:FASTA/Q> <i:read-by-read classification> <i:DBDIR> --tax-id TAX_ID - FASTA/Q : The FASTA/Q file(s) used during the `classify` step. - read-by-read classification : The JobID_classifications.tsv file generated by the `classify` step. - DBDIR : The same DBDIR used in the `classify` step. - TAX_ID : The taxonomy ID of the taxon at any rank (e.g., species, genus) from which you want to extract the reads. # Paired-end metabuli extract read_1.fna read_2.fna JobID_classifications.tsv dbdir --tax-id TAX_ID # Single-end metabuli extract --seq-mode 1 read.fna JobID_classifications.tsv dbdir --tax-id TAX_ID # Long-read metabuli extract --seq-mode 3 read.fna JobID_classifications.tsv dbdir --tax-id TAX_ID ``` #### Output - For paired-end samples: `read_1_TAX_ID.fna` and `read_2_TAX_ID.fna` - For single-end or long-read samples: `read_TAX_ID.fna` --- ## GTDB-based custom database > ***User-provided CDS information (optional):*** Use `--cds-info` to provide absolute paths to CDS files. For included accessions, the provided CDS is used, and Prodigal\\'s gene prediction is skipped. Only GenBank/RefSeq CDS files are supported (e.g., GCA_000839185.1_ViralProj14174_cds_from_genomic.fna.gz). ### <u>***Creat a new database***</u> >[!IMPORTANT] >***Requirements***: Reference FASTA file name (or path) must include the assembly accession (e.g., `GCF_028750015.1`, regex`GC[AF]_[0-9]+\\\\.[0-9]+`). Files from RefSeq or GenBank meet this requirement. #### 1. Download taxonkit-generated GTDB taxdump files [here](https://github.com/shenwei356/gtdb-taxdump/releases). #### 2. Build ``` # GTDB_TAXDUMP: Directory with downloaded GTDB taxdump files. # FASTA_LIST: File of reference genome absolute paths. # DBDIR: Directory where the database will be generated. metabuli build --gtdb 1 <DBDIR> <FASTA_LIST> <GTDB_TAXDUMP/taxid.map> --taxonomy-path <GTDB_TAXDUMP> [options] * Options --threads : The number of threads to utilize (all by default) --max-ram : The maximum RAM usage. (128 GiB by default) --accession-level : Set 1 to creat a DB for accession level classification (0 by default). --cds-info : List of absolute paths to CDS files. ``` This will generate **diffIdx**, **info**, **split**, and **taxID_list** and some other files. You can delete `*_diffIdx` and `*_info` files. ### <u>***Add new sequences to an existing database***</u> > [!NOTE] > If you want to use new GTDB release, please build a new database from scratch. You can add new sequences to a GTDB-based database. Expanding the taxonomy for virus or eukaryote is also possible. #### \\\\<Add GTDB genomes> ``` # GTDB_TAXDUMP: Directory with downloaded GTDB taxdump files. # FASTA_LIST: File of absolute paths to new sequences. # NEW DBDIR: Updated database is generated here. # OLD DBDIR: Directory of an existing database. metabuli updateDB --gtdb 1 <NEW DBDIR> <FASTA_LIST> <GTDB_TAXDUMP/taxid.map> <OLD DBDIR> [options] * Options --make-library: When many species are in the same FASTA, enable it for faster execution (0 by default). --new-taxa: List of new taxa to be added. --threads: The number of threads to utilize (all by default) --max-ram: The maximum RAM usage. (128 GiB by default) --accession-level: Set 1 to add new sequences for accession level classification (0 by default). --cds-info: List of absolute paths to CDS files. ``` #### \\\\<Add sequences of new taxa> > [!WARNING] > Mixing taxonomies within the same domain is not recommended. For example, adding prokaryotes to a GTDB database using NCBI taxonomy will cause issues, but adding eukaryotes or viruses using NCBI taxonomy is fine since GTDB does not cover them. 1\\\\. **Check taxdump** files to see if you need to add new taxa. `taxdump` command retrieves taxdump files of an existing database. 2-1\\\\. **Create a new taxa list** If you have **accession2taxid** and **taxonomy dump** files of the new sequences, you can use `createnewtaxalist` to create an input for `--new-taxa` option. If not, you have to prepare the input manually (see below). ``` metabuli createnewtaxalist <OLD DBDIR> <FASTA_LIST> <new taxonomy dump> <accession2taxid> <OUTDIR> ``` It generates `newtaxa.tsv` for `--new-taxa` option and `newtaxa.accession2taxid`. ##### Example Suppose you\\'re adding eukaryotes to a GTDB database. As GTDB doesn\\'t include eukaryotes, you may want to use NCBI taxonomy for eukaryotes. You can download `taxdump` files from [here](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/) and `accession2taxid` from [here](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/). ``` metabuli createnewtaxalist <GTDB dir> <new seq list> <NCBI taxdump dir> <NCBI accession2taxid> <out dir> metabuli updateDB <new db dir> <new seq list> <out dir/newtaxa.accession2taxid> <GTDB dir> --new-taxa <out dir/newtaxa.tsv> ``` 2-2\\\\. **Manually prepare a new taxa list** For the `--new-tax` option, provide a four-column TSV file in the following format. ``` taxID parentID rank name ``` The new taxon must be linked to a taxon in the existing database\\'s taxonomy. ##### Example Suppose you want to add *Saccharomyces cerevisiae* to a GTDB database. After inspecting taxonomy with `taxdump`, you find that the taxonomy lacks the Fungi kingdom and only includes one eukaryote (*Homo sapiens*). In this scenario, your new taxa list and accession2taxid should be as follows. ``` # New taxa list ## taxid parentTaxID rank name // Don\\'t put this header in your actual file. 10000013 10000012 species Saccharomyces cerevisiae 10000012 10000011 genus Saccharomyces 10000011 10000010 family Saccharomycetaceae 10000010 10000009 order Saccharomycetales 10000009 10000008 class Saccharomycetes 10000008 10000007 phylum Ascomycota 10000007 10000000 kingdom Fungi // 10000000 is Eukaroyte taxID of the pre-built DB. # accession2taxid accession accession.version taxid gi newseq1 newseq1 10000013 0 newseq2 newseq2 10000013 0 ``` ## NCBI or custom taxonomy based database ### <u>***Creat a new database***</u> >[!IMPORTANT] Three requirements: > 1. **FASTA files** : Each sequence must have a unique `>accession.version` or `>accesion` header (e.g., `>CP001849.1` or `>CP001849`). > 2. **NCBI-style accession2taxid** : Sequences with accessions absent here are skipped, and versions are ignored. > 3. **NCBI-style taxonomy dump** : `names.dmp`, `nodes.dmp`, and `merged.dmp`. Sequences with tax. IDs absent here are skipped. #### 1. Prepare NCBI-format taxonomy dump files and accession2taxid * Download `accession2taxid` from [here](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/). * Download `taxdump` files from [here](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/). * For custom sequences, edit `accession2taxid` and `taxdump` files as follows. * `accession2taxid` * For a sequence whose header is `>custom`, add `custom[tab]custom[tab]taxid[tab]anynumber`. * As above, version number is not necessary. * `taxid` must be included in the `nodes.dmp` and `names.dmp`. * Put any number for the last column. It is not used in Metabuli. * `taxdump` * Edit `nodes.dmp` and `names.dmp` if you introduced a new `taxid` in `accession2taxid`. #### 2. Build ``` # DBDIR: Directory where the database will be generated. # FASTA_LIST: A file containing absolute paths to FASTA files. # accession2taxid : NCBI-style accession2taxid file. # TAXDUMP: Directory with taxonomy dump files. metabuli build <DBDIR> <FASTA_LIST> <accession2taxid> --taxonomy-path <TAXDUMP> [options] * Options --make-library: When many species are in the same FASTA, enable it for faster execution (0 by default). --threads: The number of threads to use (all by default) --max-ram: The maximum RAM usage. (128 GiB by default) --accession-level: Set 1 to creat a DB for accession level classification (0 by default). --cds-info: List of absolute paths to CDS files. ``` This will generate **diffIdx**, **info**, **split**, and **taxID_list** and some other files. You can delete `*_diffIdx` and `*_info` files and `DATE-TIME` folder (e.g., `2025-1-24-10-32`) if generated. ### <u>***Add new sequences to an existing database***</u> You can add new sequences to an existing database, of which taxonomy will be used. You can add new taxa if the previous taxonomy does not include them (see \"Add sequences of new taxa\" below). #### \\\\<Add sequences of existing taxa> 1\\\\. **Prepare two files** - **New FASTA file list** : Each sequence must have a unique `>accession.version` or `>accesion` header. - **NCBI-style accession2taxid** : Sequences with accessions absent here are skipped. Put any number in the GI column. Version number is ignored. ``` accession accession.version taxID gi SequenceA SequenceA.1 960611 0 SequenceB SequenceB.1 960612 0 NoVersionOkay NoVersionOkay 960613 0 ``` 2\\\\. **Update database** ``` # NEW DBDIR: Directory where the updated database will be generated. # FASTA_LIST: A file of paths to new FASTA files. # accession2taxid : NCBI-style accession2taxid file. # OLD DBDIR: Directory of an existing database. metabuli updateDB <NEW DBDIR> <FASTA_LIST> <accession2taxid> <OLD DBDIR> [options] - FASTA list: A file of paths to the FASTA file to be added. - accession2taxid : A path to NCBI-style accession2taxid. * Options --threads : The number of threads used (all by default) --max-ram : The maximum RAM usage. (128 GiB by default) --accession-level : Set 1 to create a DB for accession level classification (0', 'by default). --make-library : Make species library for faster execution (1 by default). --new-taxa : List of new taxa to be added. ``` #### \\\\<Add sequences of new taxa> - Please refer [this section](#add-sequences-of-new-taxa). ## Example > The example here was detecting SARS-CoV-2 variant-specific reads, but has changed since the pre-built DB no longer contains the variant genomes. Classifying RNA-seq reads from a COVID-19 patient. The whole process must take less than 10 mins using a personal machine. #### 1. Download RefSeq Virus DB (1.5 GiB) ``` metabuli databases RefSeq_virus OUTDIR tmp ``` #### 2. Download an RNA-seq result (SRR14484345) ``` fasterq-dump --split-files SRR14484345 ``` > Download SRA Toolkit containing `fasterq-dump` [here](https://github.com/ncbi/sra-tools/wiki/02.-Installing-SRA-Toolkit) #### 3. Classify the reads using metabuli ``` metabuli classify SRR14484345_1.fq SRR14484345_2.fq OUTDIR/refseq_virus RESULT_DIR JOB_ID --max-ram RAM_SIZE ``` #### 4. Check RESULT_DIR/JOB_ID_report.tsv Find a section like the example below ``` 92.2331 510490 442 species 694009 Severe acute respiratory syndrome-related coronavirus 92.1433 509993 509993 no rank 2697049 Severe acute respiratory syndrome coronavirus 2 ``` ## Reference Shen, W., Ren, H., TaxonKit: a practical and efficient NCBI Taxonomy toolkit, Journal of Genetics and Genomics, https://doi.org/10.1016/j.jgg.2021.03.006']```"}, {"role": "user", "content": "Content from https://metabuli.steineggerlab.com/:\n```\nRepository metadata:\n{'name': 'Metabuli', 'label': ['Metabuli'], 'description': [{'term': 'Metabuli: specific and sensitive metagenomic classification via joint analysis of DNA and amino acid.', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['1.1.0', '1.0.9.2', '1.0.8', '1.0.7', '1.0.6', '1.0.5', '1.0.4', '1.0.3', '1.0.2', '1.0.1', '1.0.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/steineggerlab/Metabuli', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/k-mer', 'term': 'k-mer', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/metagenomics', 'term': 'metagenomics', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/taxonomic-classification', 'term': 'taxonomic-classification', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/taxonomy', 'term': 'taxonomy', 'vocabulary': ''}, 'id': 4}], 'operations': [], 'authors': [{'term': {'name': 'Jaebeom Kim', 'type': 'person', 'email': '68528165+jaebeom-kim@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Jaebeom Kim', 'type': 'person', 'email': 'jbeom0731@gmail.com', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/steineggerlab/Metabuli/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/steineggerlab/Metabuli/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/steineggerlab/Metabuli:\n```\nREADME content:\n['[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/metabuli/README.html) ![Platform](https://img.shields.io/badge/platform-Mac%20%7C%20Windows%20%7C%20Linux-brightgreen) # Metabuli ***Metabuli*** classifies metagenomic reads by comparing them to reference genomes. You can use Metabuli to profile the taxonomic composition of your samples or to detect specific (pathogenic) species. ***Sensitive and Specific.*** Metabuli uses a novel k-mer structure, called *metamer*, to analyze both amino acid (AA) and DNA sequences. It leverages AA conservation for sensitive homology detection and DNA mutations for specific differentiation between closely related taxa. ***A laptop is enough.*** Metabuli operates within user-specified RAM limits, allowing it to search any database that fits in storage. A PC with 8 GiB of RAM is sufficient for most analyses. ***A few clicks are enough.*** Metabuli App is now available [here](https://github.com/steineggerlab/Metabuli-App). With just a few clicks, you can run Metabuli and browse the results with Sankey and Krona plots on your PC. ***Short reads, long reads, and contigs.*** Metabuli can classify all types of sequences. --- For more details, please see [Nature Methods](https://www.nature.com/articles/s41592-024-02273-y), [PDF](https://www.nature.com/articles/s41592-024-02273-y.epdf?sharing_token=je_2D5Su0-xVOSjuKSAXF9RgN0jAjWel9jnR3ZoTv0M7gE7NDF_xi_3sW8QdRiwfSJNwqaXItSoeCvr7cvcoQxKLt0oROgWc6urmki9tP80cXEuHPN0D7b4y9y3i8Yv7sZw8MxxhAj7W6p9eZE2zaK3eozdOkXvwADVfso9cXIM%3D), [bioRxiv](https://www.biorxiv.org/content/10.1101/2023.05.31.543018v2), or [ISMB 2023 talk](https://www.youtube.com/watch?v=vz2fuRcVwyk). Please cite: [Kim J, Steinegger M. Metabuli: sensitive and specific metagenomic classification via joint analysis of amino acid and DNA. Nature Methods (2024).](https://doi.org/10.1038/s41592-024-02273-y) --- ### \ud83d\udda5\ufe0f [Metabuli App](https://github.com/steineggerlab/Metabuli-App) for Windows, MacOS, and Linux are now available! > Run taxonomic profiling in just a few clicks and explore results with Sankey and Krona plots. > Download the app for your OS [here](https://github.com/steineggerlab/Metabuli-App/releases)\u2014no separate Metabuli installation needed. <p align=\"center\"><img src=\"https://raw.githubusercontent.com/jaebeom-kim/Metabuli/master/.github/metabuli.jpg\" style=\"max-height: 500px; width: auto;\" /></p> --- ### Update in v1.1.0 - Fix errors in v1.0.9 - Custom DB creation became easier - Improve `updateDB` command ### Update in v1.0.9 - DB creation process improved - `updateDB` module to add new sequences to an existing database. - Users can provide CDS information to skip Prodigal\\'s gene prediction. - `--max-ram` parameter added to `build` module. - Compatibility with taxdump files generated using [taxonkit](https://bioinf.shenwei.me/taxonkit/). - Please check release note for details. ### Update in v1.0.8 - Added `extract` module to extract reads classified into a certain taxon. ### Update in v1.0.7 - **Metabuli became faster \ud83d\ude80** - Windows: *8.3* times faster - MacOS: *1.7* times faster - Linux: *1.3* times faster - Test details are in release note. - Fixed a bug in score calculation that could affect classification results. --- ## Table of Contents - [Installation](#installation) - [Precompiled binaries](#precompiled-binaries) - [Compile from source code](#compile-from-source-code) - [Pre-built databases](#pre-built-databases) - [Classification](#classification) - [Extract](#extract) - [GTDB-based custom database](#gtdb-based-custom-database) - [Creat a new database](#creat-a-new-database) - [Add new sequences to an existing database](#add-new-sequences-to-an-existing-database) - [NCBI or custom taxonomy based database](#ncbi-or-custom-taxonomy-based-database) - [Creat a new database](#creat-a-new-database-1) - [Add new sequences to an existing database](#add-new-sequences-to-an-existing-database-1) - [Example](#example) ## Installation ### Precompiled binaries ``` # install via conda conda install -c conda-forge -c bioconda metabuli # Linux AVX2 build (fast, recommended for most Linux system # check using: cat /proc/cpuinfo | grep avx2) wget https://mmseqs.com/metabuli/metabuli-linux-avx2.tar.gz; tar xvzf metabuli-linux-avx2.tar.gz; export PATH=$(pwd)/metabuli/bin/:$PATH # Linux SSE2 build (slower, for old systems) wget https://mmseqs.com/metabuli/metabuli-linux-sse2.tar.gz; tar xvzf metabuli-linux-sse2.tar.gz; export PATH=$(pwd)/metabuli/bin/:$PATH # MacOS (Universal, works on Apple Silicon and Intel Macs) wget https://mmseqs.com/metabuli/metabuli-osx-universal.tar.gz; tar xvzf metabuli-osx-universal.tar.gz; export PATH=$(pwd)/metabuli/bin/:$PATH ``` Metabuli also works on Linux ARM64 and Windows systems. Please check [https://mmseqs.com/metabuli](https://mmseqs.com/metabuli) for static builds for other architectures. ### Compile from source code ``` git clone https://github.com/steineggerlab/Metabuli.git cd Metabuli mkdir build && cd build cmake -DCMAKE_BUILD_TYPE=Release .. make -j 16 ``` The built binary can be found in `./build/src`. --- ## Pre-built databases You can download [pre-built databases](https://metabuli.steineggerlab.workers.dev/) using `databases` workflow. NOTE: The `databases` workflow may not work if you don\\'t use the latest version of Metabuli. In that case, please manually download databases from this [link](https://metabuli.steineggerlab.workers.dev/). ``` Usage: metabuli databases DB_NAME OUTDIR tmp # NOTE - A human genome (T2T-CHM13v2.0) is included in all databases below. 1. RefSeq Virus (8.1 GiB) - NCBI RefSeq release 223 virus genomes - Database will be in OUT_DIR/refseq_virus metabuli databases RefSeq_virus OUT_DIR tmp 2. RefSeq Prokaryote and Virus (115.6 GiB) - RefSeq prokaryote genomes (Complete Genome/Chromosome, 2024-03-26) + RefSeq Virus above. - Database will be in OUT_DIR/refseq_prokaryote_virus metabuli databases RefSeq OUTDIR tmp 3. GTDB (101 GiB) - GTDB 214.1 (Complete Genome/Chromosome, CheckM completeness > 90 and contamination < 5). - Database will be in OUT_DIR/gtdb metabuli databases GTDB OUTDIR tmp 4. RefSeq Releases 224 (619 GiB) - Viral and prokaryotic genomes of RefSeq release 224. metabuli databases RefSeq_release OUTDIR tmp ``` Downloaded files are stored in `OUTDIR/DB_NAME` directory, which can be provided for `classify` module as `DBDIR`. --- ## Classification ``` metabuli classify <i:FASTA/Q> <i:DBDIR> <o:OUTDIR> <Job ID> [options] - INPUT : FASTA/Q file of reads you want to classify. (gzip supported) - DBDIR : The directory of reference DB. - OUTDIR : The directory where the result files will be generated. - Job ID: It will be the prefix of result files. # Paired-end metabuli classify read_1.fna read_2.fna dbdir outdir jobid # Single-end metabuli classify --seq-mode 1 read.fna dbdir outdir jobid # Long-read metabuli classify --seq-mode 3 read.fna dbdir outdir jobid * Important parameters: --threads : The number of threads used (all by default) --max-ram : The maximum RAM usage. (128 GiB by default) --min-score : The minimum score to be classified --min-sp-score : The minimum score to be classified at or below species rank. --taxonomy-path: Directory where the taxonomy dump files are stored. (DBDIR/taxonomy by default) --accession-level : Set 1 to use accession level classification (0 by default). It is available when the DB is also built with accession level taxonomy. ``` - Paratemers for precision mode (Metabuli-P) - Illumina short reads: `--min-score 0.15 --min-sp-score 0.5` - PacBio HiFi reads: `--min-score 0.07 --min-sp-score 0.3` - PacBio Sequel II reads: `--min-score 0.005` - ONT reads: `--min-score 0.008` This will generate three result files: `JobID_classifications.tsv`, `JobID_report.tsv`, and `JobID_krona.html`. > Sankey diagram is available in the [GUI app](https://github.com/steineggerlab/Metabuli-App). #### JobID_classifications.tsv 1. Classified or not 2. Read ID 3. Taxonomy identifier 4. Effective read length 5. DNA level identity score 6. Classification Rank 7. List of \"taxID : k-mer match count\" ``` 1 read_1 2688 294 0.627551 subspecies 2688:65 1 read_2 2688 294 0.816327 subspecies 2688:78 0 read_3 0 294 0 no rank ``` #### JobID_report.tsv The proportion of reads that are assigned to each taxon. ``` 33.73 77571 77571 0 no rank unclassified 66.27 152429 132 1 no rank root 64.05 147319 2021 8034 superkingdom d__Bacteria 22.22 51102 3 22784 phylum p__Firmicutes 22.07 50752 361 22785 class c__Bacilli 17.12 39382 57 123658 order o__Bacillales 15.81 36359 3 126766 family f__Bacillaceae 15.79 36312 26613 126767 genus g__Bacillus 2.47 5677 4115 170517 species s__Bacillus amyloliquefaciens 0.38 883 883 170531 subspecies RS_GCF_001705195.1 0.16 360 360 170523 subspecies RS_GCF_003868675.1 ``` #### JobID_krona.html It is for an interactive taxonomy report (Krona). You can use any modern web browser to open `JobID_krona.html`. <p align=\"left\"><img src=\"https://raw.githubusercontent.com/steineggerlab/Metabuli/master/.github/image.png\" style=\"max-height: 350px; width: auto;\" /></p> ### Resource requirements Metabuli can classify reads against a database of any size as long as the database is fits in the hard disk, regardless of the machine\\'s RAM size. We tested it with a MacBook Air (2020, M1, 8 GiB), where we classified about 15 M paired-end 150 bp reads (~5 GiB in size) against a database built with ~23K prokaryotic genomes (~69 GiB in size). --- ## Extract After running the `classify` command, you can extract reads that are classified under a specific taxon. This requires the FASTA/Q files used in the `classify` step and the `JobID_classifications.tsv` file, which is generated as one of the output files. ``` metabuli extract <i:FASTA/Q> <i:read-by-read classification> <i:DBDIR> --tax-id TAX_ID - FASTA/Q : The FASTA/Q file(s) used during the `classify` step. - read-by-read classification : The JobID_classifications.tsv file generated by the `classify` step. - DBDIR : The same DBDIR used in the `classify` step. - TAX_ID : The taxonomy ID of the taxon at any rank (e.g., species, genus) from which you want to extract the reads. # Paired-end metabuli extract read_1.fna read_2.fna JobID_classifications.tsv dbdir --tax-id TAX_ID # Single-end metabuli extract --seq-mode 1 read.fna JobID_classifications.tsv dbdir --tax-id TAX_ID # Long-read metabuli extract --seq-mode 3 read.fna JobID_classifications.tsv dbdir --tax-id TAX_ID ``` #### Output - For paired-end samples: `read_1_TAX_ID.fna` and `read_2_TAX_ID.fna` - For single-end or long-read samples: `read_TAX_ID.fna` --- ## GTDB-based custom database > ***User-provided CDS information (optional):*** Use `--cds-info` to provide absolute paths to CDS files. For included accessions, the provided CDS is used, and Prodigal\\'s gene prediction is skipped. Only GenBank/RefSeq CDS files are supported (e.g., GCA_000839185.1_ViralProj14174_cds_from_genomic.fna.gz). ### <u>***Creat a new database***</u> >[!IMPORTANT] >***Requirements***: Reference FASTA file name (or path) must include the assembly accession (e.g., `GCF_028750015.1`, regex`GC[AF]_[0-9]+\\\\.[0-9]+`). Files from RefSeq or GenBank meet this requirement. #### 1. Download taxonkit-generated GTDB taxdump files [here](https://github.com/shenwei356/gtdb-taxdump/releases). #### 2. Build ``` # GTDB_TAXDUMP: Directory with downloaded GTDB taxdump files. # FASTA_LIST: File of reference genome absolute paths. # DBDIR: Directory where the database will be generated. metabuli build --gtdb 1 <DBDIR> <FASTA_LIST> <GTDB_TAXDUMP/taxid.map> --taxonomy-path <GTDB_TAXDUMP> [options] * Options --threads : The number of threads to utilize (all by default) --max-ram : The maximum RAM usage. (128 GiB by default) --accession-level : Set 1 to creat a DB for accession level classification (0 by default). --cds-info : List of absolute paths to CDS files. ``` This will generate **diffIdx**, **info**, **split**, and **taxID_list** and some other files. You can delete `*_diffIdx` and `*_info` files. ### <u>***Add new sequences to an existing database***</u> > [!NOTE] > If you want to use new GTDB release, please build a new database from scratch. You can add new sequences to a GTDB-based database. Expanding the taxonomy for virus or eukaryote is also possible. #### \\\\<Add GTDB genomes> ``` # GTDB_TAXDUMP: Directory with downloaded GTDB taxdump files. # FASTA_LIST: File of absolute paths to new sequences. # NEW DBDIR: Updated database is generated here. # OLD DBDIR: Directory of an existing database. metabuli updateDB --gtdb 1 <NEW DBDIR> <FASTA_LIST> <GTDB_TAXDUMP/taxid.map> <OLD DBDIR> [options] * Options --make-library: When many species are in the same FASTA, enable it for faster execution (0 by default). --new-taxa: List of new taxa to be added. --threads: The number of threads to utilize (all by default) --max-ram: The maximum RAM usage. (128 GiB by default) --accession-level: Set 1 to add new sequences for accession level classification (0 by default). --cds-info: List of absolute paths to CDS files. ``` #### \\\\<Add sequences of new taxa> > [!WARNING] > Mixing taxonomies within the same domain is not recommended. For example, adding prokaryotes to a GTDB database using NCBI taxonomy will cause issues, but adding eukaryotes or viruses using NCBI taxonomy is fine since GTDB does not cover them. 1\\\\. **Check taxdump** files to see if you need to add new taxa. `taxdump` command retrieves taxdump files of an existing database. 2-1\\\\. **Create a new taxa list** If you have **accession2taxid** and **taxonomy dump** files of the new sequences, you can use `createnewtaxalist` to create an input for `--new-taxa` option. If not, you have to prepare the input manually (see below). ``` metabuli createnewtaxalist <OLD DBDIR> <FASTA_LIST> <new taxonomy dump> <accession2taxid> <OUTDIR> ``` It generates `newtaxa.tsv` for `--new-taxa` option and `newtaxa.accession2taxid`. ##### Example Suppose you\\'re adding eukaryotes to a GTDB database. As GTDB doesn\\'t include eukaryotes, you may want to use NCBI taxonomy for eukaryotes. You can download `taxdump` files from [here](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/) and `accession2taxid` from [here](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/). ``` metabuli createnewtaxalist <GTDB dir> <new seq list> <NCBI taxdump dir> <NCBI accession2taxid> <out dir> metabuli updateDB <new db dir> <new seq list> <out dir/newtaxa.accession2taxid> <GTDB dir> --new-taxa <out dir/newtaxa.tsv> ``` 2-2\\\\. **Manually prepare a new taxa list** For the `--new-tax` option, provide a four-column TSV file in the following format. ``` taxID parentID rank name ``` The new taxon must be linked to a taxon in the existing database\\'s taxonomy. ##### Example Suppose you want to add *Saccharomyces cerevisiae* to a GTDB database. After inspecting taxonomy with `taxdump`, you find that the taxonomy lacks the Fungi kingdom and only includes one eukaryote (*Homo sapiens*). In this scenario, your new taxa list and accession2taxid should be as follows. ``` # New taxa list ## taxid parentTaxID rank name // Don\\'t put this header in your actual file. 10000013 10000012 species Saccharomyces cerevisiae 10000012 10000011 genus Saccharomyces 10000011 10000010 family Saccharomycetaceae 10000010 10000009 order Saccharomycetales 10000009 10000008 class Saccharomycetes 10000008 10000007 phylum Ascomycota 10000007 10000000 kingdom Fungi // 10000000 is Eukaroyte taxID of the pre-built DB. # accession2taxid accession accession.version taxid gi newseq1 newseq1 10000013 0 newseq2 newseq2 10000013 0 ``` ## NCBI or custom taxonomy based database ### <u>***Creat a new database***</u> >[!IMPORTANT] Three requirements: > 1. **FASTA files** : Each sequence must have a unique `>accession.version` or `>accesion` header (e.g., `>CP001849.1` or `>CP001849`). > 2. **NCBI-style accession2taxid** : Sequences with accessions absent here are skipped, and versions are ignored. > 3. **NCBI-style taxonomy dump** : `names.dmp`, `nodes.dmp`, and `merged.dmp`. Sequences with tax. IDs absent here are skipped. #### 1. Prepare NCBI-format taxonomy dump files and accession2taxid * Download `accession2taxid` from [here](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/). * Download `taxdump` files from [here](https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/). * For custom sequences, edit `accession2taxid` and `taxdump` files as follows. * `accession2taxid` * For a sequence whose header is `>custom`, add `custom[tab]custom[tab]taxid[tab]anynumber`. * As above, version number is not necessary. * `taxid` must be included in the `nodes.dmp` and `names.dmp`. * Put any number for the last column. It is not used in Metabuli. * `taxdump` * Edit `nodes.dmp` and `names.dmp` if you introduced a new `taxid` in `accession2taxid`. #### 2. Build ``` # DBDIR: Directory where the database will be generated. # FASTA_LIST: A file containing absolute paths to FASTA files. # accession2taxid : NCBI-style accession2taxid file. # TAXDUMP: Directory with taxonomy dump files. metabuli build <DBDIR> <FASTA_LIST> <accession2taxid> --taxonomy-path <TAXDUMP> [options] * Options --make-library: When many species are in the same FASTA, enable it for faster execution (0 by default). --threads: The number of threads to use (all by default) --max-ram: The maximum RAM usage. (128 GiB by default) --accession-level: Set 1 to creat a DB for accession level classification (0 by default). --cds-info: List of absolute paths to CDS files. ``` This will generate **diffIdx**, **info**, **split**, and **taxID_list** and some other files. You can delete `*_diffIdx` and `*_info` files and `DATE-TIME` folder (e.g., `2025-1-24-10-32`) if generated. ### <u>***Add new sequences to an existing database***</u> You can add new sequences to an existing database, of which taxonomy will be used. You can add new taxa if the previous taxonomy does not include them (see \"Add sequences of new taxa\" below). #### \\\\<Add sequences of existing taxa> 1\\\\. **Prepare two files** - **New FASTA file list** : Each sequence must have a unique `>accession.version` or `>accesion` header. - **NCBI-style accession2taxid** : Sequences with accessions absent here are skipped. Put any number in the GI column. Version number is ignored. ``` accession accession.version taxID gi SequenceA SequenceA.1 960611 0 SequenceB SequenceB.1 960612 0 NoVersionOkay NoVersionOkay 960613 0 ``` 2\\\\. **Update database** ``` # NEW DBDIR: Directory where the updated database will be generated. # FASTA_LIST: A file of paths to new FASTA files. # accession2taxid : NCBI-style accession2taxid file. # OLD DBDIR: Directory of an existing database. metabuli updateDB <NEW DBDIR> <FASTA_LIST> <accession2taxid> <OLD DBDIR> [options] - FASTA list: A file of paths to the FASTA file to be added. - accession2taxid : A path to NCBI-style accession2taxid. * Options --threads : The number of threads used (all by default) --max-ram : The maximum RAM usage. (128 GiB by default) --accession-level : Set 1 to create a DB for accession level classification (0', 'by default). --make-library : Make species library for faster execution (1 by default). --new-taxa : List of new taxa to be added. ``` #### \\\\<Add sequences of new taxa> - Please refer [this section](#add-sequences-of-new-taxa). ## Example > The example here was detecting SARS-CoV-2 variant-specific reads, but has changed since the pre-built DB no longer contains the variant genomes. Classifying RNA-seq reads from a COVID-19 patient. The whole process must take less than 10 mins using a personal machine. #### 1. Download RefSeq Virus DB (1.5 GiB) ``` metabuli databases RefSeq_virus OUTDIR tmp ``` #### 2. Download an RNA-seq result (SRR14484345) ``` fasterq-dump --split-files SRR14484345 ``` > Download SRA Toolkit containing `fasterq-dump` [here](https://github.com/ncbi/sra-tools/wiki/02.-Installing-SRA-Toolkit) #### 3. Classify the reads using metabuli ``` metabuli classify SRR14484345_1.fq SRR14484345_2.fq OUTDIR/refseq_virus RESULT_DIR JOB_ID --max-ram RAM_SIZE ``` #### 4. Check RESULT_DIR/JOB_ID_report.tsv Find a section like the example below ``` 92.2331 510490 442 species 694009 Severe acute respiratory syndrome-related coronavirus 92.1433 509993 509993 no rank 2697049 Severe acute respiratory syndrome coronavirus 2 ``` ## Reference Shen, W., Ren, H., TaxonKit: a practical and efficient NCBI Taxonomy toolkit, Journal of Genetics and Genomics, https://doi.org/10.1016/j.jgg.2021.03.006']```"}, {"role": "user", "content": "Content from https://github.com/steineggerlab/Metabuli:\n```\nRepository metadata:\n{'name': 'Metabuli', 'label': ['Metabuli'], 'description': [{'term': 'Metabuli: specific and sensitive metagenomic classification via joint analysis of DNA and amino acid.', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['1.1.0', '1.0.9.2', '1.0.8', '1.0.7', '1.0.6', '1.0.5', '1.0.4', '1.0.3', '1.0.2', '1.0.1', '1.0.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/steineggerlab/Metabuli', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/k-mer', 'term': 'k-mer', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/metagenomics', 'term': 'metagenomics', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/taxonomic-classification', 'term': 'taxonomic-classification', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/taxonomy', 'term': 'taxonomy', 'vocabulary': ''}, 'id': 4}], 'operations': [], 'authors': [{'term': {'name': 'Jaebeom Kim', 'type': 'person', 'email': '68528165+jaebeom-kim@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Jaebeom Kim', 'type': 'person', 'email': 'jbeom0731@gmail.com', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/steineggerlab/Metabuli/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/steineggerlab/Metabuli/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bamtools/suite": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/bamtools/suite/None\",\n    \"name\": \"bamtools\",\n    \"description\": [\n      \"BamTools provides a fast, flexible C++ API & toolkit for reading, writing, and managing BAM files.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/pezmaster31/bamtools\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Derek Barnett\",\n        \"email\": \"derekwbarnett@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a9d7d3b8acd1a2f9bb64\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/bamtools/cmd/2.4.0\",\n    \"name\": \"bamtools\",\n    \"description\": [\n      \"BAM datasets and perform other transformations.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fdevteam%2Fbamtools%2Fbamtools%2F2.4.0\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/pezmaster31/bamtools:\n```\nREADME content:\n[\"-------------------------------------------------------------------------------- README : BAMTOOLS -------------------------------------------------------------------------------- BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. I. Learn More II. License III. Acknowledgements IV. Contact -------------------------------------------------------------------------------- I. Learn More: -------------------------------------------------------------------------------- Installation steps, tutorial, API documentation, etc. are all now available through the BamTools project wiki: https://github.com/pezmaster31/bamtools/wiki Join the mailing list(s) to stay informed of updates or get involved with contributing: https://github.com/pezmaster31/bamtools/wiki/Mailing-lists -------------------------------------------------------------------------------- II. License : -------------------------------------------------------------------------------- Both the BamTools API and toolkit are released under the MIT License. Copyright (c) 2009-2010 Derek Barnett, Erik Garrison, Gabor Marth, Michael Stromberg See included file LICENSE for details. -------------------------------------------------------------------------------- III. Acknowledgements : -------------------------------------------------------------------------------- * Aaron Quinlan for several key feature ideas and bug fix contributions * Baptiste Lepilleur for the public-domain JSON parser (JsonCPP) * Heng Li, author of SAMtools - the original C-language BAM API/toolkit. -------------------------------------------------------------------------------- IV. Contact : -------------------------------------------------------------------------------- Feel free to contact me with any questions, comments, suggestions, bug reports, etc. Derek Barnett Marth Lab Biology Dept., Boston College Email: derekwbarnett@gmail.com Project Website: http://github.com/pezmaster31/bamtools\"]```"}, {"role": "user", "content": "Content from https://github.com/pezmaster31/bamtools:\n```\nRepository metadata:\n{'name': 'bamtools', 'label': ['bamtools'], 'description': [{'term': 'C++ API & command-line toolkit for working with BAM data', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v2.5.2', 'v2.5.1', 'v2.5.0', 'v2.4.2', 'v2.4.1'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/pezmaster31/bamtools', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Vladimir Petko', 'type': 'person', 'email': 'vladimir.petko@canonical.com', 'maintainer': False}, 'id': 0}, {'term': {'name': '\u00c9tienne Mollier', 'type': 'person', 'email': 'emollier@debian.org', 'maintainer': False}, 'id': 1}, {'term': {'name': 'David Seifert', 'type': 'person', 'email': 'SoapZA@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Andreas F\u00fcglistaler', 'type': 'person', 'email': 'andreas.fueglistaler@unifr.ch', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Andreas', 'type': 'person', 'email': 'andreas.fueglistaler@protonmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Rob Davies', 'type': 'person', 'email': 'rmd+git@sanger.ac.uk', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Ted Wong', 'type': 'person', 'email': 'tedwong@smallchess.com', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Jonathan MERCIER', 'type': 'person', 'email': 'bioinfornatics@gmail.com', 'maintainer': False}, 'id': 7}, {'term': {'name': 'Dominique Belhachemi', 'type': 'person', 'email': 'domibel@debian.org', 'maintainer': False}, 'id': 8}, {'term': {'name': 'Debian Med Packaging Team', 'type': 'person', 'email': 'debian-med-packaging@lists.alioth.debian.org', 'maintainer': False}, 'id': 9}, {'term': {'name': 'Michael R. Crusoe', 'type': 'person', 'email': 'crusoe@ucdavis.edu', 'maintainer': False}, 'id': 10}, {'term': {'name': 'David Seifert', 'type': 'person', 'email': 'dseifert@pacificbiosciences.com', 'maintainer': False}, 'id': 11}, {'term': {'name': 'Derek Barnett', 'type': 'person', 'email': 'derekwbarnett@gmail.com', 'maintainer': False}, 'id': 12}, {'term': {'name': 'Heng Li', 'type': 'person', 'email': 'lh3@me.com', 'maintainer': False}, 'id': 13}, {'term': {'name': 'Christopher Dunn', 'type': 'person', 'email': 'cdunn@pacificbiosciences.com', 'maintainer': False}, 'id': 14}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/pezmaster31/bamtools/blob/main/CMakeLists.txt'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"mea/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/mea/cmd/1.0\",\n    \"name\": \"mea\",\n    \"description\": [\n      \"A Methylomic and Epigenomic Allele-specific analysis pipeline.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/julienrichardalbert/MEA\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c828a310e2f3aa0b3b7e00\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/mea/cmd/0.6.4\",\n    \"name\": \"mea\",\n    \"description\": [\n      \"Mea was developed as part of the lab class \\\"Bioinformatik von RNA- und Proteinstrukturen (Praktikum, Modul 10-202-2208)\\\". The package predicts maximum expected accuracy (MEA) RNA secondary structures from dot plots of RNAs while correcting the score in dependence of base pair span. Furthermore, it provides tools to evaluate predictions and optimize parameters.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.bioinf.uni-leipzig.de/Software/mea/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://www.bioinf.uni-leipzig.de/Software/mea/:\n```\nContent:\n['## Mea - Prediction of maximum expected accuracy RNA secondary structures Supplemental material of manuscript \"The Trouble with Long Range Base Pairs in RNA Folding\" by _The Students of the Bioinformatics Computer Lab 2013, Fabian Amman, Stephan H. Bernhart, Gero Dose, Jing Qin, Peter F. Stadler, and Sebastian Will_ __ Software written by _Sarah Berkemer and Sebastian Will_ Mea was developed as part of the lab class \"Bioinformatik von RNA- und Proteinstrukturen (Praktikum, Modul 10-202-2208)\". The package predicts maximum expected accuracy (MEA) RNA secondary structures from dot plots of RNAs while correcting the score in dependence of base pair span. Furthermore, it provides tools to evaluate predictions and optimize parameters. ### Download Mea is free software (GNU GPL 3). Please use the newest release unless you know what you are doing. Older releases are available below for reference. [mea-0.6.4 source [tar.gz]](mea-0.6.4.tar.gz) Version 0.6.x adds slide rule and conflicts rule for comparisons to the reference structure; these are turned on by default. 0.6.4 corrects major bugs in the structure comparison of 0.6; the prediction itself is unchanged. The package can be compiled and installed in the common autotool\\'s way by ./configure; make; make install. #### Old releases [mea-0.5 source [tar.gz]](mea-0.5.tar.gz) Version 0.5 enables comparing two structures w/o folding (option --structure). Structure comparison is buggy:( [mea-0.4 source [tar.gz]](mea-0.4.tar.gz) Version 0.4 has been used to produce the published results. ### Mea package tools * mea [options] dotplotpredict mea structure from dot plot; control base pair penalty by parameters alpha, beta, gamma, and delta * mea_evalevaluate mea prediction in decpendence of parameters, given reference data * mea_mix [options] dotplot1 dotplot2predict mea structure by mixing two dot plots; control mixing weights by gamma1 and gamma2 Since release 0.5 the tool supports the general comparison of a given \"predicted\" structure to a \"reference\" structure by confusion matrix, F1-score, and MCC (Mathew\\'s correlation coefficient.) Furthermore, the package allows to compile a library mea-eval-R.so; this lib can be imported into R, where it enables optimizing parameters using R functionality. Please call tools with help option to learn more about their parameters. Dot plots are generally read from files in ViennaRNA\\'s \"dp.ps\" format.']```"}, {"role": "user", "content": "Content from https://github.com/julienrichardalbert/MEA:\n```\nREADME content:\n['# MEA: a Methylomic and Epigenomic Allele-specific analysis pipeline ![MEA Pipeline](doc/MEA-diagram.png) Building on our previous work on the ALEA software package, a computational toolbox for allele-specific epigenomics analysis incorporating allelic variation data, we detail here ALEA\u2019s successor - MEA. This package dramatically increases the functionality and usability of ALEA, incorporating allelic variation with existing resources, allowing for the identification of significant associations of epigenetic modifications and specific allelic variants in human and mouse cells. Similar to ALEA, MEA provides a customizable pipeline for allele-specific analysis of next-generation sequencing data which takes raw sequencing data for ChIP-seq, RNA-seq and DNA Methylation analysis, producing a UCSC track hub. MEA takes advantage of the available genomic resources for human (The 1000 Genomes Project Consortium) and mouse (The Mouse Genome Project) to reconstruct diploid in silico genomes for human samples or hybrid mouse samples. Then, for each accompanying ChIP-seq, RNA-seq or DNA Methylation dataset, MEA generates two wig files from short reads aligned differentially to each haplotype. This pipeline has been validated using human and hybrid mouse ChIPseq, RNAseq and DNA Methylation data (See [Test Data](#test-data)). ## Please refer to the MEA user guide (PDF) for installation and run instructions * [Docker installation](https://github.com/julienrichardalbert/MEA/raw/master/docker) * [distribution package](https://github.com/julienrichardalbert/MEA/raw/master/dist/mea.1.0.tar.gz) * [documentation (PDF)](https://github.com/julienrichardalbert/MEA/raw/master/doc/MEA_UserGuide_v1.0.pdf) * test data: ftp://ftp.bcgsc.ca/supplementary/ALEA/files/test-data/ ## Credits Hamid Younesy, Torsten M\u00f6ller, Alireza Heravi-Moussavi, Jeffrey B. Cheng, Joseph F. Costello, Matthew C. Lorincz, Mohammad M. Karimi and Steven J. M. Jones, \"ALEA: a toolbox for allele-specific epigenomics analysis.\" Bioinformatics 30.8 (2014): 1172-1174. [[link to paper](http://bioinformatics.oxfordjournals.org/content/30/8/1172.long)] Julien Richard Albert, Tasuku Koike, Hamid Younesy, Richard Thompson, Aaron B. Bogutz, Mohammad M. Karimi and Matthew C. Lorincz, \"Development and application of an integrated allele-specific pipeline for methylomic and epigenomic analysis (MEA).\" BMC Genomics201819:463 (https://doi.org/10.1186/s12864-018-4835-2)']```"}, {"role": "user", "content": "Content from https://github.com/julienrichardalbert/MEA:\n```\nRepository metadata:\n{'name': 'MEA', 'label': ['MEA'], 'description': [], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/julienrichardalbert/MEA', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'julienrichardalbert', 'type': 'person', 'email': 'jrichardalbert@gmail.com', 'maintainer': False}, 'id': 0}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/julienrichardalbert/MEA/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"plink/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/plink/cmd/none\",\n    \"name\": \"plink\",\n    \"description\": [\n      \"Free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://zzz.bwh.harvard.edu/plink/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a419d3b8acd1a2f9b575\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/plink/cmd/1.90b6.21\",\n    \"name\": \"plink\",\n    \"description\": [\n      \"Whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://www.cog-genomics.org/plink2\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://www.cog-genomics.org/plink2:\n```\nContent:\n['PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. 1: Solaris is no longer explicitly supported, but it should be able to run the Linux binaries. 2: These are just mirrors of the binaries posted at [https://zzz.bwh.harvard.edu/plink/download.shtml](https://zzz.bwh.harvard.edu/plink/download.shtml#download) . Continue using PLINK 1.07 for most of these operations. However, be aware that 3: We believe this now has almost no practical value, since the file format it expects is too different from VCF. 4: This was not fully developed in PLINK 1.07, and has been superseded by other IBD analysis packages. 5: PLINK\\'s haplotype phasing and imputation algorithms are obsolete. Future PLINK versions will be able to import phase and dosage information emitted by other programs; the haplotype association commands will be reintroduced when that functionality is in place. Until then, BEAGLE 3.3 should be more accurate than PLINK for case/control haplotype association. 6: Free database software handles these operations in a more flexible and powerful manner. 7: Just use gzip / gunzip for this. Or better, [pigz/unpigz.](https://zlib.net/pigz/) 13 Oct: --annotate subset= and --gene-subset should work properly now. 11 Oct: Fixed --r[2] dprime bug that affected runs covering both chrX/chrY and autosomal data. Main part of version string has been reformatted to adhere to [Semantic Versioning 2.0.0 spec](https://semver.org) . 18 Aug: --r[2] now explicitly reports active filters when generating tabular output. 4 Aug 2024: .ped and .map lines starting with \\'#\\' are now consistently treated as comments, following [hint #2 in the original PED documentation](https://zzz.bwh.harvard.edu/plink/data.shtml#ped) . We have updated the respective PLINK 1.9 and 2.0 \"File formats\" entries, and apologize for overlooking this earlier. 11 Dec 2023: --ld-window-cm should work properly now. 18 Oct: Fixed --clump bug that, when multiple input files were provided, could cause it to fail to open the first file. 16 Jan 2023 (beta 7): .bim reader now defaults to treating \\'.\\' as a missing allele code; without this, merge could fail with a spurious \"variants with 3+ alleles present\" error when given .bim files generated by PLINK 2.0 that didn\\'t have all allele codes filled in. This can be controlled with the [--missing-genotype2](/plink/1.9/input#missing_genotype) flag. 10 Dec 2022: --r/--r2 triangle + --parallel bugfix. 2 Apr: --make-set and similar flags no longer error out when --allow-extra-chr is specified and a contig is present in its input file but absent from the main dataset. 5 Mar 2022: --recode recognizes \\'Av\\' and \\'tped\\' as aliases for the \\'A-transpose\\' and \\'transpose\\' formats, respectively. --pedmap is now recognized as an alias for --file. 6 Jun 2021: --clump results should now be consistent across operating systems. (Previously, minor variations were possible when there were p-value ties.) 28 May: Genotyping rate is explicitly logged when equal to, or very close to, 1. These two cases are distinguished (\"in [0.9999995, 1)\" vs. \"exactly 1\"). 16 Apr 2021: --data/--gen now supports .gen files with 6 leading columns. 19 Oct 2020: Linux binaries no longer default to allowing Intel MKL to use processor-specific code paths, since this behavior made some results more difficult to reproduce across different machines. (This may make some commands a bit slower. You can remove this restriction with [--native](/plink/1.9/other#native) .) 21 Sep: Fixed --logistic bug that could cause the entire analysis to be skipped with an inaccurate \"Skipping --linear/--logistic since phenotype is constant\" warning. 16 Sep: Fixed --qfam bug that occurred when a variant filter (--extract, --maf, etc.) was applied simultaneously, and the remaining variant set was not contiguous (so if your only variant filter was --chr on a single chromosome, this bug did not trigger). The bug was pretty obvious if you were affected by it (the .within and .within.perm output files had mismatched variant IDs), but if you\\'re unsure you should rerun --qfam with the latest build. 16 Jun: Fixed --het bug that caused the wrong variants to be skipped when chrM was present but not at the end of the file. 28 Apr: Fixed --allow-extra-chr + --autosome-num/--cow/... segfault that could occur when the chromosome-set flag prohibited X/Y/XY/MT but that chromosome code was in the dataset anyway; an appropriate error message is now printed instead. --ld-window-r2 can now be used with --r. 19 Feb: --bcf now parses header line IDX fields; previously, if e.g. a FILTER key and an INFO key were identical in a BCFv2.2 file, --bcf may have computed the wrong string index for FORMAT:GT, in which case import would fail. 21 Jan: \"--extract range\" and \"--exclude range\" no longer error out when their input files contain a chromosome code absent from the current dataset. 7 Jan 2020: Fixed --meta-analysis-se-field bug. --meta-analysis-chr-field and --meta-analysis-bp-field flags added. In the development build, --dfam consistently errors out with an appropriate message, instead of possibly segfaulting, when no valid cases are present. 30 Nov 2019: \"--recode rlist\" no longer forgets to generate a .map file. 28 Oct: --meta-analysis-se-field flag added, for interoperation with recent PLINK 2.0 logistic regression output (which uses a \\'LOG(OR)_SE\\' field name to disambiguate the standard error\\'s units). 24 Oct: Fixed uninitialized-variable bug in --dosage + --q-score-range. \"--indep-pairwise 500kb ...\" works properly again (the previous build required a space before the \\'kb\\'). 17 Jun: --loop-assoc now works properly when the original .fam file has missing phenotype values. 4 Mar: Brackets in command-line help text are now used in a manner more similar to other tools. --tests now works properly when a numeric argument is 1 past the end. 15 Feb 2019: Logistic regression no longer reports intercept beta values when it\\'s supposed to report odds ratios. Scientific notation can now be used for command-line integer parameters. 2 Dec 2018: --set bugfix. 10 Oct: Fixed --tdt parent-of-origin-analysis handling of chrX. 13 Sep: Improved logistic regression convergence-failure detection (affects case/control --epistasis as well as --logistic). Fixed --R memory leak. 7 Aug: --assoc set-test bugfix. 17 Jul: Fixed uninitialized --make-set-border value bug. 12 Jun: \"--assoc fisher\"\\'s multiple testing corrections no longer treat zero-MAF variants as valid tests. 28 May: Fixed --file triallelic-variant handling bug which occurred when the .map was unsorted. 26 May (beta 6): --merge-equal-pos bugfixes. If you used --merge-equal-pos with .bed+.bim+.fam filesets where allele order may have differed between .bim files, or with any .ped+.map fileset at all, we recommend redoing that run. 10 Apr: --zero-cluster command-line parsing bugfix. --mendel-multigen bugfix. 21 Feb: \"--indiv-sort file\" no longer scrambles the phenotypes when used with --[b]merge or --merge-list. 9 Jan 2018: --all-pheno no longer gets into an infinite loop when one phenotype is all-missing. 20 Dec 2017: --meta-analysis \\'report-all\\' now reports original stats instead of \\'NA\\' when N=1. Fixed an inaccuracy in --adjust\\'s handling of very small p-values. --make-just-fam works properly with just .fam input again. 14 Nov (beta 5): --file + --merge[-list] bugfix. This bug was much more likely to trigger if the first .map file was unsorted, but if you have the chance we recommend redoing any --file + --merge and --file + --merge-list multi-.ped merges performed with earlier plink 1.9 builds. (.bed merging was unaffected, and --merge-list without --file was practically never affected.) 3 Nov: --meta-analysis-snp-field and --meta-analysis-ess-field bugfixes. 15 Oct: A2 allele is now consistently reference on chr3 in our [1000 Genomes phase 1 downloads](/plink/1.9/resources#phase1) . As a general rule, you shouldn\\'t expect to keep REF/ALT alleles straight with PLINK 1.x, but we tried to make A2 consistently reference in these files and still didn\\'t quite succeed; sorry about that. (Why was this discovered now, more than 2.5 years after the fact? Because [PLINK 2.0](/plink/2.0/) now has a --ref-from-fa flag which can easily check for this type of error.) 13 Oct: \"--freq counts\" no longer gives reversed results when alleles have been flipped from the initial .bim/.bed order. Fixed --logistic bug that could cause spurious NA results. 12 Oct: --snps/--exclude-snps no longer always includes other variants at the same positions as the variant(s) you named. 7 Oct: Fixed corner-case (two of the four haplotypes have no known observations) bug in --ld/--r2/--blocks/--clump\\'s statistical phasing procedure. 32-bit build can now sort >2GB .bed files without crashing. --annotate and --gene-report now work with --allow-extra-chr. 15 Aug: --lasso should now work properly when there are samples with all covariates present but the main phenotype missing. 25 Jul: --lfile now works properly when the .map file has three columns. 21 May: \"--linear interaction sex\" and \"--logistic interaction sex\" now work properly when no covariates are present. --tucc added to development build. 9 May: --tests now works properly on MT and haploid chromosomes. Improved --score compatibility with v1.07 and v2.0. 7 May: --r/--r2 + variant filter bugfix. 30 March: --ld-snps bugfix. 20 March (beta 4): --pca var-wts now reports allele codes. Minor statistical phasing bugfix. --[max-]mac, --R, and --thin-indiv[-count] no longer classified as unstable. [PLINK 2.0 alpha builds](/plink/2.0/) are now available. 13 February: --R now works properly with missing covariate entries. --epistasis + variant filter bugfix. 13 January 2017: --ld-snps now works properly with other variant filters. --set-mixed-mt-missing flag added. 17 November 2016: chrX and chrY are no longer mishandled with --chr-set. 13 November: --geno/--missing now treat heterozygous haploid calls as nonmissing; this is consistent with --mind, and PLINK 1.07\\'s behavior with --recode/--make-bed active. **They\\'re now inconsistent with what PLINK 1.07 did without --recode/--make-bed** ; use --make-bed + --set-hh-missing first if you want the old behavior. \"--ci 0.95\" no longer produces \\'L94\\'/\\'U94\\' column names when plink gets unlucky with rounding. Minor --exclude/--extract/--flip bugfix for input files with no newline at the end. --qfam[-within] bugfix. --dosage \\'sex\\' modifier now works properly when some samples have been filtered out (e.g. because some phenotypes were missing). 20 September: VCF export no longer erases an alt allele absent from the immediate dataset, due to a [retroactive revision to the VCF specification](https://github.com/samtools/hts-specs/commit/5e4b76db276aec7a80913ab6a365a18ee18037f1) . Fixed \"--model gen\" permutation testing bug. 10 September: --remove-cluster-names + --family bugfix. --indep-pairwise/--indep-pairphase consistency improvements (MAF comparison now takes floating point imprecision into account; a few pairwise comparisons are no longer improperly skipped when the window is kb-based). 16 August: --merge-x \\'no-fail\\' modifier works properly again. \"--hard-call-threshold random\" bugfix for probabilities with less than 4-digit precision. 5 August: --silent Windows bugfix. --r/--r2 \\'d\\' and \\'dprime-signed\\' modes added. --ld-window-cm flag added. --output-missing-genotype now works properly with --make-bed. --genome + --read-freq bugfix. 7 June: --23file X/Y/MT chromosome conversion bugfix. --pheno bugfix (if phenotype was quantitative, but first value was nonnumeric, it was treated as \\'0\\' instead of missing in recent builds). 16 May: Set test bugfix. \"--R debug\" bugfix. 31 March: \"--split-x b38\" now works properly. 32-bit set-test bugfix. --cluster + --within bugfix. 25 March: --recode beagle bugfix. --gene bugfix. \"--vcf-half-call reference\" mode added. 15 March: Fixed --update-alleles bug introduced in 24 Feb build. 13 March: Fixed .lgen loading bug introduced in 24 Feb build. Fixed use-after-free bug in extra chromosome name cleanup code. --allow-no-{samples,vars,covars} added to stable build. 24 February: Contig limit raised to ~65000. --annotate/--gene-report bugfix for 3-4 column case. --flip-scan bugfix. 3 February: Number-to-string encoding bugfix (occasionally affected numbers \u2265 10 6 saved in the .bim centimorgan column). 27 January 2016: Speed improvement for operating on a subset of samples. Fix minor --1 backward compatibility break. 24 December 2015: Fix --dosage + --extract/--exclude bug introduced in 4 November build. Minor --test-missing permutation bugfix. --vcf-min-qual bugfix for .bcf files. 16 December: \"--meta-analysis + study qt\" now reports regression betas instead of odds ratios in the study-specific columns. \"--score header\" no longer forgets to append .nopred to the problem list filename. 13 December: --allow-no-samples and --allow-no-vars flags added. --dosage now suppresses regression results for very-low-MAF variants in the same same manner as it does in PLINK 1.07. --lgen flag now supported. --gxe works properly again (it was inadvertently disabled a few months ago). --hardy now produces \\'nan\\' results for chrY/chrM variants (like PLINK 1.07) instead of entirely omitting them. --hwe \"observation counts\" warning is no longer triggered by chrY variants, and is now more informative when only triggered by chrX variants. 26 November: --logistic can now report intercepts. --logistic adaptive permutation bugfix. If you used adaptive permutation with --logistic in the past, we recommend that you redo the run with the latest build. 22 November: --indep{,-pairwise,-pairphase} kb-based windows now work properly with sample/variant filters. Several --indep-pairphase bugfixes. --all-pheno no longer causes exit code 127 to be returned on successful runs. --recode-allele segfault bugfix. --meta-analysis now only considers the first appearance when a variant appears multiple times in the same file. --no-const-covar and --meta-analysis-report-dups flags added. 4 November: Fixed \"--r[2] square0\" bug which occasionally caused a line break to be missing in the middle of the output file. Fixed --genome + --parallel missing-data handling bug. 17 October: Set-handling bugfixes. --[fast-]epistasis + variant filtering bugfix. --score \\'double-dosage\\' modifier added. 3 September: --[b]merge no longer crashes when two sample IDs are mostly identical but have different capitalization and different parental IDs/phenotypes. Errors and warnings are now printed to stderr instead of stdout (yes, this is overdue). --dfam implemented. --vcf now accepts \\'*\\' deletion-overlap alt allele codes (thanks to John Wallace). 15 July: --freq case-control mode added, and analogous --dosage case-control-freqs modifier added. Fixed --homozyg .summary bug that occurred when variant filters were present. Revised heterozygous haploid warning to be clearer. --mendel \\'summaries-only\\' modifier added. --R missing value bugfixes. --dosage \\'skip2\\' now works properly. 29 June: --attrib[-indiv] now handles multiple negative match conditions in the same manner as PLINK 1.07. --clump + variant filter bugfix. --lasso is now memory-efficient. 25 June: Fixed --snps bug that could arise when two named variants had positions differing by exactly 1. 17 June: X chromosome Mendelian error checking bugfixes. \"--dosage Zout\" bugfix. 13 June: --mds-plot switched from eigendecomposition-based algorithm back to SVD, and the matrix diagonal is now properly double-centered. (Update, 15 June: you can now use the --mds-plot \\'eigendecomp\\' modifier in the development build to request the eigendecomposition algorithm, with the centering bug fixed. It does have the virtue of being several times faster than SVD.) 29 May: --vcf now won\\'t error out on GATK 3.4 symbolic deletion alleles. --recode \\'gen-gz\\' modifier (for gzipping of Oxford-format .gen output) added. --pca header bugfix. --meta-analysis long allele code bugfix. 20 May: --make-grm-bin + --parallel is now permitted by the command-line parser. Fixed bug in 11 May build which broke parsing of --gen + --sample. 11 May: Fix --meta-analysis bug introduced in 18 April build. \"Options in effect:\" printed to standard output again, by popular demand. 3 May: Merge + chromosome filter bugfix. 18 April: --linear + --tests bugfix. --dosage + --exclude bugfix. 9 April: --fst now works properly with variant filters. 30 March: Fixed recent --dosage association report bug which caused null characters to appear on \\'NA\\' lines. --linear/--logistic sex modifier + --parameters bugfix. (Update, 2 April: Linux binaries should no longer fail with \"kernel too old\" on RHEL 6 and similar systems.) 18 March: --meta-analysis no-map bugfix, QFAM sibship handling bugfix. 12 March: --mendel + --mendel-duos bugfix, --linear/--logistic + --adjust works properly again (--adjust was reporting \"zero valid tests\" when nothing was actually wrong). Missing genotype calls at otherwise monomorphic loci in multichar-allele .ped files are now converted properly (they were previously encoded as a pair of \\'0\\' alleles, instead of a missing call). QFAM test closed for repairs. --distance-wts partially added to development build (GRM variant weighting coming soon). 5 March: --fast-epistasis 32-bit missing data handling bugfix. --thin-indiv and --thin-indiv-count added to development build (courtesy of Masahiro Kanai). 2 March: VCF \\'PR\\' header line is no longer malformed. (Existing malformed VCFs generated by January-February builds can be fixed by adding a \\'>\\' at the end of that line.) Fixed a set-handling bug that could affect sets containing the dataset\\'s last variant. 26 February: \"--recode vcf\" header modified for compatibility with VCFtools. Compact [1000 Genomes phase 1 data files](/plink/1.9/resources#phase1) posted on the resources page. 10 February: Low-memory --r/--r2 matrix output bugfix. --recode vcf can now generate bgzipped output. More gzip/bgzip multithreading. 2 February: --bcf now handles BCFv2.2 nonzero missing genotype and end-of-vector values (emitted by e.g. bcftools 1.1) properly. --mendel-multigen is no longer blocked by the command-line parser when a family-based association test is run. Oxford-format import now tolerates identical A1/A2 allele codes. --missing, --freq, --hardy, and --het now support gzipped output (add the \\'gz\\' modifier). --R added to development build. 15 January: --keep/--remove now works properly on newly-updated IDs when --update-ids is in the same run. --mac/--max-mac added to development build. 12 January: --dosage + --covar + --sex now works properly, and an inaccurate warning about the PLINK 1.07 implementation has been removed. --dosage logistic regression no longer reports improper p-values for very small samples. 11 January 2015 (beta 3): --epistasis implemented, with heavy optimization for quantitative traits. \"--recode vcf{,-fid,-iid}\" now flags reference alleles as \"possibly not based on a real reference genome\" unless --real-ref-alleles is also specified, and sets ALT alleles to \\'.\\' when they are not present in the immediate dataset. --vcf-min-gq and --vcf-min-gp no longer error out when a genotype entry has fewer fields than expected, since the VCF specification explicitly states that this is an acceptable way to represent trailing missing values. --make-bed no longer segfaults when resorting a file too large to fit in memory. 20 December 2014: --dummy, --show-tags, --neighbour, --mh/--bd/--mh2/--homog and --clump-field bugfixes. --q-score-range implemented for dosage data. --split-x/--merge-x \\'no-fail\\' modifier added to support data conversion scripts. 13 December: Case/control --hardy/--hwe no longer randomly excludes too many controls from control-only stage of test. (This was mostly harmless, since the all-samples test still worked, and the bug was only likely', 'to occur when --hardy/--hwe was in the same run as other filters like --exclude/--extract. But if you used the UNAFF rows in the --hardy report for anything, you should rerun --hardy with the latest build. ) set-test multiple-testing correction now counts nonempty sets with zero significant variants. 11 December: --dosage format=3 + --score bugfix. \\'beagle-nomap\\' option added to --recode. --list-duplicate-vars flag added to development build. 7 December: --adjust now applies to the joint test statistic instead of the additive effect when the former is computed, and no longer reports linearly genomic-controlled p-values when they would be invalid. Some numeric stability improvements for small p-values. --logistic permutation test and --assoc/--model set-test bugfixes. --linear/--logistic set-test and --dosage \\'sex\\' modifier implemented. --dosage + --sex error message now explains that PLINK 1.07 did not handle this flag combination properly. 25 November: Fixed --exclude/--extract memory management bug in 2 November build. --score now supports dosage data. --write-dosage no longer writes incorrect IDs when sample filters are applied. --dosage \\'noheader\\' can now be used with \\'list\\' when each batch has only one file. --assoc/--model set-test implemented. --set-missing-var-ids extended to permit inclusion of allele names, and --set-missing-snp-ids/--set-missing-nonsnp-ids retired. UNADJ column values are now correct in \"--adjust gc\" reports. 2 November: Improved variant ID lookup speed. Single-precision binary matrix output (e.g. --make-grm-bin) is now based on double-precision internal computations, and the \\'bin\\' + \\'single-prec\\' modifier combination has been replaced with \\'bin4\\'. Binary --distance output bugfix. 15 October: Fixed a bug in --vcf\\'s handling of variants with 10 or more alternate alleles. --dosage Zout no longer segfaults at the end. Merger no longer scrambles centimorgan coordinates. 26 September: Fixed --nonfounders bug which broke X chromosome MAF computation. --vcf-min-gp now tolerates \\'?\\' GP values. 20 September: Fixed non-strict --biallelic-only bug when handling multiallelic variants. If you have used --biallelic-only without \\'strict\\' on VCF files with triallelic variants, we strongly recommend rerunning the operation with the latest build. Fixed quantitative trait --assoc bug that caused it to write each output line twice, and --linear/--logistic mishandling of some datasets with heterozygous haploid calls. --vcf-min-gp added to development build. 18 September: --no-fid bugfix. Presence of nonnumeric phenotype strings (e.g. \\'NA\\') no longer force the phenotype to be treated as quantitative. --output-missing-phenotype now accepts nonnumeric strings. \"--dosage list\" now works properly with multiple batches. --oxford-single-chr flag added to allow loading of single-chromosome .gen files with ignorable SNP ID field values. --genome no longer fails to report some parent-offspring relationships. --update-alleles was still missing a few matches involving missing genotypes; this has been fixed. 8 September: --write-dosage bugfix for 2730+ samples. --[b]merge/--merge-list now usually errors out when combined with a filter flag that wouldn\\'t take effect. --meta-analysis now supports weighted Z-score-based analysis. 2 September: --dosage \\'noheader\\' should now work properly when some phenotypes are missing. --vcf + --vcf-filter now parses semicolon-delimited FILTER fields correctly. --vcf-min-gq flag added to development build. --exclude/--extract no longer have terrible performance when both the main dataset and the --exclude/--extract variant list have millions of \\'.\\' entries. 28 August: --recode HV[-1chr] now always uses \\'0\\' as the missing phenotype code, since Haploview does not accept -9. --write-covar + --with-phenotype no longer segfaults on case/control phenotypes, and multichar allele .ped loader no longer segfaults on nonstandard tri/quadallelic variants. --meta-analysis added to development build. 17 August: Fixed a recent library function bug which broke --filter, --within, and a few other flags. 14 August: Fixed a typo in the 11 August build which caused covariate loading and a few other functions to hang. 11 August: --linear/--logistic no longer uses a buggy Huber-White standard error estimator when clusters are defined. --output-chr now works properly with --make-bed when the input .bim is unsorted, and a very long allele code in an unsorted .bim no longer causes --make-bed to segfault. 1 August: --dosage logistic regression bugfix. --make-just-bim and --make-just-fam flags added. .bim/.fam files can now be processed without an accompanying .bed under some circumstances. --recode-allele now works properly with A-transpose mode. 18 July: Text filesets with both multi-character allele codes and an unsorted .map file no longer cause the autoconverter to crash. Malformed files generated by some old merges no longer cause segfaults. --pfilter should now consistently filter out \\'NA\\' entries. --dosage chromosome code output bug fixed. --read-freq now loads A1 allele codes when they\\'re missing from the main dataset, instead of erroring out in that situation. --show-tags added to development build. 4 July: Fixed two merge bugs which potentially caused data in the last few samples of an input or merged fileset to be mishandled. If you have merged filesets generated with earlier PLINK 1.90 alpha/beta builds, and more than two samples were involved, we suggest redoing the merge with a more recent build. (Most merges were unaffected, but better safe than sorry.) 3 July (beta 2): .ped file parser now properly handles --missing-genotype flag. --mh2 implemented. 1 July: Fix recently introduced (sorry about that) --data/--gen/--sample command-line parsing bug. --indep no longer misreports the number of pruned variants when there is extensive multicollinearity. --fst, --homog, and --oxford-pheno-name flags added. 27 June: \"--recode oxford\" no longer dumps incorrect IDs when used in the same run as a sample filter. --fill-missing-a2 flag added. --recode \\'A-transpose\\' and \\'include-alt\\' modifiers added. \"--het small-sample\" mode added. Basic Cochran-Mantel-Haenszel and Breslow-Day tests added to development build. 20 June: Fixed --mendel zero chromosome code/segfault bug. --merge-list no longer requires a reference fileset. --fast-epistasis + --parallel bugfix. --indep-pairphase and QFAM test completed in development build. 10 June: --merge-mode 1 now correctly merges missing calls with a single nonmissing call. --r/--r2 chromosome boundary handling bugfixes. \\'0X\\'/\\'0Y\\'/\\'0M\\' chromosome codes emitted by Oxford tools are now recognized, and also supported by --output-chr. --vcf-half-call flag added to govern handling of \\'0/.\\' VCF GT values, and default behavior is now \\'error\\' mode to force a conscious decision. --dosage completed in development build. 5 June: --ld-snp-list and multipass --r/--r2 bugfixes. Nonstandard \\'0/.\\' and similar VCF GT field values are now processed as if they did not have the trailing \\'/.\\', instead of causing a segfault. (Handling of this case may be configurable in the future, stay tuned.) --linear/--logistic permutation bugfix (permutation success count array was not initialized to all-zero). --dosage linear regression added to development build. 3 June: Gzipper no longer deletes the output file when being asked to append (this was causing big n-pass calculations such as --r2 gz to only keep the output of the last pass). Fixed --genome double-missing-call handling bug, and a --r/--r2 mixed autosomal/nonautosomal data handling bug. \"--dosage occur\" and --write-dosage added to development build. 28 May: Fixed a few nonstandard chromosome name-related segfaults. \"--r2 dprime\" missing data handling bugfix. Corrected misnamed --filter-attrib[-indiv] flags to --attrib[-indiv], fixed a positive matching bug, and added support for gzipped attribute files. \"--recode {A,AD} tab\" no longer emits spaces in the header line. --blocks now has a \\'no-pheno-req\\' modifier which removes the unnecessary phenotype requirement. --annotate added to development build. 24 May: Sample-major to variant-major .bed transposition bugfix. Merger now provides an informative error message when given an sample-major .bed file, and does not log equal-position warnings when multiple variants have bp coordinate 0 (since that\\'s often used to indicate that the variant is unlocalized). --make-bed no longer crashes on tiny nonzero centimorgan coordinates. Contig limit raised to ~5000, to support draft mosquito genomes. --gene-report added to development build. 20 May: 32-bit X/Ychr MAF calculation bugfix. --qual-scores added to development build. 13 May: --logistic + sex covariate bugfix. --snp without --window no longer behaves like --snp + \"--window 0\" when other variants share the same bp coordinate. --pca now errors out instead of returning all-zero eigenvalues/vectors when samples with no genotype data are present. 11 May (beta 1): Fast third-party --logistic code integrated (see credits page for details). Logging now permits gPLINK to consistently detect output files. --output-chr added. --family and --make-perm-pheno implemented. Within-cluster permutation bugfix. --linear/--logistic dominant/recessive models and covariate interactions work now. --tests bugfix. 2 May: --clump-verbose no longer reports negative r 2 values when phase flips. 1 May: --clump-verbose + --clump-range bugfix. 26 April: Fixed merge bug in 21 and 25 April builds. 25 April: Old sample-major PLINK binary files are now detected correctly. Malformed input error messages now include line numbers. 15 April: --bcf now adds 1 to variant coordinates, since its coordinates are defined to be 0-based while VCF is 1-based. 14 April: --check-sex/--impute-sex no longer silently considers nonmissing Y genotype counts by default. --bcf now treats missing variant IDs as if they were equal to \\'.\\', instead of erroring out. Basic parent-of-origin test implemented. 8 April: Centimorgan position loading bugfix. --mendel error description formatting bugfix. --lambda bugfix. --adjust now prints and logs estimated genomic control lambda value. --keep[-fam]/--remove[-fam] input files with duplicate IDs now just trigger a warning instead of an error. --check-sex/--impute-sex now has a \\'y-only\\' mode. 5 April: --not-chr bugfix. \"--recode fastphase\" no longer defaults to 0/1 allele codes (though they can still be requested with e.g. \"--recode 01 fastphase\"). --hardy2/--hwe2 now invoke the mid-p adjusted versions of --hardy and --hwe, to reflect the original chi-square test\\'s lack of conservative bias. 4 April: Fixed --hardy bug that caused chromosome names and marker IDs to sometimes be merged on case/control data. --check-sex/--impute-sex now use Y chromosome data when it\\'s present. --flip-subset, --flip-scan, and basic exact binomial test --tdt implemented. 28 March: Fixed --bcf bug that caused it to fail whenever there were multiple FORMAT fields. --score missing phenotype output bugfix. 27 March: Fixed --bcf header line parsing bug (loading should no longer fail when the GT header line appears after a non-PASS INFO or FILTER line, and --vcf-filter should now work with BCF2 files). --split-x \\'hg20\\' build code corrected to \\'hg38\\'. --ibc Fhat2/Fhat3 bugfix. --het and --set-me-missing implemented. 26 March: --hardy/--hwe X chromosome case/control bugfix. --extract/--exclude now considers every token in a file, instead of just the first on each line (this was undocumented PLINK 1.07 behavior). 25 March: --bcf no longer fails to load newer bcftools-generated files with \\'IDX=\\' toward the end of the GT meta-information line. File import shortcuts (--vcf + --out without --make-bed, etc.) now error out when a filter flag that wouldn\\'t take effect (e.g. --extract, --hwe, --snps-only...) is specified. --me and --mendel implemented; --mendel-duos and --mendel-multigen flags added to extend their functionality. Fixed PLINK 1.07 --mendel issue where genotypes would be set to missing before scanning was complete (i.e. if there were overlapping trios, PLINK 1.07 could fail to report some errors). 22 March: --ld-snp-list long file bugfix, PLINK 1.07 --score Y chromosome handling bugfix. 20 March: --condition-list command line parsing bugfix, --recode beagle bugfix, --make-founders bugfix, sample filtering bugfix for --regress-distance and --recode lgen/list/rlist. Basic --score implemention. Ueki/Cordell joint-effects test now skips marker pairs with less than 5 observations in any contingency table cell (where cases and controls are considered separately); this threshold is adjustable with --je-cellmin. 17 March: --vcf-idspace-to flag added to improve handling of VCF/BCF2 sample ID spaces. --blocks is now more customizable. 15 March: Multiple solutions to the haplotype frequency cubic equation (which arises when evaluating Lewontin\\'s D-prime) should now always be handled correctly; there were a few corner cases which were mishandled before. Markers with identical bp coordinates no longer cause 1.9 --blocks to yield different results than 1.07 --blocks. 14 March: --blocks implemented. D-prime computations (\"--r2 dprime\", --ld, --blocks, --clump) involving variants on the X chromosome now appropriately downweight males relative to females. --vcf and --bcf now handle sample ID spaces in a reasonable manner. 7 March: Fixed minor --genome bug that clipped Z2 estimate to 0 instead of 1 when it was too large. --all-pheno and --loop-assoc now print case/control counts for each phenotype. 6 March: Fixed --make-bed bug that threw away major allele codes of monomorphic loci when the markers were unsorted and no minor allele code was present. 28 February: --a1-allele/--a2-allele \\'fix\\' in 26 February build was backwards; this is no longer the case. If you used --a1-allele/--a2-allele from that build on a dataset with monomorphic loci and missing allele codes, you should download the latest build and rerun your pipeline from that point forward; sorry about the mistake. (We hope we were the only actual victims of this.) VCF importer now supports variants with 10+ alternate alleles. 27 February: VCF generator no longer segfaults sometimes on the X chromosome. VCF allele code Nazi now just issues a warning, since some pipelines actually depend on violating the official spec. 26 February: --a2-allele succeeds instead of giving \\'Impossible allele assignment\\' warnings when the A1 allele code is unset, and vice versa for --a1-allele. Variant IDs in --a1-allele/--a2-allele \"Impossible allele assignment\" warnings are no longer strangely truncated. VCF \\'N\\' reference allele now handled in a saner manner (converted to and back from missing). 25 February: --ci no longer prints incorrect confidence intervals with --linear/--logistic. --linear \\'intercept\\' modifier and --thin-count added. 24 February: Fixed .tped loading bug when original file was not fully sorted. VCF generator now forces the A2 (reference) allele to always be known, and outputs \\'.\\' instead of \\'0\\' when the A1 (alternate) allele is unknown; VCF allele codes are also forced to either only contain characters in {A,C,G,T,N,a,c,g,t,n} or start with \\'<\\'. 20 February: --r/--r2 bugfixes. 19 February: Fixed errors that occurred when using disallowed (via --chr-set, --dog, etc.) X/Y/XY/MT chromosome codes with --allow-extra-chr. --dog now permits mitochondrial data. 15 February: Fixed --hardy segfault on datasets with no controls. --recode-allele now works properly (it was changing the header line without flipping allele counts before). --clump-best implemented. 14 February: Fixed \"--genome full\" IBS0 column printing bug in 11 February build. --set-missing-{snp,nonsnp,var}-ids flags changed to use @ instead of ^ to mark the chromosome code\\'s position, since ^ is a reserved Windows shell character. --cm-map flag changed to also use @ for the chromosome code. LD-based result clumping (--clump) is now supported. --r/--r2 \\'in-phase\\' modifier. 11 February: Fixed --geno bug introduced in 23 January build (missing-phenotype samples were being partially thrown out for no reason). --set-missing-snp-ids and --set-missing-nonsnp-ids flags introduced, to handle the case of overlapping SNPs and indels being defined as separate variants at the same coordinate. --cow now permits mitochondrial data. Distance/relationship matrix calculations no longer waste a huge amount of time on thread creation and destruction when hundreds of cores are present. 8 February: Thread limit temporarily decreased to 23, since higher numbers result in too much thread creation/destruction overhead for now. (This multithread efficiency issue will be solved in the near future.) Also see the major bugfix above. 7 February: --genome + --parallel bugfix. --update-parents was broken in the 6 February build; it should now be fixed, and now permits very long lines (so it\\'s usable on .ped files). 6 February: IDs and pedigree information are no longer incorrect when --genome is used with sample filtering flags. --extract/--exclude now support set range files (--range). Optional \\'chr\\' chromosome prefixes may now be partly or entirely capitalized. --pca var-wts modifier, --pca-cluster-names/--pca-clusters projections. Gzipped and binary Oxford genotype files can now be directly imported. --update-sex now takes a column parameter (so it can be pointed directly at .ped/.fam files now). 30 January: Fix --snps bug introduced in 23 Jan build. --pca, --set-missing-var-ids, --test-mishap. --aec is now acceptable shorthand for --allow-extra-chr. --rel-cutoff moved earlier in order of operations (so e.g. --rel-cutoff --make-bed works). 26 January: --file now handles nonstandard chromosome names in .map files properly. --make-grm-bin + --parallel output filename fix. --test-missing max(T) permutation test. 24 January: --covar-name fixed, hopefully for good. Fixed contig name handling bug which slipped into 23 Jan builds. Fisher mid-p adjustment + permutation test bugfix. --test-missing adaptive permutation test. 23 January: Fixed a bug with --missing + sample filters, as well a minor PLINK 1.07 --missing + --within bug. Contig limit raised to ~2500. --zero-cluster, --oblig-missing. --geno and --mind should now treat sex chromosomes the same way as PLINK 1.07. 16 January: Oxford-format loader now accepts tab-delimited text files, because apparently that is a thing. --hwe interface improved (inappropriate default p-value removed, and a warning is now printed whenever observation counts vary by more than 10%). 14 January: Fixed .bim sorting bug when some loci were simultaneously being filtered out. --check-sex and --impute-sex implemented. Hardy-Weinberg and Fisher\\'s exact tests now support mid-p adjustments. Proper handling of ambiguous sex codes. Y chromosome \"nonmissing nonmale genotypes\" warning no longer gets sexes backwards (oops). Mitochondrial DNA no longer required to be haploid (though there are no plans to support full polyploidy). --split-x and --merge-x flags added to simplify handling of X chromosome pseudo-autosomal region. 9 January: --all-pheno now includes phenotype IDs in output filenames when possible (instead of \\'P1\\', \\'P2\\', etc.). --hardy + variant filter bugfix. --linear/--logistic covariate handling bugfix. Support for SHAPEIT recombination maps added (--cm-map). 7 January 2014: --indep[-pairwise] speed improvement in no-missing-call case. Cluster permutation and --covar-name range handling bugfixes. --snps-only filter added. Linux/macOS thread limit raised to 1023. 23 December 2013: Oxford-format loader no longer requires \\'missing\\' to be lowercase. Basic --test-missing. Fixed a bug which sometimes came up when using --ibs-test or association analysis commands while filtering out samples. 21 December: Covariate loader bugfix. 20 December: --condition-list bugfix, --bcf + --vcf-filter bugfix. --make-rel/--make-grm-gz/--make-grm-bin/--ibc work properly again when input file has some major alleles in the A1 position. (Also fixed the --distance segfault introduced in the 18 Dec build; sorry about that.) 11 December: Fixed loading of .map files missing a centimorgan column. --fast-epistasis set-by-set works properly with two sets now. 9 December: --r/--r2 dprime bugfixes, --fast-epistasis uninitialized N_SIG bugfix. Basic variant set file I/O, --fast-epistasis set-based tests. 5 December: --make-bed position sort and Oxford-format loading bugfixes. 32-bit --r/--r2/--fast-epistasis bugfix. --fast-epistasis now supports extended version of BOOST test (missing data permitted, df', \"properly adjusted in the face of e.g. zero homozygous minor observations). --r/--r2/--ld finished. --gplink flag supported. 26 November: --filter-nonfounders segfault fix. 25 November: --biallelic-only segfault fix when 'list' modifier was not specified. BCF2 (either uncompressed or BGZF-compressed) and Oxford-formatted data can now be directly imported. --fast-epistasis now supports the Ueki-Cordell joint effects test, and fills the 3x3 contingency tables more quickly when no missing markers are present (increasing the speedup factor to ~60 **C** in that case). --fast-epistasis + --parallel works. 17 November: --r/--r2 bugfix, --fast-epistasis, --recode oxford. (The --fast-epistasis implementation is roughly 40 **C** times as fast as PLINK 1.07, where **C** is the number of processor cores, and it also employs a more accurate variance estimator.) A bit of dead wood trimmed to make way for better implementations (--regress-pcs, dosage distance calculator); let us know if you want those functions to return sooner rather than later. 12 November: Basic VCF text loader. Add check for matching input and output filenames when using --rel-cutoff in batch mode. 10 November: --r/--r2 square matrices. Fixed --keep/--remove/--extract/--exclude bug introduced in a recent build. --genome IBD sharing calculation bugfix. --indep[-pairwise] results should no longer be slightly discordant with PLINK 1.07 when missing data is present (standard deviations were previously calculated once per site, they're now recalculated for every pair). 8 November: --indep[-pairwise] corner case bugfix. --filter-attrib[-indiv]. 3 November: .ped loader should no longer run out of memory if a single very long allele is present. 1 November: The main loading sequence and most functions should now handle very long allele names. .ped multi-character allele loading and --a1-allele/--a2-allele bugfixes. 28 October: Fixed some merge and multichar allele handling bugs. --filter can now match against more than one value. Cluster membership filters added (--keep-clusters, --keep-cluster-names, --remove-clusters, --remove-cluster-names). 19 October: --lasso now supports covariates. 17 October: --pheno missing quantitative phenotype bugfix, minor --lasso bugfix. 16 October: Basic LASSO implementation (--lasso). --linear/--logistic X chromosome bugfix. 14 October: --missing. Various --linear/--logistic bugfixes. --linear/--logistic permutation tests now correctly announce when they are employing multiple threads. 13 October: --linear/--logistic. Nonautosomal chromosome filtering bugfix. We hasten to add that the vast majority of ideas contributing to PLINK 1.9's performance were developed elsewhere; in several cases, we have simply ported little-known but outstanding implementations without significant further revision (even while possibly uglifying them beyond recognition; sorry about that, Roman...). See the [credits](/plink/1.9/credits) page for a partial list of people to thank. On a related note, if you are aware of an implementation of a PLINK command which is substantially better what we currently do, let us know; we'll be happy to switch to their algorithm and give them credit in our documentation and papers. PLINK's primary job is management and analysis of position-based SNP-like data for thousands of samples, and it is optimized for this setting. Here are a few things PLINK will probably never be able to do, since they are serious jobs best handled with fundamentally different data structures than the ones PLINK is built around.\"]```"}, {"role": "user", "content": "Content from http://zzz.bwh.harvard.edu/plink/:\n```\nContent:\n['**New (15-May-2014)** **: PLINK 1.9 is now[available](plink2.shtml)for beta-testing!** **PLINK** is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner. The focus of **PLINK** is purely on _analysis_ of genotype/phenotype data, so there is no support for steps prior to this (e.g. study design and planning, generating genotype or CNV calls from raw data). Through integration with [gPLINK](gplink.shtml) and [Haploview](http://www.broad.mit.edu/mpg/haploview) , there is some support for the subsequent visualization, annotation and storage of results. **PLINK** (one syllable) is being developed by [Shaun Purcell](http://zzz.bwh.harvard.edu) whilst at the Center for Human Genetic Research ( [CHGR](http://chgr.mgh.harvard.edu) ), Massachusetts General Hospital ( [MGH](http://www.mgh.harvard.edu) ), and the [Broad Institute](http://www.broad.mit.edu) of Harvard & MIT, with the [support of others](credits.shtml) . **New in 1.07** : [meta-analysis](metaanal.shtml) , [result annotation](annot.shtml) and analysis of [dosage data](dosage.shtml) .']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"masurca/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/masurca/cmd/4.1.1\",\n    \"name\": \"masurca\",\n    \"description\": [\n      \"MaSuRCA (Maryland Super-Read Celera Assembler) genome assembly software. MaSuRCA requires Illumina data, and supports third-generation PacBio/Nanopore MinION reads for hybrid assembly.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/alekseyzimin/masurca\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://masurca.blogspot.co.uk/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a112d3b8acd1a2f9b4c2\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/masurca/cmd/None\",\n    \"name\": \"masurca\",\n    \"description\": [\n      \"Whole genome assembly software. It combines the efficiency of the de\u00a0Bruijn graph and Overlap-Layout-Consensus (OLC) approaches. MaSuRCA can assemble data sets containing only short reads from Illumina sequencing or a mixture of short reads and long reads (Sanger, 454).\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.genome.umd.edu/masurca.html\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Aleksey Zimin\",\n        \"email\": \"alekseyz@ipst.umd.edu\",\n        \"maintainer\": false,\n        \"url\": \"http://www.genome.umd.edu/contact.html\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Guillaume Mar\u00e7ais\",\n        \"email\": \"gmarcais@umd.edu\",\n        \"maintainer\": false,\n        \"url\": \"http://www.genome.umd.edu/contact.html\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a112d3b8acd1a2f9b4c2\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://masurca.blogspot.co.uk/:\n```\nContent:\n['Imagine a situation where you have a genome of an organism that you have been studying, assembled from the previous generation sequencing data. You have spent time to produce this assembly, curated it, and it looks good, but contiguity is not up to today\\'s standards. A way to improve the existing assembly, without starting anew, is to upgrade it using additional long-read data. If you still have DNA from the same individual that was used to produce data for the original assembly, it is relatively straightforward (and cheap) now to produce additional long read data with Oxford Nanopore MinIion or PromethIon instrument. You can then use SAMBA to quickly scaffold and gap-fill your existing draft assembly with the additional long-read data. This will result in substantial improvements in contiguity, and likely correctness. SAMBA can use the long reads to check existing contigs for misassemblies using long read alignments, break at suspected misassembly locations, and then scaffold the contigs and fill in the sequence for all spanned gaps in the scaffolds. This yields both much bigger and more structurally correct contigs. SAMBA is free open-source software included with MaSuRCA version 4.0.9 and up: [Releases \u00b7 alekseyzimin/masurca \u00b7 GitHub](https://github.com/alekseyzimin/masurca/releases) SAMBA is published in PLoS Computational biology: Zimin AV, Salzberg SL. The SAMBA tool uses long reads to improve the contiguity of genome assemblies. PLoS computational biology. 2022 Feb 4;18(2):e1009860. The invocation of SAMBA is as follows: samba.sh [options] -r <contigs or scaffolds in fasta format> -q <long reads or another assembly used to scaffold in fasta or fastq format, can be gzipped> -t <number of threads> -d <scaffolding data type: ont, pbclr or asm, default:ont> -m <minimum matching length, default:5000> -o <maximum overhang, default:1000> -a <optional: allowed merges file in the format per line: contig1 contig2, only pairs of contigs listed will be considered for merging, useful for intrascaffold gap filling> -v verbose flag -h|--help|-u|--usage this message SAMBA installs with MaSuRCA and requires no external dependencies. The only parameter that is worth modifying is -m or the minimum matching length. 2000-2500 is a good value for small eukaryotic genomes 100-400Mb in size, 5000 is the default best value for large eukaryotic genomes (2-3Gbp), and 9000-10000 is the best value for large highly repetitive plant genomes (5Gbp+). MaSuRCA also provides a wrapper script for SAMBA that allows to use SAMBA to close intra-scaffold gaps in an assembly. The usage is as follows: close_scaffold_gaps.sh [options] -r <scaffolds to gapclose> MANDATORY -q <sequences used for closing gaps, can be long reads or another assembly, in fasta or fastq format, can ge gzipped> MANDATORY -t <number of threads, default:1> -i <identity% default:98> -m <minimum match length on the two sides of the gap, default:2500> -o <max overhang, default:1000> -v verbose flag -h|--help|-u|--usage this message The above script will split the scaffolds into contigs and then run SAMBA only allowing intrascaffold gaps to be filled. Contig \"flips\" inside the scaffold are allowed, making this a great tool to gapfill and fix assemblies scaffolded with HiC data, because HiC scaffolding sometimes incorrectly flips contigs in scaffolds.']```"}, {"role": "user", "content": "Content from https://github.com/alekseyzimin/masurca:\n```\nREADME content:\n['# MaSuRCA Genome Assembly and Analysis Toolkit Quick Start Guide The MaSuRCA (Maryland Super Read Cabog Assembler) genome assembly and analysis toolkit contains of MaSuRCA genome assembler, QuORUM error corrector for Illumina data, POLCA genome polishing software, Chromosome scaffolder, jellyfish mer counter, and MUMmer aligner. The usage instructions for the additional tools that are exclusive to MaSuRCA, such as POLCA and Chromosome scaffolder are provided at the end of this Guide. The MaSuRCA assembler combines the benefits of deBruijn graph and Overlap-Layout-Consensus assembly approaches. Since version 3.2.1 it supports hybrid assembly with short Illumina reads and long high error PacBio/MinION data. Citation for MaSuRCA: Zimin AV, Mar\u00e7ais G, Puiu D, Roberts M, Salzberg SL, Yorke JA. The MaSuRCA genome assembler. Bioinformatics. 2013 Nov 1;29(21):2669-77. Citation for MaSuRCA hybrid assembler: Zimin AV, Puiu D, Luo MC, Zhu T, Koren S, Yorke JA, Dvorak J, Salzberg S. Hybrid assembly of the large and highly repetitive genome of Aegilops tauschii, a progenitor of bread wheat, with the mega-reads algorithm. Genome Research. 2017 Jan 1:066100. This project is governed by the code of conduct at https://github.com/alekseyzimin/masurca/blob/master/code_of_conduct.md # 1. System requirements/run rimes for assembly ## Compile/Install requirements. To compile the assembler we require gcc version 4.7 or newer to be installed on the system. Only Linux is supported (May or may not compile under gcc for MacOS or Cygwin, Windows, etc). The assembler has been tested on the following distributions: \u2022 Fedora 12 and up \u2022 RedHat 5 and 6 (requires installation of gcc 4.7) \u2022 CentOS 5 and 6 (requires installation of gcc 4.7) \u2022 Ubuntu 12 LTS and up \u2022 SUSE Linux 16 and up WARNING: installing MaSuRCA via Bioconda is not supported, and may result in broken installation due to conflicts with other packages, especially Mummer package. If you get errors during assembly related to mummer.pm, remove the Bioconda from your path by editing your .bashrc. ## Hardware requirements for assembly. The hardware requirements vary with the size of the genome project. Both Intel and AMD x64 architectures are supported. The general guidelines for hardware configuration are as follows: \u2022 Bacteria (up to 10Mb): 16Gb RAM, 8+ cores, 10Gb disk space \u2022 Insect (up to 500Mb): 128Gb RAM, 16+ cores, 1Tb disk space \u2022 Avian/small plant genomes (up to 1Gb): 256Gb RAM, 32+ cores, 2Tb disk space \u2022 Mammalian genomes (up to 3Gb): 512Gb RAM, 32+ cores, 5Tb disk space \u2022 Plant genomes (up to 30Gb): 2Tb RAM, 64+cores, 10Tb+ disk space ## Expected assembly run times. The expected run times depend on the cpu speed/number of cores used for the assembly and on the data used. The following lists the expected run times for the minimum configurations outlined above for Illumina-only data sets. Adding long reads (454, Sanger, etc. makes the assembly run about 50-100% longer: \u2022 Bacteria (up to 10Mb): <1 hour \u2022 Insect (up to 500Mb): 1-2 days \u2022 Avian/small plant genomes (up to 1Gb): 4-5 days \u2022 Mammalian genomes (up to 3Gb): 15-20 days \u2022 Plant genomes (up to 30Gb): 60-90 days # 2. Installation instructions To install, first download the latest distribution from the github release page https://github.com/alekseyzimin/masurca/releases. Then untar/unzip the package MaSuRCA-X.X.X.tgz, cd to the resulting folder and run `./install.sh`. The installation script will configure and make all necessary packages. Only for developers: you can clone the development tree, but then there are dependencies such as swig and yaggo (http://www.swig.org/ and https://github.com/gmarcais/yaggo) that must be available on the PATH: ``` git clone https://github.com/alekseyzimin/masurca git submodule init git submodule update make ``` Note that on some systems you may encounter a build error due to lack of xlocale.h file, because it was removed in glibc 2.26. xlocale.h is used in Perl extension modules used by MaSuRCA. To fix/work around this error, you can upgrade the Perl extensions, or create a symlink for xlocale.h to /etc/local.h or /usr/include/locale.h, e.g.: ``` ln -s /usr/include/locale.h /usr/include/xlocale.h ``` # 3. Running the MaSuRCA assembler ## Overview. The general steps to run the MaSuRCA assemblers are as follows, and will be covered in details in later sections. It is advised to create a new directory for each assembly project. In the rest of this document, `/install_path` refers to a path to the directory in which `./install.sh` was run. IMPORTANT! Avoid using third party tools to pre-process the Illumina data before providing it to MaSuRCA, unless you are absolutely sure you know exactly what the preprocessing tool does. Do not do any trimming, cleaning or error correction. This will likely deteriorate the assembly. There are two ways to run MaSuRCA. For small projects that only use data from two Illumina sequencing fastq files representing forward and reverse reads from a paired end run, and (optionally) data from a long-read run such as Pacbio SMRT or Nanopore sequencing run in a single fasta/fastq file, one can use a simplified approach. This runs the full version of the MaSuRCA assembly pipeline with default settings on a command line. The options are described in the usage message that is displayed by using -h or --help switch. There are three command line switches, -i, -t and -r. -t specifies the number of threads to use, -i specifies the names and paths to Illumina paired end reads files and -r specifies the name and the path to the long reads file. For example: `/path_to_MaSuRCA/bin/masurca -t 32 -i /path_to/pe_R1.fa,/path_to/pe_R2.fa` will run assembly with only Illumina paired end reads from files /path_to/pe_R1.fa (forward) and /path_to/pe_R2.fa (reverse). An example of the hybrid assembly: `/path_to_MaSuRCA/bin/masurca -t 32 -i /path_to/pe_R1.fa,/path_to/pe_R2.fa -r /path_to/nanopore.fastq.gz` This command will run a hybrid assembly, correcting Nanopore reads with Illumina data first. Ilumina paired end reads files must be fastq, can be gzipped, and Nanopore/PacBio data files for the -r option can be fasta or fastq and can be gzipped. For bigger projects that use Ullimina data from multiple instrument runs, Illumina mate pairs, legacy Sanger data and/or need parameter adjustments, there is a more advanced mode. To use it, create a configuration file which contains the location of the compiled assembler, the location of the data and some parameters. Copy in your assembly directory the template configuration file `/install_path/sr_config_example.txt` which was created by the installer with the correct paths to the freshly compiled software and with reasonable parameters. Most assembly projects should only need to set the paths to the input data in the example configuration. More detailed description of the configuration is in the next section. Second, run the `masurca` script which will generate from the configuration file a shell script `assemble.sh`. This last script is the main driver of the assembly. Finally, run the script `assemble.sh` to assemble the data. ## Configuration. To run the assembler, one must first create a configuration file that specifies the location of the executables, data and assembly parameters for the assembler. The installation script will create a sample configuration file `sr_config_example.txt`. Lines starting with a pound sign (\\'#\\') are comments and ignored. The configuration file consists of two sections: DATA and PARAMETERS. Each section concludes with END statement. The easiest way is to copy the sample configuration file to the directory of choice for running the assembly and then modify it according to the specifications of the assembly project. Please read all comments in the example configuration file before using MaSuRCA. All options are explained in the comments. Here is the example configuration file: ``` # example configuration file # DATA is specified as type {PE,JUMP,OTHER,PACBIO} and 5 fields: # 1)two_letter_prefix 2)mean 3)stdev 4)fastq(.gz)_fwd_reads # 5)fastq(.gz)_rev_reads. The PE reads are always assumed to be # innies, i.e. --->.<---, and JUMP are assumed to be outties # <---.--->. If there are any jump libraries that are innies, such as # longjump, specify them as JUMP and specify NEGATIVE mean. Reverse reads # are optional for PE libraries and mandatory for JUMP libraries. Any # OTHER sequence data (454, Sanger, Ion torrent, etc) must be first # converted into Celera Assembler compatible .frg files (see # http://wgs-assembler.sourceforge.com) DATA #Illumina paired end reads supplied as <two-character prefix> <fragment mean> <fragment stdev> <forward_reads> <reverse_reads> #if single-end, do not specify <reverse_reads> #MUST HAVE Illumina paired end reads to use MaSuRCA PE= pe 500 50 /FULL_PATH/frag_1.fastq /FULL_PATH/frag_2.fastq #Illumina mate pair reads supplied as <two-character prefix> <fragment mean> <fragment stdev> <forward_reads> <reverse_reads> JUMP= sh 3600 200 /FULL_PATH/short_1.fastq /FULL_PATH/short_2.fastq #pacbio OR nanopore reads must be in a single fasta or fastq file with absolute path, can be gzipped #if you have both types of reads supply them both as NANOPORE type #PACBIO=/FULL_PATH/pacbio.fa #NANOPORE=/FULL_PATH/nanopore.fa #Other reads (Sanger, 454, etc) one frg file, concatenate your frg files into one if you have many #OTHER=/FULL_PATH/file.frg #synteny-assisted assembly, concatenate all reference genomes into one reference.fa; works for Illumina-only data #REFERENCE=/FULL_PATH/nanopore.fa END PARAMETERS #PLEASE READ all comments to essential parameters below, and set the parameters according to your project #set this to 1 if your Illumina jumping library reads are shorter than 100bp EXTEND_JUMP_READS=0 #this is k-mer size for deBruijn graph values between 25 and 127 are supported, auto will compute the optimal size based on the read data and GC content GRAPH_KMER_SIZE = auto #set this to 1 for all Illumina-only assemblies #set this to 0 if you have more than 15x coverage by long reads (Pacbio or Nanopore) or any other long reads/mate pairs (Illumina MP, Sanger, 454, etc) USE_LINKING_MATES = 0 #specifies whether to run the assembly on the grid USE_GRID=0 #specifies grid engine to use SGE or SLURM GRID_ENGINE=SGE #specifies queue (for SGE) or partition (for SLURM) to use when running on the grid MANDATORY GRID_QUEUE=all.q #batch size in the amount of long read sequence for each batch on the grid GRID_BATCH_SIZE=500000000 #use at most this much coverage by the longest Pacbio or Nanopore reads, discard the rest of the reads #can increase this to 30 or 35 if your reads are short (N50<7000bp) LHE_COVERAGE=25 #set to 0 (default) to do two passes of mega-reads for slower, but higher quality assembly, otherwise set to 1 MEGA_READS_ONE_PASS=0 #this parameter is useful if you have too many Illumina jumping library mates. Typically set it to 60 for bacteria and 300 for the other organisms LIMIT_JUMP_COVERAGE = 300 #these are the additional parameters to Celera Assembler. do not worry about performance, number or processors or batch sizes -- these are computed automatically. #CABOG ASSEMBLY ONLY: set cgwErrorRate=0.25 for bacteria and 0.1<=cgwErrorRate<=0.15 for other organisms. CA_PARAMETERS = cgwErrorRate=0.15 #CABOG ASSEMBLY ONLY: whether to attempt to close gaps in scaffolds with Illumina or long read data CLOSE_GAPS=1 #number of cpus to use, set this to the number of CPUs/threads per node you will be using NUM_THREADS = 16 #this is mandatory jellyfish hash size -- a safe value is estimated_genome_size*20 JF_SIZE = 200000000 #ILLUMINA ONLY. Set this to 1 to use SOAPdenovo contigging/scaffolding module. #Assembly will be worse but will run faster. Useful for very large (>=8Gbp) genomes from Illumina-only data SOAP_ASSEMBLY=0 #If you are doing Hybrid Illumina paired end + Nanopore/PacBio assembly ONLY (no Illumina mate pairs or OTHER frg files). #Set this to 1 to use Flye assembler for final assembly of corrected mega-reads. #A lot faster than CABOG, AND QUALITY IS THE SAME OR BETTER. #Works well even when MEGA_READS_ONE_PASS is set to 1. #DO NOT use if you have less than 15x coverage by long reads. FLYE_ASSEMBLY=0 END ``` DATA \u2013 in this section the user must specify the types of data available for the assembly. Each line represent a library and must start with PE=, JUMP= or OTHER= for the 3 different type of input read library (Paired Ends, Jumping or other). There can be multiple lines starting with `PE=` (or JUMP=), one line per library. PE and JUMP data must be in fastq format while the other data is in provided as a Celera Assembler frag format (`.frg`). Every PE or JUMP library is named by a unique two letter prefix. No two library can have the same prefix and a prefix should be made of two printable characters or number (no space or control characters), e.g. `aa`, `ZZ`, `l5`, or `J2`. The following types of data are supported: \u2022 Illumina paired end (or single end) reads -- MANDATORY: `PE = two_letter_prefix mean stdev /PATH/fwd_reads.fastq /PATH/rev_reads.fastq` example: `PE = aa 180 20 /data/fwd_reads.fastq /data/rev_reads.fastq` The `mean` and `stdev` parameters are the library insert average length and standard deviation. If the standard deviation is not known, set it to approximately 15% of the mean.If the second (reverse) read set is not available, do not specify it and just specify the forward reads. Files must be in fastq format and can be gzipped. \u2022 Illumina jumping/DiTag/other circularization protocol-based library mate pair reads: `JUMP = two_letter_prefix mean stdev /PATH/fwd_reads.fastq /PATH/rev_reads.fastq` example: `JUMP = cc 3500 500 /data/jump_fwd_reads.fastq /data/jump_rev_reads.fastq` By default, the assembler assumes that the jumping library pairs are \u201coutties\u201d (<-- -->). Some protocols (DiTag) use double-circularization which results in \u201cinnie\u201d pairs (--> <--). In this case please specify negative mean. \u2022 Other types of data (454, Sanger, etc) must be converted to CABOG format FRG files (see CABOG documentation at http://sourceforge.net/apps/mediawiki/wgs-assembler/index.php?title=Main_Page ): `OTHER = data.frg` \u2022 PacBio/MinION data are supported. Note that you have to have 50x + coverage in Illumina Paired End reads to use PacBio of Oxford Nanopore MinION data. Supply PacBio or MinION reads in a single fasta or fastq file (can be gzipped) as: `PACBIO=file.fa` or `NANOPORE=file.fa` If you have both PacBio and Nanopore reads, cat them all into a single fasta file and supply them as \"NANOPORE\" type. More than one entry for each data type/set of files is allowed EXCEPT for PacBio/Nanopore data. If you have several pairs of PE or JUMP fastq files, specify each pair on a separate line with a different two-letter prefix. PACBIO or NANOPORE data must be in ONE file. PARAMETERS. The following parameters are mandatory: `NUM_THREADS=16` set it to the number of cores in the computer to be used for assembly `JF_SIZE=2000000000` jellyfish hash size, set this to about 10x the genome size. `GRID_QUEUE=all.q` mandatory if USE_GRID set to 1. Name of the SGE queue to use. NOTE that number of slots must be set to 1 for each physical computer Optional parameters: `USE_LINKING_MATES=1` Most of the paired end reads end up in the same super read and thus are not passed to the assembler. Those that do not end up in the same super read are called \u201dlinking mates\u201d . The best assembly results are achieved by setting this parameter to 1 for Illumina-only assemblies. If you have more than 2x coverage by long (454, Sanger, etc) reads, set this to 0. `GRAPH_KMER_SIZE=auto` this is the kmer size to be used for super reads. \u201cauto\u201d is the safest choice. Only advanced users should modify this parameter. `LIMIT_JUMP_COVERAGE = 60` in some cases (especially for bacterial assemblies) the jumping library has too much coverage which confuses the assembler. By setting this parameter you can have assembler down-sample the jumping library to 60x (from above) coverage. For bigger eukaryotic genomes you can set this parameter to 300. `CA_PARAMETERS = cgwErrorRate=0.1 these are the additional parameters to Celera Assembler, and they should only be supplied/modified by advanced users. \u201covlMerSize=30 cgwErrorRate=0.25 ovlMemory=4GB\u201d should be used for bacterial assemblies; \u201covlMerSize=30 cgwErrorRate=0.15 ovlMemory=4GB\u201d should be used for all other genomes `SOAP_ASSEMBLY = 0` Set this to 1 if you would like to perform contigging and scaffolding done by SOAPdenovo2 instead of CABOG. This will decrease assembly runtime, but will likely result in inferior assembly. This option is useful when assembling very large genomes (5Gbp+), as CABOG may take months to rung on these. # Running MaSuRCA hybrid assembly on the grid. Starting with version 3.2.4 MaSuRCA supports execution of the mega-reads correction of the long high error reads such as PacBio or Nanopore reads on SGE grid. SLURM support is implemented in 3.3.1 and later releases. To run MaSuRCA on the grid, you must set USE_GRID=1 and specify the SGE queue to use (MANDATORY) using GRID_QUEUE=<name>. Please create a special queue on the SGE grid for this operation. The queue MUST have ONLY 1 SLOT PER PHYSICAL COMPUTER. Each batch will use NUM_THREADS threads on each computer, thus the number of slots in the SGE queue should be set to 1 for each physical computer. Long reads are corrected in batches. GRID_BATCH_SIZE is the batch size in bases of long reads. You can figure out the number of batches by dividing the total amount of long read data by the GRID_BATCH_SIZE. Memory usage for each batch does not depend on this parameter, it scales with genome size instead. Since there is some overhead to starting each batch, I recommend setting this to have not more than 100 batches. The setting that is equal to 2-3 times the number of physical computers you have in the grid works best. The total number of batches is limited to 1000. The Celera Assembler (or CABOG) will use the grid as well to run overlapper only. Overlapper jobs are not memory-intensive and thus multiple jobs can be submitted to each node. Each job will use up to 4 threads \u2013 the code is not efficient enough to use more than that number of threads effectively. # If you only need to output corrected PacBio or Nanopore reads or super-reads. You can abort the assemble.sh script after it reports \"Running assembly\" if you only need corrected reads or super-reads output. The super reads file is work1/superReadSequences.fasta. The PacBio corrected reads are in mr.41.15.17.0.029.1.fa. The Nanopore corrected reads are in mr.41.15.15.0.02.1.fa. The PacBio/Nanopore corrected read names correspond to the original read names. # The masurca and the assemble.sh scripts. Once you\u2019ve created a configuration file, use the `masurca` script from the MaSuRCA bin directory to generate the `assemble.sh` shell script that executes the assembly: `$ /install_path/ MaSuRCA-X.X.X/bin/masurca config.txt` To run the assembly, execute `assemble.sh`. Typically upon completion of the successful assembly, the current directory, where `assemble.sh` was generated, will contain the following files, in reverse chronological order: `$ ls \u2013lth` FILE CONTAINS `gapClose.err` STDOUT/STDERR for gap filling CA CABOG folder \u2013 the final assembly ends up in CA/9-terminator or CA/10-gapclose (if gapClose succeeded, most of the time) `runCA2.out`', 'CABOG stdout for scaffolder `tigStore.err` Stderr for tigStore `unitig_cov.txt` CABOG coverage statistics `global_arrival_rate.txt` CABOG coverage statistics, global arrival rate `unitig_layout.txt` Unitig layout/sequences used to recomputed the coverage statistics `genome.uid` File relating UID to read name for CABOG `runCA1.out` CABOG stdout for unitig consensus `runCA0.out` CABOG stdout for initial stages: store building, overlapping, unitigging `superReadSequences_shr.frg` FRG file for super reads, super reads >2047br are shredded with an overlap of 1500bp `pe.linking.frg` FRG file of PE pairs where the two reads ended up in different super reads `pe.linking.fa` Fasta file of PE pairs where the two reads ended up in different super reads `work1/` Working directory of the super reads code that generates the super reads from PE reads `super1.err` STDERR output of the super reads code that generates the super reads from PE reads `sj.cor.clean.frg` CABOG FRG file with corrected jumping library pairs, redundant and non-junction removed, coverage limited `sj.cor.ext.reduced.fa` Fasta file with corrected jumping library pairs, redundant and non-junction removed, coverage limited `mates_to_break.txt` File that lists the jumping library mates that are to be removed, if the jumping library clone coverage exceeds the LIMIT_JUMP_COVERAGE parameter `compute_jump_coverage.txt` Supplementary file used to compute clone coverage of the jumping library, post-filtering `sj.cor.clean.fa` Fasta file with corrected jumping library pairs, redundant and non-junction removed `redundant_sj.txt` Text file with names of the redundant jumping library mate pairs `chimeric_sj.txt` text file with names of the non-junction jumping library mate pairs `work2/` working directory for the super reads code used to filter the JUMP libraries for non-junction/redundant pairs `work2.1/` working directory for secondary jumping filter based on the variable k-mer size `super2.err` STDERR output of the super reads code used to filter the JUMP libraries for non-junction/redundant pairs `guillaumeKUnitigsAtLeast32bases_all.fasta` fasta file for k-unitigs (see the paper referred above) k_u_0 jellyfish hash created from all error corrected reads, and used to estimate the genome size `??.cor.fa` error corrected JUMP reads, one such file for each library with \\'??\\' being the prefix. The ordering of the reads is arbitrary, but the pairs are guaranteed to appear together. No quality scores. `error_correct.log` log file for error correction `pe.cor.fa` error corrected PE reads. The ordering of the reads is arbitrary, but the pairs are guaranteed to appear together. No quality scores combined_0 special combined Jellyfish hash for error correction `pe_data.tmp` supplementary information to figure out the GC content and lengths of PE reads `sj.renamed.fastq` the ??.renamed.fastq file is created for each \u201cJUMP= \u2026\u201d entry in the configuration file. These file(s) contain renamed reads in the fastq format `meanAndStdevByPrefix.sj.txt` auto-generated file of \u201cfake\u201d mean and stdev for non-junction jumping library pairs. The Illumina protocol states that these are about 200-700bp long `pe.renamed.fastq` the `??.renamed.fastq file` is created for each \u201cPE= \u2026\u201d entry in the configuration file. These file(s) contain renamed reads in the fastq format. `meanAndStdevByPrefix.pe.txt` auto-generated file of means and stdevs for PE reads `assemble.sh` the original assemble.sh script # Restarting a failed assembly. If something fails or goes wrong, or you noticed a mistake made in configuration, you can stop and re-start the assembly as follows. \u2022 Terminate the assembly by Control-C or by killing the `assemble.sh` process \u2022 Examine the assembly folder and delete all files that contain incorrect/failed contents (see table above for file designations). \u2022 Run $/install_path/MaSuRCA-X.X.X/bin/masurca config.txt in the assembly directory. This will create a new `assemble.sh` script accounting for the files that are already present and checking for all dependencies to only run the steps that need to be run \u2022 Run ./assemble.sh For example \u2022 if you noticed that CABOG failed due to lack of disk space, then, after freeing some space, simply run $/install_path/MaSuRCA-X.X.X/bin/masurca config.txt and execute `assemble.sh` \u2022 if you noticed that you omitted or misspecified one of the jumping library files, add the files to the DATA section of config.txt and run $/install_path/ MaSuRCA-X.X.X/bin/masurca config.txt and execute `assemble.sh` \u2022 if error correction failed then remove the files named `??.cor.fa` and then run $/install_path/ MaSuRCA-X.X.X/bin/masurca config.txt and execute `assemble.sh` # Assembly result. The final assembly scaffolds file is under CA/{primary,alternative}.scf.fasta or CA.mr...../{primary,alternative}.scf.fasta if CABOG assembler was used for contigging/scaffolding, which is default, or flye.mr....../assembly.scaffolds.fasta if Flye was used for final contigging/scaffolding. # 4. Additional tools ## POLCA POLCA is a polishing tool aimed at improving the consensus accuracy in genome assemblies produced from long high error sequencing data generated by PacBio SMRT or Oxford Nanopore sequencing technologies. POLCA utilizes Illumina or PacBio HIFI reads for the same genome for improving the consensus quality of the assembly. Its inputs are the genome sequence and a fasta or fastq file (or files) of Illumina or PacBioHIFI reads and its outputs are the polished genome and a VCF file with the variants called from the read data. Please cite POLCA as follows: Zimin AV, Salzberg SL. The genome polishing tool POLCA makes fast and accurate corrections in genome assemblies. PLoS computational biology. 2020 Jun 26;16(6):e1007981. It is very important to use fastq files of polishing reads for proper operation of freebayes. If you have fasta files, you can convert it to fastq file faux quality scores (use \"G\" for quality). For example, ``` >read ACGT ``` becomes ``` @read ACGT + GGGG ``` POLCA has one external dependency: bwa mem aligner (http://bio-bwa.sourceforge.net/). It requires that bwa mem aligner is available on the $PATH. Usage: ``` polca.sh [options] -a <assembly fasta file> -r <\\'polishing _reads1.fastq polishing_reads2.fastq\\'> -t <number of cpus> -n <optional: do not fix errors, just call variants> -m <optional: memory per thread to use in samtools sort> ``` Example: polca.sh -a genome.fasta -r \\'reads1.fastq reads2.fastq.gz\\' -t 16 -m 1G POLCA also includes a tool to introduce random errors into an assembly. This tool is primarily useful for testing the polishing techniques. It is also useful for aligning to genomes that have long exact repeats (such as GRCh38) with Nucmer without use of --maxmatch option. To use the tool, one must perform two steps. First step is to generate a pseudo-VCF file with random errors by running: $ introduce_errors_fasta_file.pl sequence.fasta error_probability <optional:maximum_indel_size, default 20> > errors.evcf where sequence.fasta is the multi-fasta file of the original contigs/scaffolds, error_probability is a floating point number less than 1, equal to the probability of error per base. The last argument is optional, it specifies the maximum insertion/deletion size. By defaul, the last argument is 20. 90% of the errors introduced are substitutions and 10% are insertions and deletions. The rate of introduced errors is rougly equal to error_probability*(1+maximum_indel_size/20). To introduce the errors to the sequence.fasta file, run: $ fix_consensus_from_vcf.pl sequence.fasta < errors.evcf > sequence_with_errors.fasta ## Chromosome scaffolder The chromosome scaffolder tools allows to scaffold the assembled contigs using (large) reference scaffolds or chromosome sequences from the same or closely related species. For example, you\\'ve assembled a novel human genome and you wish to create a new reference genome with contigs placed on the chromosomes. The chromosome scaffolder will let you do exactly that. It will examine your contigs to see if there are any misassemblies in places where the contigs disagree with the reference using read alignments. The scaffolder will then break the contigs at all putative misassembled locations, creating clean contigs (you can disable that optionally). Then it will order and orient the clean contigs onto the chromosomes using the reference alignments. The scaffolder can be invoked as: ``` chromosome_scaffolder.sh [options] -r <reference genome> MANDATORY -q <assembly to be scaffolded with the reference> MANDATORY -t <number of threads> -i <minimum sequence similarity percentage: default 97> -m <merge equence alignments slack: default 100000> -nb do not align reads to query contigs and do not attempt to break at misassemblies: default off -v <verbose> -s <reads to align to the assembly to check for misassemblies> MANDATORY unless -nb set -cl <coverage threshold for splitting at misassemblies: default 3> -ch <repeat coverage threshold for splitting at misassemblies: default 30> -M attempt to fill unaligned gaps with reference contigs: defalut off -h|-u|--help this message ``` This tool is primarily designed for assemblies wigh good contiguity produced from long PacBio or Nanopore reads. The long reads (minimum 20x coverage) can be supplied with -s option. If you do not supply the lobg reads, you must set the -nb option which will skip splitting contigs and scaffold them as is. The -cl and -ch options set the coverage thresholds for splitting at suspect misassemblies, I recommend keeping -cl option at 3 and setting -ch option to about 1.5x the coverage of the long reads supplied with the -s option. ## SAMBA scaffolder SAMBA is a tool that is designed to scaffold and gap-fill existing genome assemblies with additional long-read data, resulting in substantially greater contiguity. SAMBA is the only tool of its kind that also computes and fills in the sequence for all spanned gaps in the scaffolds, yielding much longer contigs. Please cite SAMBA as follows: Zimin AV, Salzberg SL. The SAMBA tool uses long reads to improve the contiguity of genome assemblies. PLoS computational biology. 2022 Feb 4;18(2):e1009860. The invocation of SAMBA is as follows: ``` samba.sh [options] -r <contigs or scaffolds in fasta format> -q <long reads or another assembly used to scaffold in fasta or fastq format, can be gzipped> -t <number of threads> -d <scaffolding data type: ont, pbclr or asm, default:ont> -m <minimum matching length, default:5000> -o <maximum overhang, default:1000> -a <optional: allowed merges file in the format per line: contig1 contig2, only pairs of contigs listed will be considered for merging, useful for intrascaffold gap filling> -v verbose flag -h|--help|-u|--usage this message ``` SAMBA installs with MaSuRCA and requires no external dependencies. The only parameter that is worth modifying is -m or the minimum matching length. 2000-2500 is a good value for small eukaryotis genomes 100-400Mb in size, 5000 is the default best value for large eukaryotic genomes (2-3Gbp), and 9000-10000 is the best value for large highly repetitive plant genomes (5Gbp+). MaSuRCA also provides a wrapper script for SAMBA that allows to use SAMBA to close intra-scaffold gaps in an assembly. The usage is as follows: ``` close_scaffold_gaps.sh [options] -r <scaffolds to gapclose> MANDATORY -q <sequences used for closing gaps, can be long reads or another assembly, in fasta or fastq format, can ge gzipped> MANDATORY -t <number of threads, default:1> -i <identity% default:98> -m <minimum match length on the two sides of the gap, default:2500> -o <max overhang, default:1000> -v verbose flag -h|--help|-u|--usage this message ``` The above script will split the scaffolds into contigs and then run SAMBA only allowing intrascaffold gaps to be filled. Contig \"flips\" inside the scaffold are allowed, making this a great tool to gapfill and fix assemblies scaffolded with HiC data, because HiC scaffolding sometimes incorrectly flips contigs in scaffolds.']```"}, {"role": "user", "content": "Content from https://github.com/alekseyzimin/masurca:\n```\nRepository metadata:\n{'name': 'masurca', 'label': ['masurca'], 'description': [], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v4.1.2', 'v4.1.1', 'v4.1.0', 'v4.0.9', 'v4.0.8', 'v4.0.7', 'v4.0.6', 'v4.0.5', 'v4.0.4', 'v4.0.3', 'v4.0.2', 'v4.0.1', 'v3.4.2', 'v3.4.1', 'v3.4.0', 'v3.3.9', 'v3.3.8', 'v3.3.7', 'v3.3.6', 'v3.3.5', 'v3.3.4', 'v3.3.3', '3.3.2', 'v3.3.1', 'v3.3.0', '3.2.9', '3.2.8', '3.2.7', '3.2.6', '3.2.4.1', '3.2.4'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/alekseyzimin/masurca', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/masurca', 'term': 'masurca', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/genome', 'term': 'genome', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/assembly', 'term': 'assembly', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 3}], 'operations': [], 'authors': [{'term': {'name': 'gmarcais', 'type': 'person', 'email': 'gmarcais@cs.cmu.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Aleksey Zimin', 'type': 'person', 'email': 'aleksey.zimin@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Aleksey Zimin', 'type': 'person', 'email': 'alekseyz@salz2.idies.jhu.edu', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Aleksey Zimin', 'type': 'person', 'email': 'alekseyz@salz4.idies.jhu.edu', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Aleksey Zimin', 'type': 'person', 'email': 'alekseyz@ipst.umd.edu', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Aleksey Zimin', 'type': 'person', 'email': '27226909+alekseyzimin@users.noreply.github.com', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/alekseyzimin/masurca/blob/main/LICENSE.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/alekseyzimin/masurca/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'code_of_conduct', 'url': 'https://github.com/alekseyzimin/masurca/blob/main/code_of_conduct.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"msaboot/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/msaboot/cmd/0.1.2\",\n    \"name\": \"msaboot\",\n    \"description\": [\n      \"Generate bootstrapping replicates for multiple sequence alignment data.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/phac-nml/msaboot\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/phac-nml/msaboot\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Apache-2.0\",\n        \"url\": \"https://spdx.org/licenses/Apache-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/msaboot/cmd/0.1.2\",\n    \"name\": \"msaboot\",\n    \"description\": [\n      \"Output PHYLIP file with bootstrapped multiple sequence alignment data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fmsaboot%2Fmsaboot%2F0.1.2\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/phac-nml/msaboot:\n```\nREADME content:\n['## Introduction ## This tool can be used to generate bootstrapped replicates of multiple sequence alignments. The bootstrapped replicates are saved in relaxed PHYLIP format. ## Release ## MSABOOT 0.1.2 This release replaced the use of Biopython\\'s AlignIO.write function with a custom implementation. ## Installation ## Requires Python3 and pip3. pip3 install . ## Legal ## Copyright Government of Canada 2018 Written by: Matthew Fogel, Public Health Agency of Canada Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this work except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ## Contact ## **Gary van Domselaar**: gary.vandomselaar@canada.ca']```"}, {"role": "user", "content": "Content from https://github.com/phac-nml/msaboot:\n```\nRepository metadata:\n{'name': 'msaboot', 'label': ['msaboot'], 'description': [{'term': 'A Python tool for generating bootstrapping data from multiple sequence alignments.', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.1.2', 'v0.1.1', 'v0.1.0'], 'license': [{'term': {'name': 'Apache License 2.0', 'url': 'http://choosealicense.com/licenses/apache-2.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/phac-nml/msaboot', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Matthew Fogel', 'type': 'person', 'email': 'mfogel@corefacility.ca', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Eric Marinier', 'type': 'person', 'email': 'emarinier@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Matthew Fogel', 'type': 'person', 'email': 'matthew.fogel@canada.ca', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/phac-nml/msaboot/blob/main/LICENSE.txt'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/phac-nml/msaboot/blob/main/NOTICE.txt'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/phac-nml/msaboot/blob/main/README.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"morpheus/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/morpheus/cmd/287\",\n    \"name\": \"morpheus\",\n    \"description\": [\n      \"Mass spectrometry\u2013based proteomics database search algorithm.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/cwenger/Morpheus\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/cwenger/Morpheus/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8291110e2f3aa0b3b81f2\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/morpheus/cmd/2.255.0\",\n    \"name\": \"morpheus\",\n    \"description\": [\n      \"Database search algorithm for high-resolution tandem mass spectra.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fgalaxyp%2Fmorpheus%2Fmorpheus%2F2.255.0\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/cwenger/Morpheus/:\n```\nREADME content:\n[\"# Morpheus mass spectrometry\u2013based proteomics database search algorithm ## Requirements * Windows: [Microsoft .NET Framework](http://www.microsoft.com/net) 4.0 Client Profile or higher (4.5 or higher recommended) * [Thermo MSFileReader](https://thermo.flexnetoperations.com/control/thmo/search?query=MSFileReader) for Thermo version * Linux / Mac OS X: [Mono](http://www.mono-project.com/) 2.8 or later ## Notes * In revision 83 and later, sites inducing and preventing cleavage must be separated by commas in proteases.tsv, since multi-amino acid sites are now supported. * In revision 108 and later, proteome databases may be UniProt XML files in addition to FASTA files to support [G-PTM searching](http://pubs.acs.org/doi/abs/10.1021/acs.jproteome.5b00599). * When an XML file is loaded, its modifications will be read (masses are read from [ptmlist.txt](http://www.uniprot.org/docs/ptmlist)), added to your variable modifications list, and checked by default. * In revision 167 and above, an extra column was added to modifications.tsv to allow for neutral loss modification searching. * In revision 255 and above, mzIdentML support necessitated the need to link every modification to UniMod. Therefore, two extra columns, 'UniMod Accession Number' and 'UniMod Name', were were added to modifications.tsv.\"]```"}, {"role": "user", "content": "Content from https://github.com/cwenger/Morpheus/:\n```\nRepository metadata:\n{'name': 'Morpheus', 'label': ['Morpheus'], 'description': [{'term': 'Morpheus mass spectrometry\u2013based proteomics database search algorithm', 'id': 0}], 'links': [], 'webpage': [{'term': 'http://cwenger.github.io/Morpheus/', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['r290', 'r288', 'r287', 'r272', 'r255', 'r225', 'r99', 'r83', 'r55', 'r51', 'r45', 'r35', 'r34', 'r32', 'r31', 'r27', 'r23', 'r22', 'r200', 'r178', 'r171', 'r165', 'r142', 'r14', 'r131', 'r13', 'r110'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/cwenger/Morpheus', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Craig Wenger', 'type': 'person', 'email': 'craig.wenger@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Craig Wenger', 'type': 'person', 'email': 'cwenger@quantum-si.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'stefanks', 'type': 'person', 'email': 'solntsev@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Jim Johnson', 'type': 'person', 'email': 'jj@umn.edu', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/cwenger/Morpheus/blob/main/LICENSE.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/cwenger/Morpheus/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/cwenger/Morpheus/blob/main/exclude.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/cwenger/Morpheus:\n```\nREADME content:\n[\"# Morpheus mass spectrometry\u2013based proteomics database search algorithm ## Requirements * Windows: [Microsoft .NET Framework](http://www.microsoft.com/net) 4.0 Client Profile or higher (4.5 or higher recommended) * [Thermo MSFileReader](https://thermo.flexnetoperations.com/control/thmo/search?query=MSFileReader) for Thermo version * Linux / Mac OS X: [Mono](http://www.mono-project.com/) 2.8 or later ## Notes * In revision 83 and later, sites inducing and preventing cleavage must be separated by commas in proteases.tsv, since multi-amino acid sites are now supported. * In revision 108 and later, proteome databases may be UniProt XML files in addition to FASTA files to support [G-PTM searching](http://pubs.acs.org/doi/abs/10.1021/acs.jproteome.5b00599). * When an XML file is loaded, its modifications will be read (masses are read from [ptmlist.txt](http://www.uniprot.org/docs/ptmlist)), added to your variable modifications list, and checked by default. * In revision 167 and above, an extra column was added to modifications.tsv to allow for neutral loss modification searching. * In revision 255 and above, mzIdentML support necessitated the need to link every modification to UniMod. Therefore, two extra columns, 'UniMod Accession Number' and 'UniMod Name', were were added to modifications.tsv.\"]```"}, {"role": "user", "content": "Content from https://github.com/cwenger/Morpheus:\n```\nRepository metadata:\n{'name': 'Morpheus', 'label': ['Morpheus'], 'description': [{'term': 'Morpheus mass spectrometry\u2013based proteomics database search algorithm', 'id': 0}], 'links': [], 'webpage': [{'term': 'http://cwenger.github.io/Morpheus/', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['r290', 'r288', 'r287', 'r272', 'r255', 'r225', 'r99', 'r83', 'r55', 'r51', 'r45', 'r35', 'r34', 'r32', 'r31', 'r27', 'r23', 'r22', 'r200', 'r178', 'r171', 'r165', 'r142', 'r14', 'r131', 'r13', 'r110'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/cwenger/Morpheus', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Craig Wenger', 'type': 'person', 'email': 'craig.wenger@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Craig Wenger', 'type': 'person', 'email': 'cwenger@quantum-si.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'stefanks', 'type': 'person', 'email': 'solntsev@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Jim Johnson', 'type': 'person', 'email': 'jj@umn.edu', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/cwenger/Morpheus/blob/main/LICENSE.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/cwenger/Morpheus/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/cwenger/Morpheus/blob/main/exclude.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"lra/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"github/LRA/None/v1.3.7.1\",\n    \"name\": \"LRA\",\n    \"description\": [\n      \"Long read aligner.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/ChaissonLab/LRA\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"github\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"sourceforge/lra/None/None\",\n    \"name\": \"lra\",\n    \"description\": [\n      \"Latronix Regnum Arts - Open Source Faction is an open source and freeware developer for all sorts of goodies. Be it games or applications. This is the home for the Drug Turfs Game (A Drug Wars like game) and the Drug Turfs Library.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://sourceforge.net/projects/lra\",\n        \"kind\": \"sourceforge\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://lra.sourceforge.net/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"sourceforge\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/lra:\n```\nProject metadata:\n{'description': 'Latronix Regnum Arts - Open Source Faction is an open source and freeware developer for all sorts of goodies. Be it games or applications. This is the home for the Drug Turfs Game (A Drug Wars like game) and the Drug Turfs Library.', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['/p/lra/activity/', 'https://sourceforge.net/auth/?return_to=%2Fprojects%2Flra%2F', '/p/lra/activity/feed']}, {'text': 'Categories\\nGames\\n,\\nBio-Informatics\\nLicense\\nGNU General Public License version 2.0 (GPLv2)\\nCategories\\nGames\\n,\\nBio-Informatics\\nLicense\\nGNU General Public License version 2.0 (GPLv2)', 'hrefs': ['/directory/games/', '/directory/bio-informatics/']}, {'text': 'Follow LRA-OSF\\nLRA-OSF Web Site\\nFollow LRA-OSF\\nLRA-OSF Web Site', 'hrefs': ['http://lra.sourceforge.net']}, {'text': 'User Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n0\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n0\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n0\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n0\\n/ 5\\nUser Ratings\\n5.0\\nout of 5 stars\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\n1\\n0\\n0\\n0\\n0\\nease\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n0\\n/ 5\\nfeatures\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n0\\n/ 5\\ndesign\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n0\\n/ 5\\nsupport\\n1 of 5\\n2 of 5\\n3 of 5\\n4 of 5\\n5 of 5\\n0\\n/ 5', 'hrefs': []}, {'text': 'User Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\nmarcophoenix\\nPosted 2013-06-05\\nGreat stuff.\\n1\\nuser\\nfound this review helpful.\\nRead more reviews >\\nUser Reviews\\nFilter Reviews:\\nAll\\nAll\\n\u2605\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\u2605\\n\u2605\u2605\u2605\\n\u2605\u2605\\n\u2605\\nmarcophoenix\\nPosted 2013-06-05\\nGreat stuff.\\n1\\nuser\\nfound this review helpful.\\nRead more reviews >', 'hrefs': ['#', '/projects/lra/reviews/#reviews']}, {'text': 'Additional Project Details\\nOperating Systems\\nMS-DOS\\n,\\nMinGW/MSYS2\\n,\\nSolaris\\n,\\nLinux\\n,\\nBSD\\n,\\nWindows\\nLanguages\\nEnglish, Spanish\\nIntended Audience\\nScience/Research, Developers, End Users/Desktop\\nUser Interface\\n.NET/Mono, Win32 (MS Windows), wxWidgets\\nProgramming Language\\nC#\\n,\\nC++\\n,\\nAssembly\\n,\\nC\\nRelated Categories\\nC# Games\\n,\\nC# Bio-Informatics Software\\n,\\nC++ Games\\n,\\nC++ Bio-Informatics Software\\n,\\nAssembly Games\\n,\\nAssembly Bio-Informatics Software\\n,\\nC Games\\n,\\nC Bio-Informatics Software\\nRegistered\\n2003-05-24\\nAdditional Project Details\\nOperating Systems\\nMS-DOS\\n,\\nMinGW/MSYS2\\n,\\nSolaris\\n,\\nLinux\\n,\\nBSD\\n,\\nWindows\\nLanguages\\nEnglish, Spanish\\nIntended Audience\\nScience/Research, Developers, End Users/Desktop\\nUser Interface\\n.NET/Mono, Win32 (MS Windows), wxWidgets\\nProgramming Language\\nC#\\n,\\nC++\\n,\\nAssembly\\n,\\nC\\nRelated Categories\\nC# Games\\n,\\nC# Bio-Informatics Software\\n,\\nC++ Games\\n,\\nC++ Bio-Informatics Software\\n,\\nAssembly Games\\n,\\nAssembly Bio-Informatics Software\\n,\\nC Games\\n,\\nC Bio-Informatics Software\\nRegistered\\n2003-05-24', 'hrefs': ['/directory/bio-informatics/c/', '/directory/solaris/', '/directory/mingw-msys2/', '/directory/windows/', '/directory/games/c++/', '/directory/c-sharp/', '/directory/c/', '/directory/ms-dos/', '/directory/assembly/', '/directory/bio-informatics/c++/', '/directory/bsd/', '/directory/games/c#/', '/directory/games/assembly/', '/directory/bio-informatics/assembly/', '/directory/c-plus-plus/', '/directory/games/c/', '/directory/linux/', '/directory/bio-informatics/c#/']}]}```"}, {"role": "user", "content": "Content from http://lra.sourceforge.net/:\n```\nContent:\n['4/7/07 Drug Turfs game released. Drug Turfs is a game that resembles the classic DOS game Drug Wars. I developed a C++ class library to handle the game logic and algorithms, called DT_LIB (drug turfs library). The purpose of DT_LIB is to be a platform independent library that would allow me to develop the game in any architecture, framework, GUI API, or O/S that I want. By far I have implemented the Drug Turfs game in Console and a Win32 API GUI version. I intent to implement Drug Turfs in wxWidgets, MFC, and U++ as a test of a complete window application with a lot controls and see which GUI API is best for the job. The Drug Turfs Win32 GUI looks generic (nothing fancy, just a window with lot of edit controls, listboxes, and buttons). Hopefully the MFC and wxWidget versions will look much better. In regards to the game, it is fully functional and hopefully it should be fun to play. Enjoy. **[Contact Information]** email: [latronix@users.sourceforge.net](mailto:latronix@users.sourceforge.net) **Very Old** 5/24/03 (legacy) Back when I was in my first college C++ class in 2003, I did a very crappy console version of Drug Wars, which I called \"Puntos de Drogas\" (English: Drug Turfs). The program has a very lousy and crude design and I only include the news to ascertain the origin of the LRA and Drug Turfs projects.']```"}, {"role": "user", "content": "Content from https://github.com/ChaissonLab/LRA:\n```\nREADME content:\n['![](image/logo_small.png) ## <a name=\"started\"></a>Getting Started - [Getting Started](#started) ```sh # index reference genome lra index -CCS/CLR/ONT/CONTIG ref.fa # Map sequence to reference lra align -CCS/CLR/ONT/CONTIG ref.fa read.fa -t 16 -p s > output.sam ``` ## Table of Contents - [Introduction](#intro) - [Users\\' Guide](#uguide) - [Installation](#install) - [Index reference](#index) - [Align reads/contigs to reference](#align) - [Output format](#output) ## <a name=\"intro\"></a>Introduction <img align=\"middle\" width=\"600\" src=\"image/runtime.png\"/> <img align=\"middle\" width=\"600\" src=\"image/truvari_plot.png\"/> <br clear=\"left\"/> lra is a sequence alignment program that aligns long reads from single-molecule sequencing (SMS) instruments, or megabase-scale contigs from SMS assemblies. lra implements seed chaining sparse dynamic programming with a concave gap function to read and assembly alignment, which is also extended to allow for inversion cases. lra alignment approach increases sensitivity and specificity for SV discovery, particularly for variants above 1kb and when discovering variation from ONT reads, while having runtime that arecomparable (1.05-3.76\u00d7) to current methods. When applied to calling variation from *de novo* assembly contigs, there is a 3.2% increase in Truvari F1 score compared to minimap2+htsbox. ## <a name=\"uguide\"></a>Users\\' Guide ### <a name=\"install\"></a>Installation Install lra by bioconda: `conda install -c bioconda lra` Install lra from github or release: The dependencies are [zlib][zlib], [htslib][htslib]. Users can install `zlib` and `htslib` through conda and build lra in conda environment. 1. `conda activate env`; 2. Install dependency: `conda install -c bioconda htslib` and `conda install -c anaconda zlib`; 3. Get released latest source code from github `wget https://github.com/ChaissonLab/lra/archive/VX.XX.tar.gz && tar -xvf VX.XX.tar.gz && cd lra-X.XX/ && make`. Or get source code directly from the master branch `git clone --recursive https://github.com/ChaissonLab/lra.git -b master && cd lra && make`. You are all set for the installation! ### <a name=\"index\"></a>Index reference lra needs to first build a two-tiered minimizer indexes (global and local) for the reference before mapping. Both can be built at once using commands: ``` lra index -CCS/CLR/ONT/CONTIG ref.fa ``` lra has different parameters setting for the index when aligning reads from different sequencing instruments (CCS/CLR/ONT/CONTIG). You can also custimize the parameters. Details see `lra index --help`. lra takes a few minutes to index the human reference genome. Alternatively the global and local indexes may be built separately: ``` lra global -CCS/CLR/ONT/CONTIG ref.fa lra local -CCS/CLR/ONT/CONTIG ref.fa ``` ### <a name=\"align\"></a>Align reads/contigs to reference lra takes reads fasta, fastq or bam format in the mapping step. The output format can be SAM, PAF, BED and pairwise alignment. Details see `lra align --help`. The usage of multiple threads can be specified by `-t`. lra uses the same base algorithm for mapping all datatypes with different parameters settings. It is recommended to choose among `CCS/CLR/ONT/CONTIG` based on the accuracy and average length of the input reads. ``` lra align -CCS/CLR/ONT/CONTIG ref.fa read.fa -t 16 -p s > output.sam lra align -CCS/CLR/ONT/CONTIG ref.fa read.fa -t 16 -p p > output.paf lra align -CCS/CLR/ONT/CONTIG ref.fa read.fa -t 16 -p b > output.bed ``` If you have read.fa.gz, you may pip the read.fa to lra. ``` zcat read.fa.gz | lra align -CCS ref.fa /dev/stdin -t -p s > output.sam ``` ### <a name=\"output\"></a>Output format lra uses a set of customized tags in SAM and PAF output. |Tag|Type |Description | |:--|:----:|:-------------------------------------------------------------------| |NM |i |Number of mismatches + insertions + deletions in the alignment. | |NX |i |Number of mismatches in the alignment. | |ND |i |Number of bases of deletions in the alignment. | |TD |i |Number of deletions in the alignment. | |NI |i |Number of bases of insertions in the alignment. | |TI |i |Number of insertions in the alignment. | |NV |f |The alignment score. | |TP |A |Type of aln, P/primary, S/secondary, I/inversion. | |RT |i |runtime. | |CG |z |CIGAR string. | |AO |i |This number shows the order of the aligned segment when a read is split.| [zlib]: http://zlib.net/ [htslib]: https://github.com/samtools/htslib/']```"}, {"role": "user", "content": "Content from https://github.com/ChaissonLab/LRA:\n```\nRepository metadata:\n{'name': 'LRA', 'label': ['LRA'], 'description': [{'term': 'Long read aligner', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.3.7.1', '1.3.7.2', 'v1.3.6', 'v1.3.5', '1.3.4', 'v1.3.3', '1.3.2', '1.3.1', '1.3.0', 'V1.2.0', 'V1.1.2', 'V1.1.1', 'V1.1.0', 'V1.0.10', '1.0.4', '1.0.3', 'V1.0.9', 'V1.0.8', 'V1.0.7', 'V1.0.6', 'V1.0.5', 'V1.0.2', 'V1.0.1', 'v1.0.0'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/ChaissonLab/LRA', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Mark Chaisson', 'type': 'person', 'email': 'mchaisso@usc.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'jingwen7', 'type': 'person', 'email': 'jingwenr@usc.edu', 'maintainer': False}, 'id': 1}, {'term': {'name': 'jo-mc', 'type': 'person', 'email': '38674063+jo-mc@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Niema Moshiri', 'type': 'person', 'email': 'niemamoshiri@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'jingwenr', 'type': 'person', 'email': 'jingwenr@endeavour1.usc.edu', 'maintainer': False}, 'id': 4}, {'term': {'name': 'jingwenr', 'type': 'person', 'email': 'jingwenr@endeavour2.hpc.usc.edu', 'maintainer': False}, 'id': 5}, {'term': {'name': 'jingwenr', 'type': 'person', 'email': 'jingwenr@e10-06.hpc.usc.edu', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Mark Chaisson', 'type': 'person', 'email': 'mchaisso@gmail.com', 'maintainer': False}, 'id': 7}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/ChaissonLab/LRA/blob/main/LICENSE.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/ChaissonLab/LRA/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"longstitch/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/longstitch/cmd/1.0.5\",\n    \"name\": \"longstitch\",\n    \"description\": [\n      \"A genome assembly correction and scaffolding pipeline using long reads.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/bcgsc/LongStitch\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://bcgsc.ca/resources/software/longstitch\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8335110e2f3aa0b3bb8d0\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/longstitch/cmd/None\",\n    \"name\": \"longstitch\",\n    \"description\": [\n      \"A genome assembly correction and scaffolding pipeline using long reads.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/bcgsc/longstitch\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Lauren Coombe\",\n        \"email\": \"lcoombe@bcgsc.ca\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Inanc Birol\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Janet X Li\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Theodora Lo\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Johnathan Wong\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Vladimir Nikolic\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Ren\u00e9 L Warren\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8335110e2f3aa0b3bb8d0\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/bcgsc/longstitch:\n```\nREADME content:\n['![GitHub release (latest by date)](https://img.shields.io/github/v/release/bcgsc/longstitch) [![Published in BMC Bioinformatics](https://img.shields.io/badge/Published%20in-BMC%20Bioinformatics-blue)](https://doi.org/10.1186/s12859-021-04451-7) [![Conda](https://img.shields.io/conda/dn/bioconda/longstitch?label=Conda)](https://anaconda.org/bioconda/longstitch) ![Logo](https://github.com/bcgsc/longstitch/blob/master/longstitch-logo.png) # LongStitch A genome assembly correction and scaffolding pipeline using long reads, consisting of up to three steps: * [Tigmint](https://github.com/bcgsc/tigmint) cuts the draft assembly at potentially misassembled regions * [ntLink](https://github.com/bcgsc/ntLink) is then used to scaffold the corrected assembly * followed by [ARKS](https://github.com/bcgsc/arcs) for further scaffolding (optional extra step of scaffolding) <img src=\"https://github.com/bcgsc/LongStitch/blob/master/LongStitch_overview.png\" width=\"480\"> ### Credits LongStitch was developed and designed by Lauren Coombe, Janet Li, Theodora Lo and Rene Warren. ### Citing LongStitch If you use LongStitch in your research, please cite: Coombe L, Li JX, Lo T, Wong J, Nikolic V, Warren RL and Birol I. LongStitch: high-quality genome assembly correction and scaffolding using long reads. BMC Bioinformatics 22, 534 (2021). https://doi.org/10.1186/s12859-021-04451-7 ### Installing LongStitch LongStitch is available from conda: ``` conda install -c bioconda -c conda-forge longstitch ``` All dependencies for LongStitch are also available from homebrew: ``` brew tap brewsci/bio brew install tigmint ntlink arcs ``` Alternatively, use the latest release tarball: ``` wget https://github.com/bcgsc/LongStitch/releases/download/v1.0.5/longstitch-1.0.5.tar.gz ``` #### Dependencies * GNU Make * [Tigmint](https://github.com/bcgsc/tigmint) v1.2.4+ * [ntLink](https://github.com/bcgsc/ntLink) v1.3.3+ * [ARCS/ARKS](https://github.com/bcgsc/arcs) v1.2.5+ * [ABySS](https://github.com/bcgsc/abyss) v2.3.0+ * [LINKS](https://github.com/bcgsc/links) v1.8.7 * [samtools](http://www.htslib.org/) ### Example command For example, to run the default pipeline on a draft assembly `draft-assembly.fa` with the reads `reads.fa.gz` and a genome size of `gsize`: ``` longstitch run draft=draft-assembly reads=reads G=gsize ``` Note that specifying `G` is required when `span=auto` for Tigmint-long, and that all input sequences files should be in single-line fasta/fastq format. The output scaffolds can be found in soft-links with the suffix `longstitch-scaffolds.fa` ### LongStitch demo To test your LongStitch installation and see examples of how to run the pipeline, see `tests/run_longstitch_demo.sh` To run the demo script, ensure all dependencies are in your PATH, and run the bash script: ``` cd tests ./run_longstitch_demo.sh ``` ### Full help page To run the LongStitch pipeline, you can use the Makefile driver script `longstitch`. ``` Usage: ./longstitch [COMMAND] [OPTION=VALUE]\u2026 Commands: run run default LongStitch pipeline: Tigmint, then ntLink tigmint-ntLink-arks run full LongStitch pipeline: Tigmint, ntLink, then ARCS in kmer mode tigmint-ntLink run Tigmint, then ntLink (Same as \\'run\\' target) ntLink-arks run ntLink, then run ARCS in kmer mode General options (required): draft draft name [draft]. File must have .fa extension reads read name [reads]. The reads file can be uncompressed or gzipped. Accepted read file extensions: .fq, .fq.gz, .fastq, .fastq.gz, .fa, .fa.gz, .fasta, .fasta.gz General options (optional): t number of threads [8] z minimum size of contig (bp) to scaffold [1000] out_prefix if supplied, final scaffolds will be soft-linked to <out_prefix>.scaffolds.fa Tigmint options: span min number of spanning molecules to be considered correctly assembled [auto] dist maximum distance between alignments to be considered the same molecule [auto] G haploid genome size (bp) for calculating span parameter (e.g. \\'3e9\\' for human genome). Required when span=auto [0] longmap long read technology - used for minimap2 preset. \\'ont\\' for nanopore, \\'pb\\' for pacbio, \\'hifi\\' for pacbio HiFi reads [ont] ntLink options: k_ntLink k-mer size for minimizers [32] w window size for minimizers [100] gap_fill use gap-filling feature [False] rounds number of ntLink rounds [1] ARCS+LINKS options: j minimum fraction of read kmers matching a contigId (used in kmer mode) [0.05] k_arks size of a k-mer (used in kmer mode) [20] c minimum aligned read pairs per molecule [4] l minimum number of links to compute scaffold [4] a maximum link ratio between two best contain pairs [0.3] Notes: - by default, span is automatically calculated as 1/4 of the sequence coverage of the input long reads - G (genome size) must be specified if span=auto - by default, dist is automatically calculated as p5 of the input long read lengths - Ensure that all input files are in the current working directory, making soft-links if needed ``` ## Tips for running LongStitch ### Optimizing k/w for ntLink step * The default k (`k_ntLink`) and w (`w`) values for ntLink generally work well, but (depending on your input data) you may get better results by tuning these parameters * Generally, we suggest setting the k-mer and window size to values in these approximate ranges: * `k_ntLink` (k-mer size): 24-40 * `w` (window size): 100-500 * These values can be optimized using a grid search * For example, trying all combinations of k-mer sizes 24, 32, 40 and window sizes 100, 250, 500 ### Running the default pipeline or including ARKS-long * The default LongStitch pipeline consists of Tigmint-long + ntLink, but you can also run an additional scaffolding step with ARKS-long by specifying `tigmint-ntLink-arks` as the target in your command * Different results from these steps are expected for different input data * Some datasets will show more gains with the additional scaffolding step than others * Generally, if you want to be more conservative in terms of minimizing misassemblies and faster runtimes, the default pipeline (`run`, Tigmint-long + ntLink) is recommended. However, if you want to maximize scaffolding and contiguity, running the additional ARKS-long step (`tigmint-ntLink-arks`) is often valuable * See the [LongStitch paper](https://doi.org/10.1101/2021.06.17.448848) for more details and examples ### Changing parameters for minimap2 * `minimap2` is used for mapping reads in the Tigmint step * To change the (`-x`) preset used for mapping, specify `longmap=<mode>` * For example, to use the nanopore mapping preset, use `longmap=ont` (default), or for PacBio use `longmap=pb` ### Running LongStitch in pipelines * To change to a particular before running LongStitch, you can use the `-C dir` option with the `longstitch` command * All input files must be in the working directory for `longstitch` - these can either be created manually or using the `longstitch make_links` command * This command only requires the parameters `reads_path` and `draft_path` to be set - _full_ paths to the reads file and draft fasta file, respectively ## License LongStitch Copyright (c) 2020 British Columbia Cancer Agency Branch. All rights reserved. LongStitch is released under the GNU General Public License v3 This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, version 3. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/. For commercial licensing options, please contact Patrick Rebstein (prebstein@bccancer.bc.ca).']```"}, {"role": "user", "content": "Content from https://github.com/bcgsc/longstitch:\n```\nRepository metadata:\n{'name': 'LongStitch', 'label': ['LongStitch'], 'description': [{'term': 'Correct and scaffold assemblies using long reads', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.0.5', 'v1.0.4', 'v1.0.3', 'v1.0.2', 'v1.0.1', 'v1.0.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/bcgsc/LongStitch', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Lauren Coombe', 'type': 'person', 'email': 'lauren.e.coombe@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Ren\u00e9', 'type': 'person', 'email': 'rwarren@bcgsc.ca', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Johnathan Wong', 'type': 'person', 'email': '34543031+jowong4@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'theottlo', 'type': 'person', 'email': '33464594+theottlo@users.noreply.github.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Janet Li', 'type': 'person', 'email': '56139490+janetxinli@users.noreply.github.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Theodora Lo', 'type': 'person', 'email': 'tlo@hpce705.hpc.bcgsc.ca', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/bcgsc/longstitch/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/bcgsc/LongStitch:\n```\nREADME content:\n['![GitHub release (latest by date)](https://img.shields.io/github/v/release/bcgsc/longstitch) [![Published in BMC Bioinformatics](https://img.shields.io/badge/Published%20in-BMC%20Bioinformatics-blue)](https://doi.org/10.1186/s12859-021-04451-7) [![Conda](https://img.shields.io/conda/dn/bioconda/longstitch?label=Conda)](https://anaconda.org/bioconda/longstitch) ![Logo](https://github.com/bcgsc/longstitch/blob/master/longstitch-logo.png) # LongStitch A genome assembly correction and scaffolding pipeline using long reads, consisting of up to three steps: * [Tigmint](https://github.com/bcgsc/tigmint) cuts the draft assembly at potentially misassembled regions * [ntLink](https://github.com/bcgsc/ntLink) is then used to scaffold the corrected assembly * followed by [ARKS](https://github.com/bcgsc/arcs) for further scaffolding (optional extra step of scaffolding) <img src=\"https://github.com/bcgsc/LongStitch/blob/master/LongStitch_overview.png\" width=\"480\"> ### Credits LongStitch was developed and designed by Lauren Coombe, Janet Li, Theodora Lo and Rene Warren. ### Citing LongStitch If you use LongStitch in your research, please cite: Coombe L, Li JX, Lo T, Wong J, Nikolic V, Warren RL and Birol I. LongStitch: high-quality genome assembly correction and scaffolding using long reads. BMC Bioinformatics 22, 534 (2021). https://doi.org/10.1186/s12859-021-04451-7 ### Installing LongStitch LongStitch is available from conda: ``` conda install -c bioconda -c conda-forge longstitch ``` All dependencies for LongStitch are also available from homebrew: ``` brew tap brewsci/bio brew install tigmint ntlink arcs ``` Alternatively, use the latest release tarball: ``` wget https://github.com/bcgsc/LongStitch/releases/download/v1.0.5/longstitch-1.0.5.tar.gz ``` #### Dependencies * GNU Make * [Tigmint](https://github.com/bcgsc/tigmint) v1.2.4+ * [ntLink](https://github.com/bcgsc/ntLink) v1.3.3+ * [ARCS/ARKS](https://github.com/bcgsc/arcs) v1.2.5+ * [ABySS](https://github.com/bcgsc/abyss) v2.3.0+ * [LINKS](https://github.com/bcgsc/links) v1.8.7 * [samtools](http://www.htslib.org/) ### Example command For example, to run the default pipeline on a draft assembly `draft-assembly.fa` with the reads `reads.fa.gz` and a genome size of `gsize`: ``` longstitch run draft=draft-assembly reads=reads G=gsize ``` Note that specifying `G` is required when `span=auto` for Tigmint-long, and that all input sequences files should be in single-line fasta/fastq format. The output scaffolds can be found in soft-links with the suffix `longstitch-scaffolds.fa` ### LongStitch demo To test your LongStitch installation and see examples of how to run the pipeline, see `tests/run_longstitch_demo.sh` To run the demo script, ensure all dependencies are in your PATH, and run the bash script: ``` cd tests ./run_longstitch_demo.sh ``` ### Full help page To run the LongStitch pipeline, you can use the Makefile driver script `longstitch`. ``` Usage: ./longstitch [COMMAND] [OPTION=VALUE]\u2026 Commands: run run default LongStitch pipeline: Tigmint, then ntLink tigmint-ntLink-arks run full LongStitch pipeline: Tigmint, ntLink, then ARCS in kmer mode tigmint-ntLink run Tigmint, then ntLink (Same as \\'run\\' target) ntLink-arks run ntLink, then run ARCS in kmer mode General options (required): draft draft name [draft]. File must have .fa extension reads read name [reads]. The reads file can be uncompressed or gzipped. Accepted read file extensions: .fq, .fq.gz, .fastq, .fastq.gz, .fa, .fa.gz, .fasta, .fasta.gz General options (optional): t number of threads [8] z minimum size of contig (bp) to scaffold [1000] out_prefix if supplied, final scaffolds will be soft-linked to <out_prefix>.scaffolds.fa Tigmint options: span min number of spanning molecules to be considered correctly assembled [auto] dist maximum distance between alignments to be considered the same molecule [auto] G haploid genome size (bp) for calculating span parameter (e.g. \\'3e9\\' for human genome). Required when span=auto [0] longmap long read technology - used for minimap2 preset. \\'ont\\' for nanopore, \\'pb\\' for pacbio, \\'hifi\\' for pacbio HiFi reads [ont] ntLink options: k_ntLink k-mer size for minimizers [32] w window size for minimizers [100] gap_fill use gap-filling feature [False] rounds number of ntLink rounds [1] ARCS+LINKS options: j minimum fraction of read kmers matching a contigId (used in kmer mode) [0.05] k_arks size of a k-mer (used in kmer mode) [20] c minimum aligned read pairs per molecule [4] l minimum number of links to compute scaffold [4] a maximum link ratio between two best contain pairs [0.3] Notes: - by default, span is automatically calculated as 1/4 of the sequence coverage of the input long reads - G (genome size) must be specified if span=auto - by default, dist is automatically calculated as p5 of the input long read lengths - Ensure that all input files are in the current working directory, making soft-links if needed ``` ## Tips for running LongStitch ### Optimizing k/w for ntLink step * The default k (`k_ntLink`) and w (`w`) values for ntLink generally work well, but (depending on your input data) you may get better results by tuning these parameters * Generally, we suggest setting the k-mer and window size to values in these approximate ranges: * `k_ntLink` (k-mer size): 24-40 * `w` (window size): 100-500 * These values can be optimized using a grid search * For example, trying all combinations of k-mer sizes 24, 32, 40 and window sizes 100, 250, 500 ### Running the default pipeline or including ARKS-long * The default LongStitch pipeline consists of Tigmint-long + ntLink, but you can also run an additional scaffolding step with ARKS-long by specifying `tigmint-ntLink-arks` as the target in your command * Different results from these steps are expected for different input data * Some datasets will show more gains with the additional scaffolding step than others * Generally, if you want to be more conservative in terms of minimizing misassemblies and faster runtimes, the default pipeline (`run`, Tigmint-long + ntLink) is recommended. However, if you want to maximize scaffolding and contiguity, running the additional ARKS-long step (`tigmint-ntLink-arks`) is often valuable * See the [LongStitch paper](https://doi.org/10.1101/2021.06.17.448848) for more details and examples ### Changing parameters for minimap2 * `minimap2` is used for mapping reads in the Tigmint step * To change the (`-x`) preset used for mapping, specify `longmap=<mode>` * For example, to use the nanopore mapping preset, use `longmap=ont` (default), or for PacBio use `longmap=pb` ### Running LongStitch in pipelines * To change to a particular before running LongStitch, you can use the `-C dir` option with the `longstitch` command * All input files must be in the working directory for `longstitch` - these can either be created manually or using the `longstitch make_links` command * This command only requires the parameters `reads_path` and `draft_path` to be set - _full_ paths to the reads file and draft fasta file, respectively ## License LongStitch Copyright (c) 2020 British Columbia Cancer Agency Branch. All rights reserved. LongStitch is released under the GNU General Public License v3 This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, version 3. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/. For commercial licensing options, please contact Patrick Rebstein (prebstein@bccancer.bc.ca).']```"}, {"role": "user", "content": "Content from https://github.com/bcgsc/LongStitch:\n```\nRepository metadata:\n{'name': 'LongStitch', 'label': ['LongStitch'], 'description': [{'term': 'Correct and scaffold assemblies using long reads', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.0.5', 'v1.0.4', 'v1.0.3', 'v1.0.2', 'v1.0.1', 'v1.0.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/bcgsc/LongStitch', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Lauren Coombe', 'type': 'person', 'email': 'lauren.e.coombe@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Ren\u00e9', 'type': 'person', 'email': 'rwarren@bcgsc.ca', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Johnathan Wong', 'type': 'person', 'email': '34543031+jowong4@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'theottlo', 'type': 'person', 'email': '33464594+theottlo@users.noreply.github.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Janet Li', 'type': 'person', 'email': '56139490+janetxinli@users.noreply.github.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Theodora Lo', 'type': 'person', 'email': 'tlo@hpce705.hpc.bcgsc.ca', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/bcgsc/LongStitch/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://bcgsc.ca/resources/software/longstitch:\n```\nContent:\n['**LongStitch** LongStitch is a _de novo_ genome assembly correction and scaffolding pipeline. LongStitch runs in up to three stages, which includes initial assembly correction (Tigmint-long), followed by two incremental scaffolding stages (ntLink and ARKS-long). If you use LongStitch in your research, please cite: Coombe L, Li JX, Lo T, Wong J, Nikolic V, Warren RL and Birol I. LongStitch: high-quality genome assembly correction and scaffolding using long reads. BMC Bioinformatics 22, 534 (2021). [https://doi.org/10.1186/s12859-021-04451-7](https://doi.org/10.1186/s12859-021-04451-7) **Current release:** Github release page for LongStitch: [https://github.com/bcgsc/LongStitch/releases](https://github.com/bcgsc/LongStitch/releases)']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"mash/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/mash/cmd/2.3\",\n    \"name\": \"mash\",\n    \"description\": [\n      \"Fast sequence distance estimator that uses MinHash.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/marbl/Mash\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/marbl/Mash\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"https://github.com/marbl/Mash/blob/master/.txt\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/mash/cmd/None\",\n    \"name\": \"mash\",\n    \"description\": [\n      \"Fast genome and metagenome distance estimation using MinHash.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/marbl/mash\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Adam M. Phillippy\",\n        \"email\": \"adam.phillippy@nih.gov\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c828b910e2f3aa0b3b7ec2\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/marbl/Mash:\n```\nREADME content:\n['Mash is normally distributed as a dependency-free binary for Linux or OSX (see https://github.com/marbl/Mash/releases). This source distribution is intended for other operating systems or for development. Mash requires c++14 to build, which is available in and GCC >= 5 and XCode >= 6. See http://mash.readthedocs.org for more information.']```"}, {"role": "user", "content": "Content from https://github.com/marbl/Mash:\n```\nRepository metadata:\n{'name': 'Mash', 'label': ['Mash'], 'description': [{'term': 'Fast genome and metagenome distance estimation using MinHash', 'id': 0}], 'links': [], 'webpage': [{'term': 'mash.readthedocs.org', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v2.3', 'v2.2', 'v2.1.1', 'v2.1', 'v2.0', 'v1.1.1', 'v1.1', 'v1.0.2', 'v1.0.1', 'v1.0'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/marbl/Mash', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Brian Ondov', 'type': 'person', 'email': 'ondovb@umd.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Brian Ondov', 'type': 'person', 'email': 'brian.ondov@nih.gov', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Brian P. Walenz', 'type': 'person', 'email': 'thebri@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Fabian Kl\u00f6tzl', 'type': 'person', 'email': 'fabian@kloetzl.info', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Torsten Seemann', 'type': 'person', 'email': 'torsten.seemann@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Brian Ondov', 'type': 'person', 'email': 'brian.ondov@gmail.com', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Florian Breitwieser', 'type': 'person', 'email': 'Florian Breitwieser', 'maintainer': False}, 'id': 6}, {'term': {'name': 'S\u00e9bastien Boisvert', 'type': 'person', 'email': 'sebastien.boisvert.3@ulaval.ca', 'maintainer': False}, 'id': 7}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'installation', 'url': 'https://github.com/marbl/Mash/blob/main/INSTALL.txt'}, 'id': 0}, {'term': {'type': 'license', 'url': 'https://github.com/marbl/Mash/blob/main/LICENSE.txt'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/marbl/Mash/blob/main/README.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/marbl/mash:\n```\nREADME content:\n['Mash is normally distributed as a dependency-free binary for Linux or OSX (see https://github.com/marbl/Mash/releases). This source distribution is intended for other operating systems or for development. Mash requires c++14 to build, which is available in and GCC >= 5 and XCode >= 6. See http://mash.readthedocs.org for more information.']```"}, {"role": "user", "content": "Content from https://github.com/marbl/mash:\n```\nRepository metadata:\n{'name': 'Mash', 'label': ['Mash'], 'description': [{'term': 'Fast genome and metagenome distance estimation using MinHash', 'id': 0}], 'links': [], 'webpage': [{'term': 'mash.readthedocs.org', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v2.3', 'v2.2', 'v2.1.1', 'v2.1', 'v2.0', 'v1.1.1', 'v1.1', 'v1.0.2', 'v1.0.1', 'v1.0'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/marbl/Mash', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Brian Ondov', 'type': 'person', 'email': 'ondovb@umd.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Brian Ondov', 'type': 'person', 'email': 'brian.ondov@nih.gov', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Brian P. Walenz', 'type': 'person', 'email': 'thebri@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Fabian Kl\u00f6tzl', 'type': 'person', 'email': 'fabian@kloetzl.info', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Torsten Seemann', 'type': 'person', 'email': 'torsten.seemann@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Brian Ondov', 'type': 'person', 'email': 'brian.ondov@gmail.com', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Florian Breitwieser', 'type': 'person', 'email': 'Florian Breitwieser', 'maintainer': False}, 'id': 6}, {'term': {'name': 'S\u00e9bastien Boisvert', 'type': 'person', 'email': 'sebastien.boisvert.3@ulaval.ca', 'maintainer': False}, 'id': 7}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'installation', 'url': 'https://github.com/marbl/mash/blob/main/INSTALL.txt'}, 'id': 0}, {'term': {'type': 'license', 'url': 'https://github.com/marbl/mash/blob/main/LICENSE.txt'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/marbl/mash/blob/main/README.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"unitas/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/unitas/cmd/None\",\n    \"name\": \"unitas\",\n    \"description\": [\n      \"Complete annotation of small RNA datasets from NGS.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://sourceforge.net/projects/unitas/files/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Daniel Gebert\",\n        \"email\": \"dgebert@students.uni-mainz.de\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a5b6d3b8acd1a2f9b5c9\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/unitas/cmd/1.6.1\",\n    \"name\": \"unitas\",\n    \"description\": [\n      \"Unitas is a convenient tool for efficient annotation of small non-coding RNA sequence datasets produced by Next Generation Sequencing.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.smallrnagroup.uni-mainz.de/software.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"CC-BY-NC-2.0\",\n        \"url\": \"https://spdx.org/licenses/CC-BY-NC-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/unitas/files/:\n```\nProject metadata:\n{'description': None, 'sections': []}```"}, {"role": "user", "content": "Content from http://www.smallrnagroup.uni-mainz.de/software.html:\n```\nContent:\n['Here you will find some useful software for analysis and processing of NGS datasets. The software is provided under the terms of the [Creative Commons Attribution Non-Commercial License V2.0](https://creativecommons.org/licenses/by-nc/2.0/) . Detailed information about usage of the software is provided in a separate documentation. To download the software right-click on the logo and choose \"save link/target as\". Execution of Perl scripts requires installation of a Perl interpreter. Free Perl distributions can be found at [http://strawberryperl.com/](http://strawberryperl.com/) or [http://www.activestate.com/activeperl/downloads](http://www.activestate.com/activeperl/downloads) . <software/unitas_1.7.0.zip> <software/unitas_documentation_1.7.0.pdf> unitas is a convenient tool for efficient annotation of small non-coding RNA sequence datasets produced by Next Generation Sequencing. All you need is a computer and a connection to the internet. _unitas_ uses latest reference sequences from publicly available online databases to annotate user input sequences. No installation, no further prerequisites; it runs out-of-the-box on any popular platform (Linux, MacOS, Windows) and can be started with one simple command from the command line (terminal). _unitas_ accepts sequence files in FASTA or FASTQ format, or alternatively map files in SAM or ELAND3 format (standard output of sRNAmapper). * contact us for older versions of _unitas_ . We will constantly provide versions with updated internal URL link lists. Since _unitas_ will always first try to use the URL link lists stored on our server (instead of its internal list), older versions of _unitas_ will still work properly. New executables (incl. a updated documentation) will be provided in case of new functionality or bug fixes. **Information for Mac and Linux users:** Commonly, executable files downloaded from the web cannot be executed without changing file permissions. Therefore, try the following terminal command: chmod 755 unitas or chmod a+rwx unitas Further, _unitas_ will download the SeqMap source code and compile it on your local machine with g++. If g++ is missing on your computer _unitas_ will download a precompiled SeqMap executable. In this case, you will most likely have to change file permissions manually, to make this file executable. <software/proTRAC_2.4.2.pl> <software/proTRAC_documentation_v2.4.2.pdf> proTRAC predicts and analyzes genomic piRNA clusters based on mapped piRNA sequence reads. proTRAC 2.0 and later versions apply a sliding window approach to detect loci that exhibit high sequence read coverage. Subsequently, sequences mapped to these loci are analyzed with respect to typical piRNA and piRNA cluster characteristics to ensure high specificity. proTRAC runs with basic core Perl which is commonly pre-installed on Unix and Mac computers. Windows users can install a free Perl distribution such as [Strawberry Perl](http://strawberryperl.com/) or [ActivePerl](http://www.activestate.com/activeperl/downloads) . Alternatively we provide a [precompiled proTRAC executable (v.2.4.2)](software/Windows/proTRAC_2.4.2.exe) that runs on 64 bit Windows computers. <software/PACKEIS.zip> <software/PACKEIS_documentation_v1.0.pdf> PACKEIS is a software that allows us to assess whether or not a coding sequence represents an extreme solution in terms of backfolding, considering the alternative coding sequences that could have been realized by evolution in order to encode the given peptide sequence based on usage of synonymous codons. <software/TBr2.zip> <software/TBr2_documentation.pdf> NGS TOOLBOX is a collection of simple open source Perl scripts that perform basic analyses and processing steps using next generation sequencing (NGS) datasets. Each tool is designed to ensure convenient and intuitive usage. Installation and usage does not require any bioinformatics skills. All scripts work out-of-the-box. Advanced users may use the command line based Perl scripts to build their own automated sequence analyses/processing pipelines. <software/sRNAmapper.pl> <software/sRNAmapper_documentation.pdf> sRNAmapper is specifically designed to map small RNA sequences to genomes. To this end it uses a specialized mapping algorithm that requires a perfect 5\\' seed match (default: 18 nt) and optionally allows non-template 3\\' nucleotides as well as internal mismatches in the part of the sequence that follows the seed match. Allowing non-template 3\\' ends will ensure the mapping of 3\\' modified (adenylated/uridylated) small RNAs while allowing internal mismatches can enhance sensitivity considering degressive read quality towards 3\\' ends. <software/reallocate.pl> <software/reallocate_documentation.pdf> reallocate post-processes map files in order to reallocate read counts of multiple mapping sequences according to the transcription rate of genomic loci based on uniquely mapping reads. Map files must be in ELAND format and can be created using sRNAmapper which is provided along with the proTRAC software. reallocate will output a modified map file that contains two additional columns that refer to i) total number of genomic hits of a sequence and ii) read counts that are assigned to this locus. proTRAC 2.0.5 and later versions accept this format and utilize this information for cluster prediction. Generally, using reallocate will result in a higher amount of sequence reads that can be assigned to predicted piRNA clusters and may also alter the number of predicted piRNA clusters (more true-positives, less false-positives). Using reallocate is specifically recommended for datasets with large amounts (>= 50%) of transposon related small RNAs such as pre-pachytene mammalian piRNA transcriptomes or drosophila piRNA transcriptomes.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"xxmotif/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/xxmotif/cmd/1.6\",\n    \"name\": \"xxmotif\",\n    \"description\": [\n      \"EXhaustive, weight matriX-based motif discovery in nucleotide sequences.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/soedinglab/xxmotif\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/soedinglab/xxmotif\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/xxmotif/cmd/None\",\n    \"name\": \"xxmotif\",\n    \"description\": [\n      \"Web server that can discover motifs that are enriched in sets of nucleotide sequences provided by the user. It uses a new approach for finding enriched motifs: It directly optimizes the statistical significance of enrichment for PWMs. It can also score conservation and positional clustering of motifs. In several benchmarks on yeast and metazoan sequences, the underlying XXmotif method showed better sensitivity and produced PWMs of higher quality than state-of-the-art tools.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bammmotif.mpibpc.mpg.de/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"xxmotif@genzentrum.lmu.de\",\n        \"maintainer\": false,\n        \"url\": \"https://www.mpibpc.mpg.de/soeding\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82b9610e2f3aa0b3b9653\"\n      },\n      {\n        \"$oid\": \"67c82b9610e2f3aa0b3b9652\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/soedinglab/xxmotif:\n```\nREADME content:\n['#[ ![Codeship Status for AnjaSophie/gimmemotif](https://codeship.com/projects/537b9f00-a0d2-0133-1498-46172c989b73/status?branch=master)](https://codeship.com/projects/128271) #\"!https://codeship.com/projects/537b9f00-a0d2-0133-1498-46172c989b73/status?branch=master!\":https://codeship.com/projects/128271 #{<img src=\"https://codeship.com/projects/537b9f00-a0d2-0133-1498-46172c989b73/status?branch=master\" alt=\"Status?branch=master\" />}[https://codeship.com/projects/128271] ================================================ == How to Use GIMMEmotif from the command line ? == ================================================ The easiest way to use XXmotif is by the XXmotif web server (http://xxmotif.genzentrum.lmu.de) From the command line, XXmotif can be used as follows: Usage: XXmotif OUTDIR SEQFILE [options] OUTDIR: output directory for all results SEQFILE: file name with sequences from positive set Options: --XXmasker mask the input sequences for homology, repeats and low complexity regions --XXmasker-pos mask only the positive set for homology, repeats and low complexity regions --no-graphics run XXmotif without graphical output -h|--help show this output --negSet <FILE> sequence set which has to be used as a reference set --mops use multiple occurrence per sequence model --revcomp search in reverse complement of sequences as well (DEFAULT: NO) --background-model-order <NUMBER> order of background distribution (DEFAULT: 2, 8(--negset) ) --pseudo <NUMBER> percentage of pseudocounts used (DEFAULT: 10) -g|--gaps <NUMBER> maximum number of gaps used for start seeds [0-3] (DEFAULT: 0) --type <TYPE> defines what kind of start seeds are used (DEFAULT: ALL) - possible types: ALL,FIVEMERS,PALINDROME,TANDEM,NOPALINDROME,NOTANDEM --merge-motif-threshold <MODE> defines the similarity threshold for merging motifs (DEFAULT: HIGH) - possible modes: LOW, MEDIUM, HIGH --batch suppress progress bars (reduce output size for batch jobs) --maxPosSetSize <NUMBER> maximum number of sequences from the positive set used [DEFAULT: all] --trackedMotif <SEED> inspect extensions and refinement of a given seed (DEFAULT: not used) ( a possible fivemer SEED for the TATA-Box is: TATAW a possible palindromic gapped sixmer SEED for GAL4 is: CGG...........CCG ) ================== = Options to include conservation information ================== --format FASTA|MFASTA defines what kind of format the input sequences have (DEFAULT: FASTA) --maxMultipleSequences <NUMBER> maximum number of sequences used in an alignment [DEFAULT: all] ================== == Options to include localization information ================== --localization use localization information to calculate combined P-values (sequences should have all the same length) --downstream <NUMBER> number of residues in positive set downstream of anchor point (DEFAULT: 0) ================== == Jump start XXmotif with self-defined motif (skip elongation phase) ================== -m|--startMotif <MOTIF> Start motif (IUPAC characters) E.g.: -m \"TATAWAWR\" to jump start XXmotif with a TATA-Box -p|--profileFile <FILE> profile file every column corresponds to one motif position. Every line to A,C,G,T, respectively E.g To jump start with a TATA-Box PWM the profile File should contain: 0.0 1.0 0.0 1.0 0.5 1.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 1.0 0.0 1.0 0.0 0.5 0.0 0.5 0.0 --startRegion <NUMBER> expected start position for motif occurrences relative to anchor point (--localization) --endRegion <NUMBER> expected end position for motif occurrences relative to anchor point (--localization) ==================================================================================================================================== ==================================================================================================================================== =================================== == Which Output files do exist ? == =================================== 1.) *_MotifFile.txt: Summary of the motif results It contains a representation of the found motifs without degenerate nucleotides (Line 1) and including degerenate nucleotides (Line 4). Line 3 contains the motif significance and information about the motif occurrence. If zoops or oops model is used, the percentage of sequences with motifs is given, if the mops model is used only the number of binding sites is given. If the option \"--localization\" is used Line 5 gives information about the enriched region (startRegion, endRegion) and the position with highest motif occurrence (mode) with respect to the anchor point. 1: ----Motif Number 1: ---- 2: => C/G/T A/T A T C A/C/G G C/G C/G/T C/G/T C/T C A T C C G/T 3: E-Value: 2.48e-27, occ: 20.00 % (13 of 65 sequences) 4: IUPAC: B W A T C V G S B B Y C A T C C K 5: mode: 57, startRegion: 42 endRegion: 76 6: 7: ----Motif Number 2: ---- 8: ... 2.) *_Pvals.txt : Summary of the Pvalues for each motif site After header information (Line 1 - 3) all sites for all motifs are given including the upstream and downstream sequences. Star positions are with respect to the sequence start. More information about the sequence with the specified Sequence Number (seq nb) can be found in file *_sequence.txt 1: ######################################################################################################################## 2: ### upstrem site site downstream site seq nb start pos strand site Pval ### 3: ######################################################################################################################## 4: 5: Motif 1: B W A T C V G S B B Y C A T C C K E-Value: 2.48e-27 6: GGGTGAAGGGAGGTTGAGCT CAATCAGGCCCCATCCT GCTTCACAGCCTA 2 372 + 3.2566e-05 7: GTTTCTGTGGCGTTCGAGTT CCATCGGCTCCCATCCG GGCTATCCTGCCGCCTTAGC 5 363 + 4.2578e-05 8: CCCTGTCTTTATTTCCTTAC CAATCGGCTGCCATCCG AGGAGCTGAGGAAGCCTAG 8 366 + 6.3485e-07 9: ATTCGGCGGGCAAGCGGGTG TAATCAGCAGCCATCCG TTCTTGGGCATGGTGGCTTC 12 364 + 1.3936e-05 10:... 3.) *_sequence.txt After header information (Line 1 - 3) Sequence Numbers (seq nb), sequencence length and the header information given in the input files are given 1: ##################################################################### 2: ### seq nb seq length seq header ### 3: ##################################################################### 4: 5: 1 401 EP73002 (+) Hs RPL7 range -300 to 100. 6: 2 401 EP73003 (+) Hs RPS8 range -300 to 100. 7: 3 401 EP73015 (+) Hs RPS11 range -300 to 100. 4.) *.pwm All position weight matrices of the detected motifs are given. Line 1 contains an IUPAC representation of the motif with its E-value. Lines 2-5 contain the PWM. Every column corresponds to one position. Every line to A, C, G, T, respectively 1: Motif 1: B W A T C V G S B B Y C A T C C K E-Value: 2.48e-27 2: 0.01791 0.57735 0.92700 0.01791 0.01791 0.15777 0.01791 0.01791 0.08784 0.01791 0.08784 0.01791 0.92700 0.01791 0.01791 0.01791 0.01791 3: 0.44610 0.09645 0.02652 0.02652 0.93561 0.58596 0.02652 0.72582 0.37617 0.16638 0.72582 0.93561 0.02652 0.02652 0.93561 0.93561 0.02652 4: 0.37579 0.02614 0.02614 0.02614 0.02614 0.23593 0.93524 0.23593 0.16600 0.51565 0.02614 0.02614 0.02614 0.02614 0.02614 0.02614 0.65552 5: 0.16020 0.30006 0.02034 0.92944 0.02034 0.02034 0.02034 0.02034 0.36999 0.30006 0.16020 0.02034 0.02034 0.92944 0.02034 0.02034 0.30006 5.) *_hf.fasta If the option \"--XXmasker\") is used, input sequences are masked for homology, repeats, and low-complexity regions. The output of XXmasker is stored into this file and subsequently used by XXmotif as input. 6.) *.png Graphical output of the motif search. It is created if the option (--no-graphics) is not selected and is designed especially for localized motifs. Every plot contains the occurrence of the motif ( binding sites / number of sequences), which can be greater than 100% in case of the \"multiple occurrence per sequence model\" (--mops). If the option localization is used (--localization) the position with most motif occurrences is given (max). The upper part of the plot contains the PWM logo, and the distribution of binding sites selected by XXmotif. If localization is used, a red bar indicates the region of highest motif enrichment. If no red bar is present, no region contains significantly enriched motifs. The lower part of the plot shows the distribution of binding sites with a minimum log-odds score on the forward strand (red) and on the backward strand (blue). All binding sites are counted that have log-odds score > 50% of the highest possible log-odds score of the PWM. This plot is useful to reveal whether the motif is strand specific. Moreover, in case of localized motifs, it is useful to analyze whether there are binding sites outside the enriched region that are not selected by XXmotif. The axis at the bottom shows the position of the binding site within the input sequence. If the option \"--localization\" is selected it is assumed that the \"--downstream\" option is used to define the number of nucleotides downstream of the anchor point. If \"--downstream\" is not used, it is set to 0. If the motif search is performed on both strands (--revcomp) the reverse strand is simply added after the forward strand. Hence, the sequences seem to be twice as long. ===================== == Version Track ===================== Version 1.6: - fixed bug reported by Zhihao Ling concerning parameter --max-match-positions Version 1.5: - improved speed to read in large input sets (> 10.000 sequences) Version 1.4: - report motifs with e-value < 1e2 (before: < 1e3) Version 1.3: - removed a bug leading to a segmentation fault when using sequences longer than 16.000 nucleotides - removed a bug leading to wrongly counted initial seed kmers when using the option --gaps > 0 and for the tandemic seed kmers - added the option --max-motif-length to allow motifs to be found with length > 17 (larger values than 17 will lead to very long runtimes, maximum possible length is 26) - added the option --no-pwm-length-optimization to remove the time consuming pwm length optimiziation step in the iterative pwm phase Version 1.2: - Version used in the publication']```"}, {"role": "user", "content": "Content from https://github.com/soedinglab/xxmotif:\n```\nRepository metadata:\n{'name': 'xxmotif', 'label': ['xxmotif'], 'description': [{'term': 'XXmotif: eXhaustive, weight matriX-based motif discovery in nucleotide sequences', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/soedinglab/xxmotif', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/motif-discovery', 'term': 'motif-discovery', 'vocabulary': ''}, 'id': 1}], 'operations': [], 'authors': [{'term': {'name': 'Christian Roth', 'type': 'person', 'email': 'christian.roth@mpibpc.mpg.de', 'maintainer': False}, 'id': 0}, {'term': {'name': 'AnjaSophie', 'type': 'person', 'email': 'kiesel@genzentrum.lmu.de', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Matthias Siebert', 'type': 'person', 'email': 'siebert@genzentrum.lmu.de', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/soedinglab/xxmotif/blob/main/CMakeLists.txt'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://bammmotif.mpibpc.mpg.de/:\n```\nContent:\n['[Motif database](/database/) **Search and browse our databases** with hundreds of higher-order models learnt on ChIP-seq peaks from GTRD.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"oncocnv/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/oncocnv/cmd/7.0\",\n    \"name\": \"oncocnv\",\n    \"description\": [\n      \"A package to detect copy number changes in Deep Sequencing data.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/BoevaLab/ONCOCNV\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/oncocnv/cmd/None\",\n    \"name\": \"oncocnv\",\n    \"description\": [\n      \"Package to detect copy number changes in Deep Sequencing data developed by OncoDNA.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://oncocnv.curie.fr/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"infos@oncodna.com\",\n        \"maintainer\": false,\n        \"url\": \"http://www.oncodna.com/\",\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Valentina Boeva\",\n        \"email\": \"valentina.boeva@curie.fr\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8299d10e2f3aa0b3b86da\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/BoevaLab/ONCOCNV:\n```\nREADME content:\n['# ONCOCNV **ONCOCNV - a package to detect copy number changes in Deep Sequencing data** REQUIREMENTS 0. Perl and R installed and added to the PATH \\\\ E.g., export PATH=$PATH:YOURPATH/R/bin 1. SAMtools (http://samtools.sourceforge.net/) installed and added to the PATH \\\\ To add to PATH, type in the command line or add to \"ONCOCNV.sh\": \\\\ export PATH=$PATH:YOURPATH/samtools/bin or \\\\ alias samtools=YOURPATH/samtools/bin/samtools 2. BEDTools (http://bedtools.readthedocs.org/en/latest/) installed and added to the PATH \\\\ To add to PATH, type in the command line or add to \"ONCOCNV.sh\": \\\\ export PATH=$PATH:YOURPATH/BEDTools/bin/ or \\\\ alias bedtools=YOURPATH/BEDTools/bin/bedtools 3. The following R libraries should be installed: MASS, mclust, PSCBS, DNAcopy, R.cache, scales, cwhmisc, fastICA, cghseg, digest 4. The fasta sequence (one file, unzipped; e.g. \"hg19.fa\") of the targeted genome should be downloaded from http://hgdownload.soe.ucsc.edu/downloads.html 5. You need to have your data aligned (.bam files) 6. You need to have at least **three** control files to construct a **reliable** baseline. However, ONCOCNV will run with only 2 controls starting from version 5.4 and with JUST **one** control starting from version 5.7. Yet, we recommend to have at least 3 control for good performance of the algorithm. INSTALLATION 0. Download ONCOCNV.zip (or ONCOCNV.vX.X.zip) 1. Unzip files into detectory \"scripts\" 2. Check requirements (R + the necessary R packages must be installed) To install the necessary P packages (when R is installed), type in the command line: R install.packages(\"MASS\") install.packages(\"mclust\") install.packages(\"R.cache\") install.packages(\"scales\") install.packages(\"cwhmisc\") install.packages(\"fastICA\") install.packages(\"cghseg\") install.packages(\"digest\") source(\"http://bioconductor.org/biocLite.R\") biocLite(\"DNAcopy\") install.packages(\"PSCBS\") quit() RUN ONCOCNV 0. Open \"ONCOCNV.sh\" with a text editor (gedit, textpad, etc.) 1. Set correct paths and filenames in the top part of the \"ONCOCNV.sh\" 2. Check properties of \"ONCOCNV.sh\" \\\\ chmod +rwx PathToONCOCNV/scripts/ONCOCNV.sh 3. Check formats: o reads should be given in .BAM format \\\\ o amplicon coordinates should be given in .bed format (with or without the headline) and have amplicon ID in column 4 and gene symbol in column 6, e.g.: \\\\ chr1 2488068 2488201 AMPL223847 0 TNFRSF14 \\\\ It is mandatory to provide gene names in the 6th column. --------------------------------------------------------------------------------------------------------------------------- ****************VERY IMPORTANT**************** Please make sure that: - There is no duplicates in the coordinates - Coordinates are sorted - Gene names are gene names in the sense that corresponding amplicons fall in the same genomic locus and not on different chromosomes - Gene names cannot be the same as amplicon names or IDs because ONCOCNV assumes to have several amplicons per gene --------------------------------------------------------------------------------------------------------------------------- 4. Run \"ONCOCNV.sh\" from the command line: cd PathToONCOCNV/scripts ./ONCOCNV.sh or . PathToONCOCNV/scripts/ONCOCNV.sh HOW TO READ OUTPUT FILES There are three output files per sample: 1. *.profile.png - Visual representation of normalized and annotated copy number profile \\\\ Each dot corresponds to an amplicon; the X-axis is not up to scale. \\\\ Color code: \\\\ o GREEN one-point-outlier \\\\ o DARK GREY SURROUNDINGS frequent one-point-outlier \\\\ o BROWN >1 level gain \\\\ o BROWN SURROUNDINGS 1-level gain \\\\ o BLUE >1 level loss \\\\ o BLUE SURROUNDINGS 1-level loss 2. *.summary.txt - predictions per gene gene gene name \\\\ chr chromosome name \\\\ start first amplicon start \\\\ end last amplicon start \\\\ copy.number predicted copy number (no normal contamination nor subclones is taken into accout) \\\\ p.value p-value for the copy number status of the genomic region encompassing the gene \\\\ q.values q-value (\"fdr\"-corrected p-value) for the copy number status of the genomic region encompassing the gene \\\\ comments p-value for the hypothesis that the copy number of the gene does not match the copy number of the encompassing segment \\\\ (in the case of a break within a gene - it is the p-value for the break) 3. *.profile.txt - predictions per amplicon (detailed information) chr chromosome name \\\\ start first amplicon start \\\\ end last amplicon start \\\\ gene gene name \\\\ ID amplicon ID \\\\ ratio logarithm of the normalized read count (zero values correspond to the neutral copy number) \\\\ predLargeSeg copy number predicted by segmentation of normalized read counts \\\\ predLargeCorrected *final prediction for the copy number* \\\\ pvalRatioCorrected p-value of the t-test to test the difference between the normalized read counts and the value expected from the segmentation or from the gene-based copy number assessment \\\\ perGeneEvaluation copy number predicted per gene (unaware of the segementation) \\\\ pvalRatioGene gene-based p-value of the t-test for the difference of the mean of the normalized read counts from zero \\\\ predPoint predicted one-point-outlier \\\\ predPointSusp predicted (frequent) one-point-outlier \\\\ comments additional information: \\\\ SegRatio mean value of the logarithms of the normalized read counts per segment \\\\ AbsMeanSigma normalized difference of the mean value (~z-score/sqrt(#amplicons in the segment)) \\\\ pvalue p-value for AbsMeanSigma \\\\ pvalueTTest p-value of the t-test (per segment) \\\\']```"}, {"role": "user", "content": "Content from https://github.com/BoevaLab/ONCOCNV:\n```\nRepository metadata:\n{'name': 'ONCOCNV', 'label': ['ONCOCNV'], 'description': [{'term': 'ONCOCNV - a package to detect copy number changes in Targeted Deep Sequencing and Exome-seq data', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v7.0', 'v6.9', 'v6.8', 'v6.7', 'v6.6'], 'license': [], 'repository': [{'term': 'https://github.com/BoevaLab/ONCOCNV', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Valentina Boeva', 'type': 'person', 'email': 'valeu@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'valeu', 'type': 'person', 'email': 'valentina.boeva@gmail.com', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/BoevaLab/ONCOCNV/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://oncocnv.curie.fr/:\n```\nContent:\n['## Introduction **ONCOCNV** is a package to detect copy number changes in Deep Sequencing data developed by [OncoDNA](http://www.oncodna.com/) with the collaboration with the [Bioinformatics Laboratory of Institut Curie](http://bioinfo-out.curie.fr) (Paris). It is now supported by the group of [Valentina Boeva](http://boevalab.com) at Inserm U1016. **ONCOCNV** automatically computes, normalizes, segments copy number profiles, then calls copy number alterations. The user can provide **any positive number of control samples** in order to construct the baseline. However, we recommend to use at least three control samples. The more the better. **ONCOCNV can be applied to exome-seq data.** You just need to provide probe coordinates instead of amplicon coordinates, and you will get beautiful copy number profiles for your data. **Input for CNV detection** : aligned single-end or paired-end data in the [BAM](http://samtools.sourceforge.net/SAM1.pdf) format. **Output** : Annotation of genes with copy number changes + visualization of the profile (.png). **Citation:** Boeva,V. et al. (2014) Multi-factor data normalization enables the detection of copy number aberrations in amplicon sequencing data. _Bioinformatics_ , 30(24):3443-3450. [Link](http://bioinformatics.oxfordjournals.org/content/30/24/3443.long) **SeqAnswers forum:** [http://seqanswers.com](http://seqanswers.com/forums/showthread.php?p=159973#post159973) ## Downloads Read about the requirements in the [README](https://github.com/BoevaLab/ONCOCNV/) file. Download the the latest version of [ONCOCNV](https://github.com/BoevaLab/ONCOCNV/releases/) . ## Data access Raw or processed intput files: * Spreadsheet with the**raw**read count data for the**control**samples (dataset A; generated byONCOCNV_getCounts.pl):[Control.stats.txt](https://github.com/BoevaLab/ONCOCNV/blob/master/testdata/Control.stats.txt) * Spreadsheet with the**normalized**read count data for the**control**samples with additional information: amplicon length, GC-content, PC1, PC2, PC3, standard deviation (dataset A; generated byprocessControl.v5.3.R):[Control.stats.Processed.txt](https://github.com/BoevaLab/ONCOCNV/blob/master/testdata/Control.stats.Processed.txt) * Spreadsheet with the**raw**read count data for the**tumor**samples (dataset A; generated byONCOCNV_getCounts.pl):[Test.stats.samplesA1_A3.txt](https://github.com/BoevaLab/ONCOCNV/blob/master/testdata/Test.stats.samplesA1_A3.txt)and[Test.stats.samplesA4_A8.txt](https://github.com/BoevaLab/ONCOCNV/blob/master/testdata/Test.stats.samplesA4_A8.txt) Result files: ## Contacts The following members of the ONCOCNV working group are pleased to answer any question or address any concerns you may have with the ONCOCNV software: ## Visualization Example of output .png file: Additionally, you can visualize the output per chromosome. In this case, gene names will appear on the graph. To run the script: cat perChrVisualization.R | R --slave --args myTestSample.profile.txt 17 or cat perChrVisualization.R | R --slave --args myTestSample.profile.txt chr17 Blue vertical lines signify predicted breakpoints. Orange vertical lines correspond to breakpoints resulting from the segmentation (they were corrected by a t-test later). You may ignore them. Example of output .png file:']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"ccat/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/ccat/cmd/none\",\n    \"name\": \"ccat\",\n    \"description\": [\n      \"A software package for the analysis of ChIP-seq data with negative control.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.comp.nus.edu.sg/~bioinfo/CCAT3.0/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c79d35d3b8acd1a2f9b3d1\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/ccat/cmd/3.0\",\n    \"name\": \"ccat\",\n    \"description\": [\n      \"CCAT is a software package for the analysis of ChIP-seq data with negative control.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://cmb.gis.a-star.edu.sg/ChIPSeq/paperCCAT.htm\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"unknown\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c79d35d3b8acd1a2f9b3d1\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"diseasemeth/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/diseasemeth/db/3.0\",\n    \"name\": \"diseasemeth\",\n    \"description\": [\n      \"The Human Disease Methylation Database Version 3.0 is a web-based resource that focuses on the abnormal methylome of human diseases.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://218.8.241.243:8080/DiseaseMeth3.0/index.html\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Yan Zhang\",\n        \"email\": \"zhangtyo@hit.edu.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Jie Lv\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Yue Gu\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c827b410e2f3aa0b3b750f\"\n      },\n      {\n        \"$oid\": \"67c834f110e2f3aa0b3bc03a\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/diseasemeth/db/None\",\n    \"name\": \"diseasemeth\",\n    \"description\": [\n      \"A web based resource focused on the aberrant methylomes of human diseases.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://bioinfo.hrbmu.edu.cn/diseasemeth\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Lv Jie\",\n        \"email\": \"lvjielvjie@yahoo.com.cn\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c827b410e2f3aa0b3b750f\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"tiaas/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/tiaas/undefined/2.1.0\",\n    \"name\": \"tiaas\",\n    \"description\": [\n      \"Originally developed by Galaxy Europe and the Gallantries project, together with the Galaxy community we have created \\\"Training Infrastructure-as-a-Service\\\" (TIaaS), aimed at providing user-friendly training infrastructure to the global training community. TIaaS provides dedicated training resources for Galaxy-based courses and events. Event organisers register their course, after which trainees are transparently placed in a private queue on the compute infrastructure, which ensures jobs complete quickly, even when the main queue is experiencing high wait times. A built-in dashboard allows instructors to monitor student progress.\\n\\nTraining Infrastructure as a Service (TIaaS) has been in development since 21 June 2018, and three days later became a production service at Galaxy Europe on 24 June. As of June 7th, 2023 is has supported 504 training events with over 24000 learners have used this infrastructure for Galaxy training.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/galaxyproject/tiaas2\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Helena Rasche\",\n        \"email\": \"helena.rasche@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"Bundesministerium f\u00fcr Bildung und Forschung\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"National Human Genome Research Institute\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Cameron Hyde\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"John Davis\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Simon Gladman\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Nate Coraor\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Anthony Bretaudeau\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Gianmauro Cuccuru\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Wendi Bacon\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Beatriz Garc\u00eda-Jim\u00e9nez\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Jennifer Hillman-Jackson\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Saskia Hiltemann\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Miaomiao Zhou\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Bj\u00f6rn Gr\u00fcning\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Andrew Stubbs\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Organization\",\n        \"name\": \"Erasmus+ Programme\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c837c110e2f3aa0b3bcce3\"\n      },\n      {\n        \"$oid\": \"67c8323510e2f3aa0b3baecd\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/tiaas/undefined/None\",\n    \"name\": \"tiaas\",\n    \"description\": [\n      \"Training Infrastructure as a Service.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxyproject.eu/tiaas\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Helena Rasche\",\n        \"email\": \"helena.rasche@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8323510e2f3aa0b3baecd\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://galaxyproject.eu/tiaas:\n```\nContent:\n[\"#### TIaaS Presentation at GCC We presented [TIaaS at GCC2019](https://gcc2019.sched.com/event/Rsld/training-infrastructure-as-a-service) , you can see our slides on the event page. We've also produced some [training material](https://training.galaxyproject.org/training-material/topics/instructors/tutorials/setup-tiaas-for-training/tutorial.html) on the process of evaluating if the service is right for you, and the user experience for your trainees. # Training Infrastructure as a Service The [European Galaxy Server](https://usegalaxy.eu) is proud to provide **Training Infrastructure as a Service (TIaaS)** for the Galaxy training community. You provide the training, we provide the **infrastructure at no cost** . ## Why TIaaS? ## How TIaaS Works We have several \u201cpools\u201d of virtual machines (VMs) attached to UseGalaxy.eu that run user jobs. For trainings that request TIaaS, we attach a new pool of VMs that is specially labelled for that training. When users join a training using a **special URL** we provide you, they are assigned to a **special training group** . Their jobs will then **preferentially run on a training machine** , and, in the event there is no more capacity, they will run on the main queue. If a spot on a training VM opens up first, they will run there rather than continuing to wait in the main queue. The jobs run by the rest of users on our server are instructed to avoid the training pools. Some more general information about the TIaaS service: ### Before the training Before users can join a specific training, they need to be **logged into the[European Galaxy server](https://usegalaxy.eu/)** . To **import data** : * If you are using the[GTN material](training.galaxyproject.org/)for your workshop, then all the[training data is already mirrored into Galaxy](https://usegalaxy.eu/libraries/folders/Fa21272e5bd712216). * If you are using your own data, you can upload it to your account into a history. Later, you can[make it accessible](https://usegalaxy.eu/histories/sharing)to your trainees (do not forget to also share the datasets by clickingAlso make all objects within the History accessible). Your trainees will then be able to import your history and start working with your data. ### After the training Once the training is over, the data will stay available for further use. However, we encourage you to **clean up all the histories** whenever this data will not be used anymore. To keep running this free service for your trainees, **we need your feedback and support** . It would be great if you could: * Write a (short) blog post that we will**publish in our website**. You can find examples[here](/news?tag=TIaaS). * Fill out the[GTN Survey](https://galaxyproject.org/news/2020-01-training-feedback/). * Give feedback about a tutorial on[GitHub](https://github.com/galaxyproject/training-material/issues/1452). * Tweet about your training tagging[@galaxyproject](https://twitter.com/galaxyproject). ## Service Level Agreement We _cannot_ promise any degree of uptime. We will do our best to have this service online and functioning during the entire time period, but there are cases where the service may experience interruptions that are outside of our control. We will keep you informed of any outages that may affect your training. ## Eligibility We have significant capacity and have dedicated some of this to providing training using our service. Anyone providing training on using Galaxy is eligible to request the use of this service. ## Application Process * Fill out the request form:Request Training InfrastructurePlease try to submit your application**at least two weeks in advance of the training**. * With the information that you input in the form, we will:estimate how many VMs are needed for your training to run smoothly and will ask for your feedback,setup the private queue and give you a URL your users will access to join the training,give you a URL to the dashboard for you to see the queue status. * Done! You are ready to run your training!\"]```"}, {"role": "user", "content": "Content from https://github.com/galaxyproject/tiaas2:\n```\nREADME content:\n['![Tiaas logo, galaxy three bars logo with human icons instead of bars, one bar is yellow and faster looking](./images/tiaas-logo.png) # Training Infrastructure as a Service (TIaaS) TIaaS provides a service which Galaxy administrators can deploy, enabling them to easily providing training infrastructure, as a service, to their user community. We built TIaaS to streamline the process of hosting courses and training events on the Galaxy platform. It enables: - Administrators to control who is using their infrastructure for courses - Teachers to easily request resources - Teachers to monitor their classes and run efficient trainings - Administrators to dedicate resources to trainings, enabling them to run smoothly and efficiently. Once a training request is approved, teachers receive and share with students a URL like https://usegalaxy.eu/join-training/test (this is live, you can test it out!) Upon visiting, students are automatically added to a group named `training-test` (and a role automatically created.) It works on the basis of running underneath the path prefix of Galaxy and so having access to the Galaxy session cookie. This is decoded into a user id + the part after `/join-training/` decoded into a group name, and this change is made. ## Features ### Status Page For teachers giving trainings, we now offer a \"status\" page where they can see the training queue of their class, both jobs and workflows. ![Status page showing a series of jobs in green and red (for failing) as well as an overview of how many of which tools have been executed and their current job state](images/queue.png) ### Register Point your users to this service to register their training ![a registration form](images/register.png) ### Admin Interface Admins can manage TIaaS courses in their view. ![Administrator View](images/admin.png) ## Documentation - [Setup TIaaS for a Training Event](https://training.galaxyproject.org/training-material/topics/teaching/tutorials/setup-tiaas-for-training/tutorial.html) - [Install TIaaS with the ansible role (for admins)](https://training.galaxyproject.org/training-material/topics/admin/tutorials/tiaas/tutorial.html) ## License AGPL-3.0']```"}, {"role": "user", "content": "Content from https://github.com/galaxyproject/tiaas2:\n```\nRepository metadata:\n{'name': 'tiaas2', 'label': ['tiaas2', 'Training Infrastructure as a Service (TIaaS)'], 'description': [{'term': 'Reusable Training Infrastructure as a Service', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['2.1.0', '2.1.0'], 'license': [{'term': {'name': 'GNU Affero General Public License v3.0', 'url': 'http://choosealicense.com/licenses/agpl-3.0/'}, 'id': 0}, {'term': {'name': 'AGPL-3.0', 'url': ''}, 'id': 1}], 'repository': [{'term': 'https://github.com/galaxyproject/tiaas2', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Helena Rasche', 'email': '', 'maintainer': False, 'type': 'person'}, 'id': 0}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'code_of_conduct', 'url': 'https://github.com/galaxyproject/tiaas2/blob/main/CODE_OF_CONDUCT.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/galaxyproject/tiaas2/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/galaxyproject/tiaas2/blob/main/SECURITY.md'}, 'id': 2}, {'term': {'type': 'requirements', 'url': 'https://github.com/galaxyproject/tiaas2/blob/main/requirements.txt'}, 'id': 3}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [{'term': {'title': ' Training Infrastructure as a Service', 'year': 2023, 'doi': '10.1093/gigascience/giad048', 'url': ''}, 'id': 0}], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': '', 'citation': '# This CITATION.cff file was generated with cffinit.\\n# Visit https://bit.ly/cffinit to generate yours today!\\n\\ncff-version: 1.2.0\\ntitle: Training Infrastructure as a Service (TIaaS)\\nmessage: >-\\n  If you use this software, please cite it using the\\n  metadata from this file.\\ntype: software\\nauthors:\\n  - given-names: Helena\\n    family-names: Rasche\\n    email: hexylena@galaxians.org\\n    affiliation: Erasmus Medical Center\\n    orcid: \\'https://orcid.org/0000-0001-9760-8992\\'\\nidentifiers:\\n  - type: doi\\n    value: 10.1093/gigascience/giad048\\n    description: Primary Publication\\nrepository-code: \\'https://github.com/galaxyproject/tiaas2\\'\\nlicense: AGPL-3.0\\ncommit: 214c661d382f80f922d3ae997668e0c58a0da6e0\\nversion: 2.1.0\\ndate-released: \\'2023-01-27\\'\\npreferred-citation:\\n  doi: \"10.1093/gigascience/giad048\"\\n  journal: \"GigaScience\"\\n  month: 7\\n  title: \" Training Infrastructure as a Service\"\\n  year: 2023\\n  type: article\\n  authors:\\n    - family-names: Rasche\\n      given-names: Helena\\n      orcid: \"https://orcid.org/0000-0001-9760-8992\"\\n    - family-names: Hyde\\n      given-names: Cameron\\n      orcid: \"https://orcid.org/0000-0002-5913-9766\"\\n    - family-names: Davis\\n      given-names: John\\n      orcid: \"https://orcid.org/0000-0002-1363-1245\"\\n    - family-names: Gladman\\n      given-names: Simon\\n      orcid: \"https://orcid.org/0000-0002-6100-4385\"\\n    - family-names: Coraor\\n      given-names: Nate\\n      orcid: \"https://orcid.org/0000-0001-8083-2963\"\\n    - family-names: Bretaudeau\\n      given-names: Anthony\\n      orcid: \"https://orcid.org/0000-0003-0914-2470\"\\n    - family-names: Cuccuru\\n      given-names: Gianmauro\\n      orcid: \"https://orcid.org/0000-0002-5335-545X\"\\n    - family-names: Bacon\\n      given-names: Wendi\\n      orcid: \"https://orcid.org/0000-0002-8170-8806\"\\n    - family-names: Serrano-Solano\\n      given-names: Beatriz\\n      orcid: \"https://orcid.org/0000-0002-5862-6132\"\\n    - family-names: Hillman-Jackson\\n      given-names: Jennifer\\n      orcid: \"https://orcid.org/0000-0003-4012-8116\"\\n    - family-names: Hiltemann\\n      given-names: Saskia\\n      orcid: \"https://orcid.org/0000-0003-3803-468X\"\\n    - family-names: Zhou\\n      given-names: Miaomiao\\n      orcid: \"https://orcid.org/0000-0003-4426-1758\"\\n    - family-names: Gr\u00fcning\\n      given-names: Bj\u00f6rn\\n      orcid: \"https://orcid.org/0000-0002-3079-6586\"\\n    - family-names: Stubbs\\n      given-names: Andrew\\n      orcid: \"https://orcid.org/0000-0001-9817-9982\"\\n'}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"kaptive/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/kaptive/cmd/2.0.6\",\n    \"name\": \"kaptive\",\n    \"description\": [\n      \"Reports information about surface polysaccharide loci for Klebsiella and Acinetobacter baumannii genome assemblies.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/katholt/Kaptive\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/katholt/Kaptive\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/kaptive/cmd/None\",\n    \"name\": \"kaptive\",\n    \"description\": [\n      \"Identification of Acinetobacter baumannii loci for capsular polysaccharide (KL) and lipooligosaccharide outer core (OCL) synthesis in genome assemblies using curated reference databases compatible with Kaptive.\\n\\nKaptive reports information about surface polysaccharide loci for Klebsiella and Acinetobacter baumannii genome assemblies. You can also run a graphical version of Kaptive via this web interface (source code).\\n\\nGiven a novel genome and a database of known loci (K, O or OC), Kaptive will help a user to decide whether their sample has a known or novel locus. It carries out the following for each input assembly:.\\n\\nKaptive is a tool for bacterial surface polysaccharide locus typing and variant evaluation. It takes one or more pre-assembled genomes and for each finds the best matching locus from a reference database. References for Klebsiella pneumoniae species complex and Acinetobacter baumannii are available in Kaptive's web interface.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://kaptive.holtlab.net/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82cc010e2f3aa0b3b988c\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://kaptive.holtlab.net/:\n```\nContent:\n[\"## Citations If you use **Kaptive's web interface** please cite the paper below along with the reference for the relevant locus database(s). Wick RR, Heinz E, Holt KE and Wyres KL 2018. Kaptive Web: user-friendly capsule and lipopolysaccharide serotype prediction for _Klebsiella_ genomes. _Journal of Clinical Microbiology_ : [56(6). e00197-18](https://jcm.asm.org/content/56/6/e00197-18.long) **K. pneumoniaespecies complex capsule (K) locus database** Lam MMC, Wick RR, Judd LM, Holt KE and Wyres KL 2022. Kaptive 2.0: updated capsule and lipopolysaccharide locus typing for the Klebsiella pneumoniae species complex. _Microbial Genomics_ doi: [https://doi.org/10.1099/mgen.0.000800](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000800) **Current database version (as at 14 November 2022): v2.0.4** **K. pneumoniaespecies complex LPS (O) locus database** Lam MMC, Wick RR, Judd LM, Holt KE and Wyres KL 2022. Kaptive 2.0: updated capsule and lipopolysaccharide locus typing for the Klebsiella pneumoniae species complex. _Microbial Genomics_ doi: [https://doi.org/10.1099/mgen.0.000800](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000800) **Current database version (as at 14 November 2022): v2.0.4** **A. baumanniicapsule (K) locus database** Cahill SM, Hall RM and Kenyon JJ 2022. An update to the database for _Acinetobacter baumannii_ capsular polysaccharide locus typing extends the extensive and diverse repertoire of genes found at and outside the K locus. _Microbial Genomics_ doi: [https://doi.org/10.1099/mgen.0.000878](https://doi.org/10.1099/mgen.0.000878) **Current database version (as at 14 November 2022): v2.0.4** **A. baumanniiLPS (OC) locus database** Wyres KL, Cahill SM, Holt KE, Hall RM and Kenyon JJ 2020. Identification of _Acinetobacter baumannii_ loci for capsular polysaccharide (KL) and lipooligosaccharide outer core (OCL) synthesis in genome assemblies using curated reference databases compatible with Kaptive. _Microbial Genomics_ doi: [10.1099/mgen.0.000339](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000339) **Current database version (as at 14 November 2022): v2.0.4** A full list of citations for the individual _A. baumannii_ K- and OC- locus reference sequences can be found [here](https://github.com/katholt/Kaptive/tree/master/extras) . **V. parahaemolyticuscapsule (K) and LPS (O) locus databases** van der Graaf\u2013van Bloois L, Chen H, Wagenaar JA, Zomer AL 2022. Development of Kaptive databases for _Vibrio parahaemolyticus_ O- and K-antigen serotyping. _bioRxiv_ doi: [https://doi.org/10.1101/2021.07.06.451262](https://www.biorxiv.org/content/10.1101/2021.07.06.451262v2) **Current database version (as at 14 November 2022): v0.0.1**\"]```"}, {"role": "user", "content": "Content from https://github.com/katholt/Kaptive:\n```\nREADME content:\n['<p align=\"center\"><img src=\"extras/kaptive_logo.png\" alt=\"Kaptive\" width=\"400\"></p> Kaptive reports information about surface polysaccharide loci for _Klebsiella pneumoniae_ species complex and _Acinetobacter baumannii_ genome assemblies. You can also run a graphical version of Kaptive via [this web interface](http://kaptive-web.erc.monash.edu/) ([source code](https://github.com/kelwyres/Kaptive-Web)). **For information on how to install, run, interpret and cite Kaptive please visit the [Docs](https://kaptive.readthedocs.io/en/latest/).** A step-by-step tutorial for the most recent version is available [here](https://docs.google.com/document/d/1EXZanC6uCbhAniVyJn91HOVD8JF6DLRZxmFPGs_nlME/edit?usp=sharing) (Kaptive v3). The tutorial for the previous version is [here](https://bit.ly/kaptive-workshop). [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11257262.svg)](https://doi.org/10.5281/zenodo.11257262)']```"}, {"role": "user", "content": "Content from https://github.com/katholt/Kaptive:\n```\nRepository metadata:\n{'name': 'Kaptive', 'label': ['Kaptive'], 'description': [], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v3.0.0b6', 'v3.0.0b5', 'v3.0.0b4', 'v3.0.0b3', 'v3.0.0b2', 'v3.0.0b1', 'v3.0.0b0', 'v2.0.9', 'v2.0.8', 'v2.0.7', 'v2.0.6', 'v2.0.5', 'v2.0.4', 'v2.0.3', 'v2.0.2', 'v2.0.1', 'v2.0.0', 'v0.7.3', 'v0.7.2', '0.7.1', '0.7.0', '0.6.1', '0.6.0', '0.5.1', '0.5', '0.4', '0.3', '0.2', '0.1'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/klebgenomics/Kaptive', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Tom Stanton', 'type': 'person', 'email': 'tomdstanton@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'kelwyres', 'type': 'person', 'email': 'kelwyres@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'kelwyres', 'type': 'person', 'email': 'kelly.wyres@googlemail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Ryan Wick', 'type': 'person', 'email': 'rrwick@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Kat Holt', 'type': 'person', 'email': 'drkatholt@gmail.com', 'maintainer': False}, 'id': 4}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/klebgenomics/Kaptive/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'requirements', 'url': 'https://github.com/klebgenomics/Kaptive/blob/main/requirements.txt'}, 'id': 1}, {'term': {'type': 'requirements', 'url': 'https://github.com/klebgenomics/Kaptive/blob/main/docs/requirements.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"sap/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/sap/cmd/1.1.3\",\n    \"name\": \"sap\",\n    \"description\": [\n      \"Pairwise structure alignment via double dynamic programming.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/mathbio-nimr-mrc-ac-uk/SAP\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/mathbio-nimr-mrc-ac-uk/SAP\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82d3c10e2f3aa0b3b9c68\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/sap/undefined/None\",\n    \"name\": \"sap\",\n    \"description\": [\n      \"Enhancing protein backbone angle prediction by using simpler models of deep neural networks.\\n\\nThis project provides the programs and related files of the Simple Angle Predictor (SAP) that predicts protein backbone angles phi, psi, theta, and tau.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://gitlab.com/mahnewton/sap\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Fereshteh Mataeimoghadam\",\n        \"email\": \"fereshteh.mataeimoghadam@griffithuni.edu.au\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"M. A. Hakim Newton\",\n        \"email\": \"mahakim.newton@griffith.edu.au\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8326b10e2f3aa0b3bb0d7\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://gitlab.com/mahnewton/sap:\n```\nREADME content:\n['# Simpler Angle Predictor (SAP) This project provides the programs and related files of SAP that predicts protein backbone angles phi, psi, theta, and tau. # Correction Note that after the paper is published, we have found some programming bugs that affect the accuracy numbers reported in the paper. However, the generalised conclusions hold. Consequently, the programs provided here have been updtaed. We have notified the Scientific Reports journal about this issue and going through their due process. Meanwhile, should you have any queries, please do not hesitate to contact us by email. ## Requirements To run the program, for each protein named **protein_name**, you need the following files - protein_name.fasta - protein_name.dssp - protein_name.pssm - protein_name.t - protein_name.out.ss8 All of these files must be in the inputs folder. ## Getting Input Files The first four file are available from the SPOT-1D dataset and to get the .out.ss8 file, visit the website \"http://download.igb.uci.edu/\" and download SCRATCH-1D release 1.2 package. Then Run SCRATCH-1D on your dataset. It need two inputs: the fasta file of a protein and the name of the output file. ../bin/run_SCRATCH-1D_predictors.sh protein_name.fasta protein_name.out ## Running the Program - Update the prot_list.txt with the names of the protein files - Put the protein data for each protein in a separate folder inside the inputs folder - Run **bash RunSAP.sh** from the **code** folder - The outputs are generated in the outputs folder: .sap file for each protein']```"}, {"role": "user", "content": "Content from https://gitlab.com/mahnewton/sap:\n```\nRepository metadata:\n{'id': 17269923, 'description': 'This project provides the programs and related files of the Simple Angle Predictor (SAP) that predicts protein backbone angles phi, psi, theta, and tau. ', 'name': 'sap', 'name_with_namespace': 'M A Hakim Newton / sap', 'path': 'sap', 'path_with_namespace': 'mahnewton/sap', 'created_at': '2020-03-03T23:48:08.188Z', 'default_branch': 'master', 'tag_list': [], 'topics': [], 'ssh_url_to_repo': 'git@gitlab.com:mahnewton/sap.git', 'http_url_to_repo': 'https://gitlab.com/mahnewton/sap.git', 'web_url': 'https://gitlab.com/mahnewton/sap', 'readme_url': 'https://gitlab.com/mahnewton/sap/-/blob/master/README.md', 'forks_count': 0, 'avatar_url': None, 'star_count': 1, 'last_activity_at': '2021-03-29T01:23:20.569Z', 'namespace': {'id': 2570579, 'name': 'M A Hakim Newton', 'path': 'mahnewton', 'kind': 'user', 'full_path': 'mahnewton', 'parent_id': None, 'avatar_url': '/uploads/-/system/user/avatar/2069699/avatar.png', 'web_url': 'https://gitlab.com/mahnewton'}, 'container_registry_image_prefix': 'registry.gitlab.com/mahnewton/sap', '_links': {'self': 'https://gitlab.com/api/v4/projects/17269923', 'issues': 'https://gitlab.com/api/v4/projects/17269923/issues', 'merge_requests': 'https://gitlab.com/api/v4/projects/17269923/merge_requests', 'repo_branches': 'https://gitlab.com/api/v4/projects/17269923/repository/branches', 'labels': 'https://gitlab.com/api/v4/projects/17269923/labels', 'events': 'https://gitlab.com/api/v4/projects/17269923/events', 'members': 'https://gitlab.com/api/v4/projects/17269923/members', 'cluster_agents': 'https://gitlab.com/api/v4/projects/17269923/cluster_agents'}, 'packages_enabled': True, 'empty_repo': False, 'archived': False, 'visibility': 'public', 'owner': {'id': 2069699, 'username': 'mahnewton', 'name': 'M A Hakim Newton', 'state': 'active', 'locked': False, 'avatar_url': 'https://gitlab.com/uploads/-/system/user/avatar/2069699/avatar.png', 'web_url': 'https://gitlab.com/mahnewton'}, 'resolve_outdated_diff_discussions': False, 'container_expiration_policy': {'cadence': '7d', 'enabled': False, 'keep_n': None, 'older_than': None, 'name_regex': None, 'name_regex_keep': None, 'next_run_at': '2020-03-10T23:48:08.214Z'}, 'repository_object_format': 'sha1', 'issues_enabled': True, 'merge_requests_enabled': True, 'wiki_enabled': True, 'jobs_enabled': True, 'snippets_enabled': True, 'container_registry_enabled': True, 'service_desk_enabled': True, 'can_create_merge_request_in': True, 'issues_access_level': 'enabled', 'repository_access_level': 'enabled', 'merge_requests_access_level': 'enabled', 'forking_access_level': 'enabled', 'wiki_access_level': 'enabled', 'builds_access_level': 'enabled', 'snippets_access_level': 'enabled', 'pages_access_level': 'enabled', 'analytics_access_level': 'enabled', 'container_registry_access_level': 'enabled', 'security_and_compliance_access_level': 'private', 'releases_access_level': 'enabled', 'environments_access_level': 'enabled', 'feature_flags_access_level': 'enabled', 'infrastructure_access_level': 'enabled', 'monitor_access_level': 'enabled', 'model_experiments_access_level': 'enabled', 'model_registry_access_level': 'enabled', 'emails_disabled': False, 'emails_enabled': True, 'shared_runners_enabled': True, 'lfs_enabled': True, 'creator_id': 2069699, 'import_status': 'none', 'open_issues_count': 0, 'description_html': '<p data-sourcepos=\"1:1-1:153\" dir=\"auto\">This project provides the programs and related files of the Simple Angle Predictor (SAP) that predicts protein backbone angles phi, psi, theta, and tau.</p>', 'updated_at': '2023-12-09T16:32:49.273Z', 'ci_config_path': None, 'public_jobs': True, 'shared_with_groups': [], 'only_allow_merge_if_pipeline_succeeds': False, 'allow_merge_on_skipped_pipeline': None, 'request_access_enabled': True, 'only_allow_merge_if_all_discussions_are_resolved': False, 'remove_source_branch_after_merge': True, 'printing_merge_request_link_enabled': True, 'merge_method': 'merge', 'squash_option': 'default_off', 'enforce_auth_checks_on_uploads': True, 'suggestion_commit_message': None, 'merge_commit_template': None, 'squash_commit_template': None, 'issue_branch_template': None, 'warn_about_potentially_unwanted_characters': True, 'autoclose_referenced_issues': True, 'max_artifacts_size': None, 'external_authorization_classification_label': '', 'requirements_enabled': False, 'requirements_access_level': 'enabled', 'security_and_compliance_enabled': False, 'compliance_frameworks': [], 'permissions': {'project_access': None, 'group_access': None}}```"}, {"role": "user", "content": "Content from https://github.com/mathbio-nimr-mrc-ac-uk/SAP:\n```\nREADME content:\n[\"# SAP : Pairwise structure alignment via double dynamic programming ## References Users publishing results obtained with the program and its applications should acknowledge its use by the following citation: - Taylor W.R., Protein structure comparison using iterated double dynamic programming. Protein Sci. 8 (1999) 654-665. - Taylor W.R., Protein structure comparison using SAP. Methods Mol. Biol. 143 (2000) 19-32. ## Install / Uninstall Please read the general 'INSTALL' instructions. For documentation execute 'doxygen doxygen.cfg' in the 'src' directory. Documentation files are created in 'doc/html' and 'doc/latex'. The latex documentation is completed by executing 'make pdf' in the 'doc/latex' directory, which creates 'refman.ps' and 'refman.pdf'. ## Usage sap <PDB_1> <PDB_2> sap <PDB_1> <PDB_2> <one2one> 'one2one' is an optional integer value added to the diagonal; any negative value selects the default value of 1000. ## Web Server http://mathbio.crick.ac.uk/wiki/SAP ## Authorship * (C) 1999 W.R. Taylor (program author) * (C) 2007 Alessandro Pandini : SAP web server * (C) 2007 Jens Kleinjung : code release * (C) 2008 Siv Hollup : code update to ANSI C standard ## Availability The program is made available under the GNU Public License for academic scientific purposes, under the condition that proper acknowledgement is made to the authors of the program in publications resulting from the use of the program. ## License This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>.\"]```"}, {"role": "user", "content": "Content from https://github.com/mathbio-nimr-mrc-ac-uk/SAP:\n```\nRepository metadata:\n{'name': 'SAP', 'label': ['SAP'], 'description': [{'term': 'SAP: pairwise structure alignment using double-dynamic programming (by Willie Taylor)', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v.1.1.3'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/mathbio-nimr-mrc-ac-uk/SAP', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Jens Kleinjung', 'type': 'person', 'email': 'jens@jkleinj.eu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Jens Kleinjung', 'type': 'person', 'email': 'jkleinj@koeln.mathbio.nimr', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/mathbio-nimr-mrc-ac-uk/SAP/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"fusioncatcher/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/fusioncatcher/cmd/1.33\",\n    \"name\": \"fusioncatcher\",\n    \"description\": [\n      \"Finder of Somatic Fusion Genes in RNA-seq data.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/ndaniel/fusioncatcher\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/ndaniel/fusioncatcher\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a073d3b8acd1a2f9b49b\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"sourceforge/fusioncatcher/None/None\",\n    \"name\": \"fusioncatcher\",\n    \"description\": [\n      \"FusionCatcher searches for novel/known somatic fusion genes, translocations, and chimeras in RNA-seq data (paired-end reads from Illumina NGS platforms like Solexa and HiSeq) from diseased samples.\\n\\nThe aims of FusionCatcher are:\\n- very good detection rate for finding candidate fusion genes,\\n- very easy to use (i.e. no a priori knowledge of databases and bioinformatics is needed in order to run FusionCatcher),\\n- very good detection of challenging fusion genes, like for example IGH fusions, CIC fusions, DUX4 fusions, CRLF2 fusions, TCF3 fusions, etc.\\n- to be as automatic as possible (i.e. the FusionCatcher will choose automatically the best parameters in order to find candidate fusion genes, e.g. finding automatically the adapters, building the exon-exon junctions automatically based on the length of the input reads, etc.) while providing the best possible detection rate for finding fusion genes.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://sourceforge.net/projects/fusioncatcher\",\n        \"kind\": \"sourceforge\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://sourceforge.net/p/fusioncatcher/wiki/Home/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"sourceforge\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/fusioncatcher:\n```\nProject metadata:\n{'description': 'FusionCatcher searches for novel/known somatic fusion genes, translocations, and chimeras in RNA-seq data (paired-end reads from Illumina NGS platforms like Solexa and HiSeq) from diseased samples.\\nThe aims of FusionCatcher are:\\n- very good detection rate for finding candidate fusion genes,\\n- very easy to use (i.e. no a priori knowledge of databases and bioinformatics is needed in order to run FusionCatcher),\\n- very good detection of challenging fusion genes, like for example IGH fusions, CIC fusions, DUX4 fusions, CRLF2 fusions, TCF3 fusions, etc.\\n- to be as automatic as possible (i.e. the FusionCatcher will choose automatically the best parameters in order to find candidate fusion genes, e.g. finding automatically the adapters, building the exon-exon junctions automatically based on the length of the input reads, etc.) while providing the best possible detection rate for finding fusion genes.', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['/p/fusioncatcher/activity/', 'https://sourceforge.net/auth/?return_to=%2Fprojects%2Ffusioncatcher%2F', '/p/fusioncatcher/activity/feed']}, {'text': 'Categories\\nBio-Informatics\\nLicense\\nGNU General Public License version 3.0 (GPLv3)\\nCategories\\nBio-Informatics\\nLicense\\nGNU General Public License version 3.0 (GPLv3)', 'hrefs': ['/directory/bio-informatics/']}, {'text': 'Follow FusionCatcher\\nFusionCatcher Web Site\\nFollow FusionCatcher\\nFusionCatcher Web Site', 'hrefs': ['https://sourceforge.net/p/fusioncatcher/wiki/Home/']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof FusionCatcher!\\nUser Reviews\\nBe the first to\\npost a review\\nof FusionCatcher!', 'hrefs': ['/projects/fusioncatcher/reviews/new']}, {'text': 'Additional Project Details\\nOperating Systems\\nLinux\\n,\\nBSD\\nIntended Audience\\nScience/Research\\nProgramming Language\\nPython\\nRelated Categories\\nPython Bio-Informatics Software\\nRegistered\\n2013-06-24\\nAdditional Project Details\\nOperating Systems\\nLinux\\n,\\nBSD\\nIntended Audience\\nScience/Research\\nProgramming Language\\nPython\\nRelated Categories\\nPython Bio-Informatics Software\\nRegistered\\n2013-06-24', 'hrefs': ['/directory/python/', '/directory/bio-informatics/python/', '/directory/linux/', '/directory/bsd/']}]}```"}, {"role": "user", "content": "Content from https://github.com/ndaniel/fusioncatcher:\n```\nREADME content:\n['FusionCatcher ============= Finder of somatic fusion-genes in RNA-seq data. Download / Install / Update / Upgrade [FusionCatcher](http://github.com/ndaniel/fusioncatcher) ---------------------------------------------------------------------------------------------- Use this one-line command: ``` wget http://sf.net/projects/fusioncatcher/files/bootstrap.py -O bootstrap.py && python bootstrap.py -t --download ``` If one wants to have all the questions asked by boostrap.py answered automatically with yes then add `-y` to the command above. For more installing options, see: ``` bootstrap.py --help ``` On Ubuntu Linux running this command before installing FusionCatcher using `bootstrap.py` would help making the installation process smoother: ``` sudo apt-get install wget gawk gcc g++ make cmake automake curl unzip zip bzip2 tar gzip pigz parallel build-essential libncurses5-dev libc6-dev zlib1g zlib1g-dev libtbb-dev libtbb2 python python-dev python-numpy python-biopython python-xlrd python-openpyxl default-jdk ``` FusionCatcher can be installed also using `conda`, as follows: ``` conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda create -n fusioncatcher fusioncatcher source activate fusioncatcher download-human-db.sh ``` FusionCatcher can be installed also from GitHub, as follows: ``` git clone https://github.com/ndaniel/fusioncatcher cd fusioncatcher/tools/ ./install_tools.sh cd ../data ./download-human-db.sh ``` NOTE: Here it is assumed that Python 2.7.x, BioPython (>v1.5), and Java Runtime Environment 1.8 are already installed. Description ----------- FusionCatcher searches for novel/known somatic fusion genes, translocations, and chimeras in RNA-seq data (paired-end or single-end reads from Illumina NGS platforms like Solexa/HiSeq/NextSeq/MiSeq/MiniSeq) from diseased samples. The aims of FusionCatcher are: * very good detection rate for finding candidate somatic fusion genes (see somatic mutations; using a matched normal sample is optional; several databases of known fusion genes found in healthy samples are used as a list of known false positives; biological knowledge is used, like for example gene fusion between a gene and its pseudogene is filtered out), * very good RT-PCR validation rate of found candidate somatic fusion genes (this is very important for us), * very good detection of challenging fusion genes, like for example IGH fusions, CIC fusions, DUX4 fusions, CRLF2 fusions, TCF3 fusions, etc. * very easy to use (i.e. no a priori knowledge of bioinformatic databases and bioinformatics is needed in order to run FusionCatcher BUT Linux/Unix knowledge is needed; it allows a very high level of control for expert users), * to be as automatic as possible (i.e. the FusionCatcher will choose automatically the best parameters in order to find candidate somatic fusion genes, e.g. finding automatically the adapters, quality trimming of reads, building the exon-exon junctions automatically based on the length of the reads given as input, etc. while giving also full control to expert users) while providing the best possible detection rate for finding somatic fusion genes (with a very low rate of false positives but a very good precision). Manual ------ A detailed manual is available [here](doc/manual.md). Forum ----- A forum for FusionCatcher is available at [Google Groups](http://groups.google.com/d/forum/fusioncatcher). Release history --------------- A complete release history can be found [here](NEWS). Official releases ----------------- Old releases and the latest official release of FusionCatcher are on https://sourceforge.net/projects/fusioncatcher/files/ Citing ------ D. Nicorici, M. Satalan, H. Edgren, S. Kangaspeska, A. Murumagi, O. Kallioniemi, S. Virtanen, O. Kilkku, FusionCatcher \u2013 a tool for finding somatic fusion genes in paired-end RNA-sequencing data, bioRxiv, Nov. 2014, [DOI:10.1101/011650](http://dx.doi.org/10.1101/011650)']```"}, {"role": "user", "content": "Content from https://github.com/ndaniel/fusioncatcher:\n```\nRepository metadata:\n{'name': 'fusioncatcher', 'label': ['fusioncatcher'], 'description': [{'term': 'Finder of Somatic Fusion Genes in RNA-seq data', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['1.33', '1.30', '1.20', '1.10', '1.00'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/ndaniel/fusioncatcher', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/rna-seq', 'term': 'rna-seq', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/fusion-genes', 'term': 'fusion-genes', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/somatic-fusion-genes', 'term': 'somatic-fusion-genes', 'vocabulary': ''}, 'id': 2}], 'operations': [], 'authors': [{'term': {'name': 'Daniel Nicorici', 'type': 'person', 'email': 'Daniel.Nicorici@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'BSC-Support-Team', 'type': 'person', 'email': 'support-alert@bsc.es', 'maintainer': False}, 'id': 1}, {'term': {'name': 'daniel', 'type': 'person', 'email': 'daniel.nicorici@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Anders Lind', 'type': 'person', 'email': 'mail@anderslind.se', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/ndaniel/fusioncatcher/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://sourceforge.net/p/fusioncatcher/wiki/Home/:\n```\nProject metadata:\n{'description': None, 'sections': []}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"binning/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"github/binning/None/None\",\n    \"name\": \"binning\",\n    \"description\": [\n      \"Interval binning for Go.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/martijnvermaat/binning\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"github\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/binning/undefined/None\",\n    \"name\": \"binning\",\n    \"description\": [\n      \"Scripts required to calculate tetramer frequencies and create input files for ESOM.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/tetramerFreqs/Binning\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Gregory J. Dick,\",\n        \"email\": \"gdick@umich.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Sunit Jain\",\n        \"email\": \"sunitj@umich.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8333b10e2f3aa0b3bb861\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/tetramerFreqs/Binning:\n```\nREADME content:\n['[![DOI](https://zenodo.org/badge/16889/tetramerFreqs/Binning.svg)](https://zenodo.org/badge/latestdoi/16889/tetramerFreqs/Binning) Binning ======= Scripts required to calculate tetramer frequencies and create input files for ESOM.<br> See: Dick, G.J., A. Andersson, B.J. Baker, S.S. Simmons, B.C. Thomas, A.P. Yelton, and J.F. Banfield (2009). Community-wide analysis of microbial genome sequence signatures. Genome Biology, 10: R85<br> Open Access: http://genomebiology.com/2009/10/8/R85 How to ESOM? ============ These instructions are for ESOM-based for binning: see http://databionic-esom.sourceforge.net/ for software download and manual. 1. Generate input files. ------------------------- * Although not necessary but we recommend adding some reference genomes based on your 16s/OTU analysis as \\'controls\\'. The idea is that, if the ESOM worked, your reference genome should form a bin itself. You may do this by downloading genomes in fasta format from any public database, preferably a complete single sequence genome. * Use the `esomWrapper.pl` script to create the relevant input files for ESOM. In order to run this script, you\\'ll need to have all your sequence(in fasta format) files with the same extension in the same folder. For example: `perl esomWrapper.pl -path fasta_folder -ext fa`<br> For more help and examples, type:<br> `perl esomWrapper.pl -h` * The script will use the fasta file to produce three tab-delimited files that ESOM requires: * Learn file = a table of tetranucleotide frequencies (.lrn) * Names file = a list of the names of each contig (.names) * Class file = a list of the class of each contig, which can be used to color data points, etc. ( .cls) **NOTE:**`class number`: The esom mapping requires that you define your sequences as classes. We generally define all the sequences that belong to your query (meatgenome for example) as 0 and all the others 1, 2 and so on. think of these as your predefined bins, each sequence that has the same class number will be assigned the same color in the map. * These files are generated using Anders Anderssons perl script `tetramer_freqs _esom.pl` which needs to be in the same folder as the `esomWrapper.pl`. To see how to use the `tetramer_freqs _esom.pl` independent of the wrapper, type: `perl tetramer_freqs _esom.pl -h` 2. Run ESOM: ------------- * On you termial, run w/ following command from anywhere (X11 must be enabled): <br> `./esomana` * Load .lrn, .names, and .cls files (File > load .lrn etc.) * Normalize the data(optional, but recommended): under data tab, see Z-transform, RobustZT, or To\\\\[0,1\\\\] as described in the users manual. I find that this RobustZT makes the map look cleaner. 3. Train the data: ------------------- ###Using the GUI * Tools > Training: * Parameters: use default parameters with the following exceptions. Note this is what seems work best for AMD datasets but the complete parameter space has not been fully optimized. David Soergel (Brenner Lab) is working on this: * Training algorithm: K-batch * Number of rows/columns in map: I use ~5-6 times more neurons than there are data points. E.g. for 12000 data points (window, NOT contigs) I use 200 rows x 328 columns (~65600 neurons). * Start value for radius = 50 (increase/decrease for smaller/larger maps). * I\\'ve never seen a benefit to training for more than 20 epochs for the AMD data. * Hit \\'START\\' -- training will take 10 minutes to many hours depending on the size of the data set and parameters used. ###From the terminal * At this point, you may also choose to add additional data (like coverage) to your contigs. You may do so using the `addInfo2lrn.pl` script **OR** by simply using the flag `-info` in `esomTrain.pl`. * `esomTrain.pl` script maybe used to train the data without launching the GUI. This script will add the additional information to the lrn file (using `-info`), normalize it and train the ESOM. Type `perl esomTrain.pl -h` in your terminal to see the help document for this script. * To view the results of the training, simply launch ESOM by following the instructions in *Step 5: Loading a previous project* to load the relevant files. * Resume analysis from *Step 4: Analyzing the output* 4. Analyzing the output: ------------------------ * Best viewed (see VIEW tab) with UMatrix background, tiled display. Use Zoom, Color, Bestmatch size to get desired view. Also viewing without data points drawn (uncheck \"Draw bestmatches\") helps to see the underlying data structure. * Use CLASSES tab to rename and recolor classes. * To select a region of the map, go to DATA tab then draw a shape with mouse (holding left click), close it with right click. Data points will be selected and displayed in DATA tab. * To assign data points to bins, use the CLASS tab and using your pointer draw a boundary around the region of interest (e.g. using the data structure as a guide -- see also \"contours\" box in VIEW tab which might help to delineate bins). This will assign each data point to a class (bin). The new .cls file can be saved (`File > Save .cls`) for further analysis. 5. Loading a previous project: ------------------------------ * On you termial, run w/ following command from anywhere (X11 must be enabled): `./esomana` * `File > load .wts` Questions? ---------- * [Gregory J. Dick](http://www.earth.lsa.umich.edu/geomicrobiology/Index.html \"Geomicro Homepage\"), <br> gdick \\\\[AT\\\\] umich \\\\[DOT\\\\] edu,<br> Assistant Professor, Michigan Geomicrobiology Lab,<br> University of Michigan * [Sunit Jain](http://www.sunitjain.com \"Sunit\\'s Homepage\"), <br> sunitj \\\\[AT\\\\] umich \\\\[DOT\\\\] edu,<br> Bioinformatics Specialist, Michigan Geomicrobiology Lab,<br> University of Michigan.']```"}, {"role": "user", "content": "Content from https://github.com/tetramerFreqs/Binning:\n```\nRepository metadata:\n{'name': 'Binning', 'label': ['Binning'], 'description': [{'term': 'Scripts required to calculate tetramer frequencies and create input files for ESOM. See: Dick, G.J., A. Andersson, B.J. Baker, S.S. Simmons, B.C. Thomas, A.P. Yelton, and J.F. Banfield (2009).  Community-wide analysis of microbial genome sequence signatures. Genome Biology, 10: R85', 'id': 0}], 'links': [], 'webpage': [{'term': 'http://genomebiology.com/2009/10/8/R85', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.9.3'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/tetramerFreqs/Binning', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Sunit Jain', 'type': 'person', 'email': 'sunitj@umich.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Jason Stajich', 'type': 'person', 'email': 'jasonstajich.phd@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Sunit', 'type': 'person', 'email': 'mail4sunit@gmail.com', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/tetramerFreqs/Binning/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/martijnvermaat/binning:\n```\nREADME content:\n['binning: Interval binning for Go ================================ [![GoDoc](https://godoc.org/github.com/martijnvermaat/binning?status.png)](https://godoc.org/github.com/martijnvermaat/binning) [![Build Status](https://travis-ci.org/martijnvermaat/binning.svg?branch=master)](https://travis-ci.org/martijnvermaat/binning) These are some utility functions for working with the interval binning scheme as used in the [UCSC Genome Browser](http://genome.cshlp.org/content/12/6/996.full). This scheme can be used to implement fast overlap-based querying of intervals, essentially mimicking an [R-tree](https://en.wikipedia.org/wiki/R-tree) index. Note that some database systems natively support spatial index methods such as R-trees. See for example the [PostGIS](http://postgis.net) extension for PostgreSQL. Although in principle the method can be used for binning any kind of intervals, be aware that the largest position supported by this implementation is `2^29` (which covers the longest human chromosome). ```go // Use the standard UCSC binning scheme. b := binning.StandardBinning() // Get the bin for some interval. bin, error := b.Assign(74012, 173034) if error != nil { log.Fatal(\"Binning.Assign error:\", error) } fmt.Println(bin) // Output: 73 ```']```"}, {"role": "user", "content": "Content from https://github.com/martijnvermaat/binning:\n```\nRepository metadata:\n{'name': 'binning', 'label': ['binning'], 'description': [{'term': 'Interval binning for Go', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/martijnvermaat/binning', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Martijn Vermaat', 'type': 'person', 'email': 'martijn@vermaat.name', 'maintainer': False}, 'id': 0}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/martijnvermaat/binning/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"mir-prefer/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/mir-prefer/cmd/0.24\",\n    \"name\": \"mir-prefer\",\n    \"description\": [\n      \"MicroRNA PREdiction From small RNA-seq data.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/hangelwen/miR-PREFeR\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/hangelwen/miR-PREFeR\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/mir-prefer/cmd/None\",\n    \"name\": \"mir-prefer\",\n    \"description\": [\n      \"Uses expression patterns of miRNA and follows the criteria for plant microRNA annotation to accurately predict plant miRNAs from one or more small RNA-Seq data samples of the same species.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.cse.msu.edu/~leijikai/mir-prefer/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"hangelwen@gmail.com\",\n        \"maintainer\": false,\n        \"url\": \"http://www.cse.msu.edu/~leijikai/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8293410e2f3aa0b3b832b\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/hangelwen/miR-PREFeR:\n```\nREADME content:\n['<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"> <html xmlns=\"http://www.w3.org/1999/xhtml\"> <head> <title>README.html</title> </head> <body> <h1>miR-PREFeR: <strong>mi</strong>cro<strong>R</strong>NA <strong>PRE</strong>diction <strong>F</strong>rom small <strong>R</strong>NAseq data</h1> <p><strong>Plant microRNA prediction tools that utilize small RNA sequencing data are emerging quickly. These existing tools have at least one of the following problems: 1. high false positive rate; 2. long running time; 3. work only for genomes in their databases; 4. hard to install or use. We develop miR-PREFeR, which uses expression patterns of miRNA and follows the criteria for plant microRNA annotation to accurately predict plant miRNAs from one or more small RNA-Seq data samples of the same species. We tested miR-PREFeR on several plant species. The results show that miR-PREFeR is sensitive, accurate, fast, and has low memory footprint.</strong></p> <p><strong>The miR-PREFeR paper is published on Bioinformatics. <a href=\"http://bioinformatics.oxfordjournals.org/content/30/19/2837.abstract\">http://bioinformatics.oxfordjournals.org/content/30/19/2837.abstract</a>. Please cite the paper if you use the tool in your work.</strong></p> <p><strong>The miR-PREFeR pipeline is still under active development. To use the newest features from the pipeline, check the <a href=\"https://github.com/hangelwen/miR-PREFeR\">https://github.com/hangelwen/miR-PREFeR</a> page and obtain the newest version. The current version is only tested under Python 2.6.7, Python 2.7.2 and Python 2.7.3 and should work under Python 2.6.* and Python 2.7.* (Tested platforms: Linux, Mac OS). It does NOT work under Python 3.0 currently. If you find any problem, please contact the author at hangelwen@gmail.com.</strong></p> <h2>1. Required programs</h2> <p>To run the miR-PREFeR pipeline, the ViennaRNA package(tested under 1.8.5 or 2.1.2, 2.1.5) and samtools(0.1.15 or later. Tested under 0.1.18, 0.1.19) should be installed on the system. miR-PREFeR uses samtools commands to manipulate SAM and BAM alignment files, and uses RNALfold from the ViennaRNA package to do RNA secondary structure folding. The miR-PREFeR pipeline takes SAM alignment files as input, so an aligner such as Bowtie is also required (Not required by the pipeline, but needed for preparing the input data).</p> <h3>ViennaRNA package</h3> <p>The ViennaRNA package can be downloaded from <a href=\"http://www.tbi.univie.ac.at/~ronny/RNA/index.html.\">http://www.tbi.univie.ac.at/~ronny/RNA/index.html.</a>. The website provides precompiled packages for most popular Linux distributions. For these platforms, download the corresponding package and install it according to the platform package management system. If no precompiled package is provided for your platform, you can install it from source code. To install the package from source code, simply decompress the source code (e.g. type \"tar xvf *.tar.gz) archive and cd into the folder, then:</p> <pre><code>./configure make make install </code></pre> <p>The above commands only work if the user has the root permission.</p> <p>If the user does not have root permission. then the package can be installed to user specified locations (here I assume you install the program in directory /user/tools/ViennaRNA/):</p> <pre><code>./configure --prefix=\"/user/tools/ViennaRNA/\" --without-perl make make install </code></pre> <p>After finish installing the package, <strong>add the /user/tools/ViennaRNA/bin directory to the PATH environment variable.</strong> Instructions on how to add a path to the PATH environment variable can be found at <a href=\"http://www.cyberciti.biz/faq/unix-linux-adding-path/\">http://www.cyberciti.biz/faq/unix-linux-adding-path/</a> or just google \"<code>add directory to PATH</code>\".</p> <p><strong>NOTE: Because that RNALfold from the ViennaRNA package version 2.0.4 has a bug (If the input sequence has no valid folding structure, the program produces a segmentation fault), please make sure to use the newest version of the ViennaRNA package.</strong></p> <h3>Samtools</h3> <p>Samtools can be downloaded from <a href=\"http://samtools.sourceforge.net/\">http://samtools.sourceforge.net/</a>. Please follow the instructions from the package to install it. Please note the version 0.1.15 or later is needed (The pipeline uses the <code>samtools depth</code> command, which was introduced from version 0.1.15). One example set of commands to install samtools are given below. First, suppose you downloaded samtools-0.1.19.tar.bz2 and saved it in a folder with address /Users/xyz/. Then:</p> <pre><code>cd /Users/xyz/ tar jfx samtools-0.1.19.tar.bz2 cd samtools-0.1.19 make </code></pre> <p>If successful, you should see an executable program named \"samtools\" in the same folder. You need to add the samtools executable to the PATH environment variable.</p> <h3>For Mac OS users</h3> <p>Since it maybe difficult for some Mac OS users to successfully compile C/C++ programs, we provide several pre-compiled versions of samtools and RNALfold, which are put in the dependency/Mac/ folder. Mac OS users can first try to use the pre-compiled programs. If they do not work (This is possible), then try to compile from the source code by following the instructions in this section. To test whether a pre-compiled program works on your machine, change directory to the folder that contains the program, and then:</p> <pre><code>chmod +x RNALfold ./RNALfold -V </code></pre> <p>Here I used the pre-compiled RNALfold as an example. If you can see usage information from the output, then the program works on your machine. Then add the path of the RNALfold to the PATH environment variable, and RNALfold is correctly configured.</p> <p>Note that in the dependency/Mac/ folder there are several folders named after specific Mac OS X version. If you can find the same version as your own system, then use pre-compiled programs under that folder. Otherwise, try to use programs which were compiled with lower Mac OS X version number.</p> <h3>For Windows users</h3> <p>The pipeline depends on Samtools, and a few Linux/Unix utilities such as <code>sort</code>. Windows users need to install a Linux environment emulator such as Cygwin (<a href=\"http://cygwin.com/\">http://cygwin.com/</a>), then install Samtools. Please follow instructions on the Cygwin website (<a href=\"http://cygwin.com/install.html\">http://cygwin.com/install.html</a>) for more information.</p> <h2>2. Obtain and install the pipeline</h2> <p>The miR-PREFeR pipeline is hosted on Github (https://github.com/hangelwen/miR-PREFeR). To obtain the program, open an terminal window and execute the following command:</p> <pre><code>git clone https://github.com/hangelwen/miR-PREFeR.git </code></pre> <p>This makes an directory named \\'miR-PREFeR\\' in the current directory, and clones the newest version of the miR-PREFeR pipeline code into the directory. The pipeline is ready to use.</p> <p>If you do not have <code>git</code> on your system, simply go to <a href=\"https://github.com/hangelwen/miR-PREFeR/releases\">https://github.com/hangelwen/miR-PREFeR/releases</a>, download the zip file or the tar.gz file of the latest version.</p> <p><strong>The miR-PREFeR pipeline is still under active development, to use the newest features from the pipeline, check <a href=\"https://github.com/hangelwen/miR-PREFeR\">https://github.com/hangelwen/miR-PREFeR</a> and obtain the newest version.</strong></p> <h2>3. Test the pipeline.</h2> <p>The package contains a small dataset for testing whether the pipeline works after you download it. Please refer to <a href=\"HOW_TO_RUN_EXAMPLE.txt\">HOW_TO_RUN_EXAMPLE.txt</a> file to see more infomation about testing.</p> <h2>4. How to run the pipeline.</h2> <h3>a. Prepare input data for the pipeline.</h3> <p>The miR-PREFeR pipeline needs the following input:</p> <ol> <li>A fasta file, which contains the gnome sequences of the species under study.</li> <li>one or more SAM files which contains the alignments of small RNAseq data with the gnome.</li> <li>(Optional) An GFF (<a href=\"http://www.sanger.ac.uk/resources/software/gff/spec.html\">http://www.sanger.ac.uk/resources/software/gff/spec.html</a>) file which lists regions in the gnome sequences that should be ignored from miRNA analysis.</li> </ol> <h4>a). Genome fasta file</h4> <p>Fasta format specification can be found at <a href=\"http://www.ncbi.nlm.nih.gov/BLAST/blastcgihelp.shtml\">http://www.ncbi.nlm.nih.gov/BLAST/blastcgihelp.shtml</a>. In miR-PREFeR, for the string following \">\", only the first word that is delimited by any white space characters (whitespace, tab, etc) is used. For example, for the following sequence, \\'ath-MIR773a\\' is used as the identifier of the seqeunce. <strong>Thus, please ensure that all the sequences in the FASTA files have different identifiers.</strong></p> <pre><code>&gt;ath-MIR773a MI0005103 AGGAGGCAAUAGCUUGAGCAAAUAAUUGAUUGCAGAAGUCCAUCGACUAAAGCUGUCACCUGUUUGCUUCCAGCUUUUGUCUCCU </code></pre> <h4>b). SAM alignment files</h4> <p>The miR-PREFeR pipeline takes SAM format alignment files. SAM alignment files can be generated by many aligners. Here we use Bowtie (<a href=\"http://bowtie-bio.sourceforge.net/index.shtml\">http://bowtie-bio.sourceforge.net/index.shtml</a>) as an example.</p> <p><strong>Step 1. Prepare RNAseq fasta files for Bowtie</strong></p> <p><strong><em>a. Prepare RNAseq fasta files from uncollpased fasta files.</em></strong></p> <p>Because miR-PREFeR is able to use multiple small RNAseq data as input and utilizes information from multiple RNAseq samples to increase the performance of the miRNA loci prediction. <strong>Thus, It\\'s an requirement that the RNAseq reads data (fasta files that contain the RNAseq reads) be preprocessed by the provided <code>process-reads-fasta.py</code> script.</strong> The script is under the <code>scripts</code> folder. The script takes a file that contains a list of sample/library names and an list of <strong>uncollapsed fasta files</strong> as input, and renames the reads in the input files, and produces a list of collapsed fasta files. Here <code>uncollpased fasta</code> means that identical reads in the fasta file have multiple entries.</p> <p>For example, if you have 3 small RNAseq samples in uncollpased fasta format with names <code>SAMPLE1.fasta</code>, <code>SAMPLE2.fasta</code>, and <code>SAMPLE3.fasta</code>, and the samples are from \\'root\\', \\'flower\\', and \\'salt\\', respectively, then first create a file with the following three lines:</p> <pre><code>root flower salt </code></pre> <p>If you name the file as <code>samplename.txt</code>, then run (assume you are in the directory where you installed miR-PREFeR. Otherwise, using the correct path to the process-reads-fasta.py script.):</p> <pre><code>python scripts/process-reads-fasta.py samplename.txt SAMPLE1.fasta SAMPLE2.fasta SAMPLE3.fasta </code></pre> <p>Three new files with name <code>SAMPLE1.fasta.processed</code>, <code>SAMPLE2.fasta.processed</code>, and <code>SAMPLE3.fasta.processed</code> are produced <strong>in the same folder as the input fasta files</strong>. These three files are then aligned to the genome fasta file using Bowtie.</p> <p>The following is a section of the processed fasta file from the root sample. The identifier of each read contains the sample name as the first part, then a sequential number rx as to identify the read, the last part is xN, where N is the depth (the number of occurrence) of the read. <strong>Note that this is the required format of the identifiers of the reads. The read identifier in the SAM alignment file MUST follow the format. Otherwise, the pipeline stops at the very early stage and prints message complaining the format of the SAM files.</strong></p> <pre><code>&gt;root_r0_x1 ACTACTGCAAGGGCTGGCTCAACCCGC &gt;root_r1_x5 TGGTTGCTGTCGCTGGTCGCTGGT &gt;root_r2_x1 CAAGGACAACAAGTGACGCCG &gt;root_r3_x186 ATAACCGTAGTAATTCTAG &gt;root_r4_x1 CCCATATTTTCTCTGAGCCTT &gt;root_r5_x1 ATACGGTTCGTTCTGG &gt;root_r6_x31 AACTGCGAATGGCTCATTAAAT &gt;root_r7_x1 AAAGGTCGACGCGGGCTCTGCCCG &gt;root_r8_x1 GATCCGGTGAAGTGTTCGGATC &gt;root_r9_x1 GGTAGAGATCGGAGG &gt;root_r10_x24 GTGGTTGTAGTATAGCGGTTAG &gt;root_r11_x2 TTTGAACGCAAGTTGCGCCCCAA </code></pre> <p><strong><em>b. Prepare RNAseq fasta files from mirdeep/mirdeep2 format collapsed fasta files.</em></strong></p> <p>If you have fasta files in mirDeep, mirDeep2 format (Reads are already collapsed and the identifiers follow format \"Identifer_xN\", where \\'Identifier\\' is an arbitrary string without any blank characters. \\'N\\' is the number of occurrence (depth) of the read in the library.), we provide a script <code>convert-mirdeep2-fasta.py</code>, which is also under the <code>scripts</code> folder, to convert them to the required format for miR-PREFeR:</p> <pre><code>python scripts/convert-mirdeep2-fasta.py samplename.txt SAMPLE1.mirdeep.fasta SAMPLE2.mirdeep.fasta SAMPLE3.mirdeep.fasta </code></pre> <p>Three new files with name <code>SAMPLE1.mirdeep.fasta.processed</code>, <code>SAMPLE2.mirdeep.fasta.processed</code>, and <code>SAMPLE3.mirdeep.fasta.processed</code> are produced <strong>in the same folder as the input fasta files</strong>. These three files are then aligned to the genome fasta file using Bowtie.</p> <p><strong>Step 2. Align the RNA-seq fasta files with Bowtie:</strong></p> <p>A script, <code>bowtie-align-reads.py</code>, is provided to align the read files in fasta format generated by <code>process-reads-fasta.py</code>, <code>convert-mirdeep2-fasta.py</code>, or <code>convert-readcount-file.py</code>. The script is under the <code>scripts</code> folder. Here is an example to align the processed fasta files generated in the previous stage using the script:</p> <pre><code>python bowtie-align-reads.py -f -r TAIR10.fas -t bowtie-index/ -p 8 SAMPLE1.fasta.processed SAMPLE2.fasta.processed SAMPLE3.fasta.processed </code></pre> <p>The command first generates the bowtie index files in the bowtie-index/ folder for the TAIR10.fas reference sequences, and then uses 8 threads to align <code>SAMPLE1.fasta.processed</code>, <code>SAMPLE2.fasta.processed</code>, and <code>SAMPLE3.fasta.processed</code>. Unmapped alignments (bowtie outputs SAM files with both mappped and unmapped alignments.) are filtered using SAMtools (the -f opiton). The result files are in the same folder as the input files, with suffix \".sam\". For more options available in <code>bowtie-align-reads.py</code>, please refer to the <a href=\"scripts/README-SCRIPTS.txt\">scripts/README-SCRIPTS.txt</a> file, or run <code>bowtie-align-reads.py</code> with the <code>-h</code> option:</p> <pre><code>python bowtie-align-reads.py -h </code></pre> <p>For those who prefer to use bowtie directly (for example, users want to more bowtie options), please refer to Section <a href=\"#5-conducting-alignment-using-bowtie\">Conducting alignment using Bowtie</a>. The section contains the steps to do alignments using bowtie and the format of the SAM files required by the miR-PREFeR pipeline.</p> <h4>c). The (optional) genome annotation file in gff3 format</h4> <p>For a genome with known annotations, some regions in the genome can be excluded when doing the miRNA analysis. For example, many species have protein coding sequence (CDS) annotations, so there is no need to run miR-PREFeR on those regions. Most plant genomes contain highly repetitive regions such as transposons, these regions should also be excluded. These regions should be listed in a GFF file (<a href=\"http://www.sanger.ac.uk/resources/software/gff/spec.html\">http://www.sanger.ac.uk/resources/software/gff/spec.html</a>). Note that for the gff file, you should not contain introns, because miRNAs could be in introns.</p> <h3>b. Prepare a configuration file for the pipeline.</h3> <p>The miR_PREFeR.py script takes a configuration file as input. The configuration file lists all the information (such as the location of the SAM files, genome fasta file, etc) needed to run the pipeline. An example configuration file with detailed descriptions of each option is in the example data. The options are also explained in the following section.</p> <ol> <li>PIPELINE_PATH: The path of the miR-PREFeR pipeline.</li> <li>FASTA_FILE: The path of the genome sequences in fasta format. Absolute path preferred.</li> <li>ALIGNMENT_FILE: The path of the RNAseq alignment files in SAM format. Absolute path preferred.</li> <li>GFF file option. GFF_FILE_EXCLUDE: The path of the optional GFF file that contains regions you want to exclude from the miRNA analysis. GFF_FILE_INCLUDE: The path of the optional GFF file that contains regions you want to analysis. These two options are mutual exclusive thus only one of them can be specified.</li> <li>PRECURSOR_LEN: The max length of a miRNA precursor. The default is 300</li> <li>READS_DEPTH_CUTOFF: The first step of the pipeline is to identify candidate regions of the miRNA loci. If READS_DEPTH_CUTOFF = N, then genomic position that the mapped depth is smaller than N is not considered. Default value is 10.</li> <li>NUM_OF_CORE: Number of CPUS/Cores avalible for this computation. If commented out or leave blank, 1 is used.</li> <li>OUTFOLDER: Outputfolder. If not specified, use the current working directory. Please make sure that you have enough disk space for the folder, otherwise the pipeline may fail.</li> <li>TMPFOLDER: Absolute path of the folder that contains intermidate/temperary files during the run of the pipeline. If not specified, miR-PREFeR uses a folder with suffix \"_tmp\" under OUTFOLDER by default. Please make sure that you have enough disk space for the folder, otherwise the pipeline may fail.</li> <li>NAME_PREFIX: Prefix for naming the output files. For portability, please DO NOT contain any spaces and special characters. The prefered includes \\'A-Z\\', \\'a-z\\', \\'0-9\\', and underscore \\'_\\'.</li> <li>MAX_GAP: Maximum gap length between two contigs to form a candidate region. Default value is 100.</li> <li>MIN<em>MATURE_LEN and MAX_MATURE</em>LEN: the range of the mature length. The default values are 18 and 24, respectively.</li> <li>ALLOW_NO_STAR_EXPRESSION: If this is \\'Y\\', then the criteria that requries the star sequence must be expressed (that is, there should be at least one read mapped to the star position.) is loosed if the expression pattern is good enough (.e.g. the majority of the reads mapped to the region are mapped to the mature position, and the expression patterns are consistent across different samples). The default value is Y.</li> <li>ALLOW_3NT_OVERHANG: In most cases, the mature star duplex has 2nt 3\\' overhangs. If this is set to \\'Y\\', then 3nt overhangs are allowed. The default value is \\'N\\'.</li> <li>CHECKPOINT_SIZE: The pipeline makes a checkpoint after each major step. In addition, because the folding stage is the most time consuming stage, it makes a checkpiont for each folding process after folding every CHECKPOINT_SIZE sequences. If the pipeline is killed for some reason in the middle of folding, it can be restarted using <code>recover</code> command from where it was stopped. The default value is 3000.</li> </ol> <h3>c. Run the pipeline.</h3> <p>With the configuration file ready, the pipeline can be used to predict miRNAs. The miR-PREFeR pipeline can be run as following:</p> <pre><code>python miR_PREFeR.py [options] command configfile </code></pre> <p>Currently, the following options are available:</p> <ol> <li><strong>-h</strong>: Show help information.</li> <li><strong>-L</strong>: Generate a log file. From the log file you can find the status during the running of the pipeline. <strong>Recommend to always use this option.</strong></li> <li><strong>-k</strong>: After finish the whole pipeline, do not remove the temporary folder that contains the intermediate files. This will save disk space. If it\\'s not specified, you can delete the temporary folder after getting the result.</li> </ol> <p><code>command</code> could be one of <code>check</code>, <code>pipeline</code>, <code>prepare</code>, <code>candidate</code>, <code>fold</code>, <code>predict</code>, and <code>recover</code>. (Run <code>python miR_PREFeR.py -h</code> to see help on options and commands.)</p> <ol> <li><strong>check:</strong> Check the presence of the depended programs (RNALfold and samtools), and check the recovery information (Shows which stage the previous computation on the same data was ceased. See the', '<code>recover</code> command in 6.).</li> <li><strong>pipeline:</strong> Run the whole pipeline. That is, run <code>prepare</code>, <code>candidate</code>,<code>fold</code> and <code>predict</code> sequentially. This is the normal way to run miR-PREFeR.</li> <li><strong>prepare:</strong> Run the first step of the pipeline. This step prepares some data files needed in the following steps.</li> <li><strong>candidate:</strong> Identify possible candidate regions. This step can ONLY be run if the <code>prepare</code> step has been finished on the configfile file.</li> <li><strong>fold:</strong> Fold the candidate regions. This step can ONLY be run if the <code>prepare</code> and <code>candidate</code> steps have been finished on the configfile file.</li> <li><strong>predict:</strong> Predict miRNA loci. This step can ONLY be run if the <code>prepare</code>, <code>candidate</code> and <code>fold</code> steps have been finished on the configfile file.</li> <li><p><strong>recover:</strong> Tries its best to recover an unfinished job and continues to run from the unfinished step it was ceased. This is designed to easily continue a job other than re-run the whole pipeline from start. For example, if a job was kill half way (This happens sometime. For example, the job takes too long time and the user killed it. Or, if the job was run on an cluster and was killed halfway because some resources exceed the max values.) In these cases, one does not need to run the pipeline from start, the job can be continued by using the <code>recover</code> command:</p> <p><code>python miR_PREFeR.py -L recover configfile</code></p></li> </ol> <p><strong>Tips for running the pipeline:</strong> If you run the pipeline once and change the following configuration parameters and want to run it again, you do not need to run the pipeline from the start. You can just run the <code>predict</code> command. This saves you lots of time. These configuration parameters include: MIN_MATURE_LEN, MAX_MATURE_LEN, ALLOW_NO_STAR_EXPRESSION and ALLOW_3NT_OVERHANG.</p> <p><strong>NOTE on job recovery:</strong> To recover and continue a job, the intermediate output files in the temporary folder (See the Output section) should not be removed. All files needed to do recovery are in this folder.</p> <h2>4. Output</h2> <p>The miR-PREFeR pipeline produces several files which contains information about the predicted miRNAs:</p> <ol> <li>An HTML file that contains a table which summarizes the results of the miRNA predictions by miR-PREFeR. It contains the number of predictions, the length distribution of the predicted mature miRNA sequences, the list of all pre-miRNA sequences and their secondary structures, and the detailed read mapping profiles against the pre-miRNA sequences. In addition, this table provides search links to mirBase. An example output HTML page can be found at <a href=\"http://goo.gl/YQSCP8\">http://goo.gl/YQSCP8</a>.</li> <li>A csv file that contains detailed information about each prediction. The information in the table is the same as in the HTML file. This is provided for easier downstream processing/analysis.</li> <li>A list of read mapping result files in the folder <code>readmapping</code>. Each file corresponds to an miRNA and shows the reads mapped to the precursor region.</li> <li>A gff3 format annotation file that contains the predicted miRNAs. The last column for each precursor contains information about whether the star sequence is expressed in the samples (star_expressed=y/n) and the overhang sizes (overhangsize=2:2).</li> <li>Two fasta files that contain the mature sequences and the precursor sequences of the predicted miRNAs.</li> <li>A <code>.ss</code> file that contains the sequence and the predicted structure of each miRNA.</li> <li>A file named <code>miRNA.stat.txt</code> that shows the basic statistics of the mature sequences.</li> </ol> <p>An example output folder that contains all the files can be found at <a href=\"http://goo.gl/f4Jb2p\">http://goo.gl/f4Jb2p</a></p> <p>During the running of the pipeline, there is a folder named NAME_PREFIX + \"_tmp\" in the output folder (The temporary folder could be at another place if you specified the TMPFOLDER option in the config file). The folder contains intermediate files generated by the pipeline. If the job is finished without interruption, the folder is deleted by default (Specify the <code>-k</code> option can keep the folder, but this is usually not needed.). <strong>If the job is interrupted half way, then the folder is not removed by default, so that one can run the job in the <code>recover</code> mode to continue the computation. (Because all files needed to do recovery are in this temp folder, so if one wants to recover an interrupted job, please do not remove the folder. The folder will be removed automatically once the whole pipeline is successfully run.).</strong></p> <h3>5. Conducting alignment using Bowtie.</h3> <p>a1. indexing the genome sequences:</p> <pre><code>bowtie-build -q -f TAIR10.fas bowtie-index/TAIR10 </code></pre> <p>a2. conduct the alignment. Here are example bowtie commands for the same files processed in the previous step:</p> <pre><code>bowtie -a -v 0 -p 8 -m 30 -S bowtie-index/TAIR10 -f SAMPLE1.fasta.processed &gt; SAMPLE1.sam 2&gt; SMAPLE1.log bowtie -a -v 0 -p 8 -m 30 -S bowtie-index/TAIR10 -f SAMPLE2.fasta.processed &gt; SAMPLE2.sam 2&gt; SMAPLE2.log bowtie -a -v 0 -p 8 -m 30 -S bowtie-index/TAIR10 -f SAMPLE3.fasta.processed &gt; SAMPLE3.sam 2&gt; SMAPLE3.log </code></pre> <p>The -v 0 option allows no mismatch in the alignment. The -m 10 option discards reads that can be mapped to more than 30 positions. The -p 8 option uses 8 threads to do the alignment. More options of the parameters can be found at <a href=\"http://bowtie-bio.sourceforge.net/index.shtml\">http://bowtie-bio.sourceforge.net/index.shtml</a>.</p> <p><strong>Requirement for the SAM file format:</strong></p> <p>The pipeline requires the input SAM alignment files:</p> <p>a. The SAM files <strong>MUST have headers</strong>. This is a example header:</p> <pre><code>@HD VN:1.0 SO:unsorted @SQ SN:Chr1 LN:30427671 @SQ SN:Chr2 LN:19698289 @SQ SN:Chr3 LN:23459830 @SQ SN:Chr4 LN:18585056 @SQ SN:Chr5 LN:26975502 @PG ID:Bowtie VN:0.12.8 CL:\"bowtie -a -v 1 -p 10 -S bowtie-index/TAIR10 -f ../SAMPLE1.fasta.processed\" </code></pre> <p>You can use \"<code>samtools view -H yoursamfile</code>\" to check whether your SAM file has the header section. Bowtie produces headers for SAM format alignment files by default.</p> <p>b. The flag column (the second column of the alignment section) of the SAM file <strong>MUST be in a integer.</strong>. The flag generated by Bowtie/or generated by using \"<code>samtools view</code>\" command, by default, is an integer. Samtools provides options to output SAMfiles with Hex (-x option) or string (-X option) format flags. If your SAM file uses these two types of flags, you either need to generate your SAM file again, or convert it to the required format.</p> <p>c. The read identifier (the first column of the alignment section) must be in the required format as described in the fasta file preparation section. The <code>process-reads-fasta.py</code> and <code>convert-mirdeep2-fasta.py</code> scripts can help you generate fasta files of the right format.</p> <p>The miR-PREFeR pipeline checks the SAM format and if the format is not correct, it prints messages about which format problems it encountered and stops analysis.</p> </body> </html>']```"}, {"role": "user", "content": "Content from https://github.com/hangelwen/miR-PREFeR:\n```\nRepository metadata:\n{'name': 'miR-PREFeR', 'label': ['miR-PREFeR'], 'description': [{'term': 'microRNA PREdiction From small RNA-seq data', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.24', 'v0.23', 'v0.21'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/hangelwen/miR-PREFeR', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Jack Lei', 'type': 'person', 'email': 'hangelwen@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Jikai Lei', 'type': 'person', 'email': 'jilei@paypal.com', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'faq', 'url': 'https://github.com/hangelwen/miR-PREFeR/blob/main/FAQ.txt'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/hangelwen/miR-PREFeR/blob/main/GPL.txt'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/hangelwen/miR-PREFeR/blob/main/HOW_TO_RUN_EXAMPLE.txt'}, 'id': 2}, {'term': {'type': 'readme', 'url': 'https://github.com/hangelwen/miR-PREFeR/blob/main/README.md'}, 'id': 3}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"fraggenescan/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/fraggenescan/cmd/1.31\",\n    \"name\": \"fraggenescan\",\n    \"description\": [\n      \"FragGeneScan is an application for finding (fragmented) genes in short\\nreads. It can also be applied to predict prokaryotic genes in incomplete\\nassemblies or complete genomes.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://sourceforge.net/projects/fraggenescan/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"BSD\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c79fd0d3b8acd1a2f9b45f\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/fraggenescan/cmd/1.30.0\",\n    \"name\": \"fraggenescan\",\n    \"description\": [\n      \"For finding (fragmented) genes in short reads.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Ffraggenescan%2Ffraggenescan%2F1.30.0\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/fraggenescan/:\n```\nProject metadata:\n{'description': 'FragGeneScan is an application for finding (fragmented) genes in short reads. It can also be applied to predict prokaryotic genes in incomplete assemblies or complete genomes.\\nFragGeneScan was first released through omics website (\\nhttp://omics.informatics.indiana.edu/FragGeneScan/\\n) in March 2010, where you can find its old releases. FragGeneScan migrated to SourceForge in October, 2013.', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['/p/fraggenescan/activity/feed', 'https://sourceforge.net/auth/?return_to=%2Fprojects%2Ffraggenescan%2F', '/p/fraggenescan/activity/']}, {'text': 'Features\\nFragGeneScan can be used for gene prediction in complete genomes, assemblies, and short reads.\\nPlug and use -- no need to train specific models for different datasets\\nFragGeneScan handles sequencing errors.\\nFeatures\\nFragGeneScan can be used for gene prediction in complete genomes, assemblies, and short reads.\\nPlug and use -- no need to train specific models for different datasets\\nFragGeneScan handles sequencing errors.', 'hrefs': []}, {'text': 'Categories\\nBio-Informatics\\nCategories\\nBio-Informatics', 'hrefs': ['/directory/bio-informatics/']}, {'text': 'Follow FragGeneScan\\nFragGeneScan Web Site\\nFollow FragGeneScan\\nFragGeneScan Web Site', 'hrefs': ['https://sourceforge.net/projects/fraggenescan/']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof FragGeneScan!\\nUser Reviews\\nBe the first to\\npost a review\\nof FragGeneScan!', 'hrefs': ['/projects/fraggenescan/reviews/new']}, {'text': 'Additional Project Details\\nIntended Audience\\nInformation Technology, Healthcare Industry\\nUser Interface\\nConsole/Terminal\\nProgramming Language\\nPerl\\n,\\nC\\nRelated Categories\\nPerl Bio-Informatics Software\\n,\\nC Bio-Informatics Software\\nRegistered\\n2013-10-22\\nAdditional Project Details\\nIntended Audience\\nInformation Technology, Healthcare Industry\\nUser Interface\\nConsole/Terminal\\nProgramming Language\\nPerl\\n,\\nC\\nRelated Categories\\nPerl Bio-Informatics Software\\n,\\nC Bio-Informatics Software\\nRegistered\\n2013-10-22', 'hrefs': ['/directory/c/', '/directory/bio-informatics/c/', '/directory/perl/', '/directory/bio-informatics/perl/']}]}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"ale/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/ale/cmd/20180904\",\n    \"name\": \"ale\",\n    \"description\": [\n      \"ALE: Assembly Likelihood Estimator.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/sc932/ALE\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/sc932/ALE\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"NCSA\",\n        \"url\": \"https://spdx.org/licenses/NCSA.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/ale/cmd/None\",\n    \"name\": \"ale\",\n    \"description\": [\n      \"Automated label extraction from GEO metadata.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/wrenlab/label-extraction\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Jonathan D. Wren\",\n        \"email\": \"jonathan-wren@omrf.org\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Jonathan D. Wren\",\n        \"email\": \"jdwren@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a9b6d3b8acd1a2f9bb26\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/wrenlab/label-extraction:\n```\nREADME content:\n[\"## ALE CLI - Automated Label Extraction Command Line Interface Tools to extract and validate labels using wrenlab's ALE. Gold standard for validation is Wrenlab's JDW manual annotations. ## REQUIREMENTS - [wrenlab](https://gitlab.com/wrenlab/wrenlab) - [metalearn](https://gitlab.com/wrenlab/metalearn) - [click](https://pypi.python.org/pypi/click) ## INSTALL python setup.py develop --user ## USAGE The following scripts should be in your $PATH and accessible from your shell ``` ale ale-validation ``` ale extracts labels from GEO. ale-validation evaluates performance against the Wrenlab gold standard for GEO data for human data (NCBI Taxon ID 9606).\"]```"}, {"role": "user", "content": "Content from https://github.com/wrenlab/label-extraction:\n```\nRepository metadata:\n{'name': 'label-extraction', 'label': ['label-extraction'], 'description': [{'term': 'ALE Label Extraction Data and CLI Tools', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [], 'repository': [{'term': 'https://github.com/wrenlab/label-extraction', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Cory Giles', 'type': 'person', 'email': 'mail@corygil.es', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Xiavan', 'type': 'person', 'email': 'xiavan-roopnarinesingh@omrf.org', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/wrenlab/label-extraction/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/sc932/ALE:\n```\nREADME content:\n['DOCUMENTATION ------------- http://portal.nersc.gov/dna/RD/Adv-Seq/ALE-doc/ This code is no longer in active development. Pull requests and bugs will be addressed as needed. Plotting -------- The authors recommend using IGV to view the output. http://www.broadinstitute.org/igv/ Just import the assembly, bam and ALE scores. You can convert the .ale file to a set of .wig files with ale2wiggle.py and IGV can read those directly. Depending on your genome size you may want to convert the .wig files to the BigWig format. LICENSE ------- /* * Copyright (C) 2010,2011,2012 Scott Clark. All rights reserved. * * Developed by: * Scott Clark * Cornell University Center for Applied Mathematics * http://cam.cornell.edu * AND * Rob Egan * Department of Energy Joint Genome Institute * http://jgi.doe.gov * * Permission is hereby granted, free of charge, to any person obtaining a * copy of this software and associated documentation files (the \"Software\"), * to deal with the Software without restriction, including without limitation * the rights to use, copy, modify, merge, publish, distribute, sublicense, * and/or sell copies of the Software, and to permit persons to whom the * Software is furnished to do so, subject to the following conditions: * * 1. Redistributions of source code must retain the above copyright notice, * this list of conditions and the following disclaimers. * 2. Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimers in the * documentation and/or other materials provided with the distribution. * 3. Neither the names of Cornell University, The Joint Genome Institute, * nor the names of its contributors may be used to endorse or promote * products derived from this Software without specific prior written * permission. * * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE * CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER * DEALINGS WITH THE SOFTWARE. */ // For more information on the licence please see // The University of Illinois/NCSA Open Source License // http://www.opensource.org/licenses/UoI-NCSA.php INSTALL ------- http://portal.nersc.gov/dna/RD/Adv-Seq/ALE-doc/index.html#installation # # ALE comes packaged with samtools-0.1.18 Feel free to use a later version # samtools is open source (MIT license) # We would like to thank the creators for the extensive and very useful package # Requirements for plotter4.py: numpy 1.6.1 mpmath 0.17 setuptools 0.6c11 matplotlib 1.0.1 pymix 0.8 http://www.pymix.org/pymix/']```"}, {"role": "user", "content": "Content from https://github.com/sc932/ALE:\n```\nRepository metadata:\n{'name': 'ALE', 'label': ['ALE'], 'description': [{'term': 'Assembly Likelihood Estimator', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/sc932/ALE', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'rsegan@lbl.gov', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Eric  DEVEAUD', 'type': 'person', 'email': 'edeveaud@pasteur.fr', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Alexander Regueiro', 'type': 'person', 'email': 'alexreg@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'RSEgan@lbl.gov', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'github-rob@egannetworks.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'holmrenser', 'type': 'person', 'email': 'rens.holmer@wur.nl', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Scott Clark', 'type': 'person', 'email': 'scott@scottclark.io', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'genepool1.nersc.gov', 'maintainer': False}, 'id': 7}, {'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'genepool02.nersc.gov', 'maintainer': False}, 'id': 8}, {'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'regan@phoebe.nersc.gov', 'maintainer': False}, 'id': 9}, {'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'regan@genepool04.nersc.gov', 'maintainer': False}, 'id': 10}, {'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'regan@dakar.jgi-psf.org', 'maintainer': False}, 'id': 11}, {'term': {'name': 'Rob Egan', 'type': 'person', 'email': 'regan@regan2.local', 'maintainer': False}, 'id': 12}, {'term': {'name': 'Scott Clark', 'type': 'person', 'email': 'sc932@cornell.edu', 'maintainer': False}, 'id': 13}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"pilon/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/pilon/cmd/1.24\",\n    \"name\": \"pilon\",\n    \"description\": [\n      \"Pilon is an automated genome assembly improvement and variant detection tool.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/broadinstitute/pilon\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/broadinstitute/pilon/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-2.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-2.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/pilon/cmd/1.20.1\",\n    \"name\": \"pilon\",\n    \"description\": [\n      \"An automated genome assembly improvement and variant detection tool.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fpilon%2Fpilon%2F1.20.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/broadinstitute/pilon:\n```\nREADME content:\n['# Pilon For further information, please visit the [Wiki](https://github.com/broadinstitute/pilon/wiki)']```"}, {"role": "user", "content": "Content from https://github.com/broadinstitute/pilon:\n```\nRepository metadata:\n{'name': 'pilon', 'label': ['pilon'], 'description': [{'term': 'Pilon is an automated genome assembly improvement and variant detection tool', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.24', 'v1.23', 'v1.22', 'v1.21', 'v1.20', 'v1.19', 'v1.18', 'v1.17', 'v1.16', 'v1.15', 'v1.14', 'v1.13', 'v1.12', 'v1.11', 'v1.10', 'v1.9', 'v1.8', 'v1.7', 'v1.6', 'v1.5', 'v1.4', 'v1.3', 'v1.2', 'v1.1', 'v1.0', 'v0.9'], 'license': [{'term': {'name': 'GNU General Public License v2.0', 'url': 'http://choosealicense.com/licenses/gpl-2.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/broadinstitute/pilon', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Bruce Walker', 'type': 'person', 'email': 'bruce@quidnet.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'bruce', 'type': 'person', 'email': 'bw@ai.co', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Bruce Walker', 'type': 'person', 'email': 'bruce@w1bw.us', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Bruce Walker', 'type': 'person', 'email': 'bruce@appliedinvention.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Brad Langhorst', 'type': 'person', 'email': 'langhorst@neb.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Bruce Walker', 'type': 'person', 'email': 'bruce@broadinstitute.org', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/broadinstitute/pilon/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/broadinstitute/pilon/:\n```\nREADME content:\n['# Pilon For further information, please visit the [Wiki](https://github.com/broadinstitute/pilon/wiki)']```"}, {"role": "user", "content": "Content from https://github.com/broadinstitute/pilon/:\n```\nRepository metadata:\n{'name': 'pilon', 'label': ['pilon'], 'description': [{'term': 'Pilon is an automated genome assembly improvement and variant detection tool', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.24', 'v1.23', 'v1.22', 'v1.21', 'v1.20', 'v1.19', 'v1.18', 'v1.17', 'v1.16', 'v1.15', 'v1.14', 'v1.13', 'v1.12', 'v1.11', 'v1.10', 'v1.9', 'v1.8', 'v1.7', 'v1.6', 'v1.5', 'v1.4', 'v1.3', 'v1.2', 'v1.1', 'v1.0', 'v0.9'], 'license': [{'term': {'name': 'GNU General Public License v2.0', 'url': 'http://choosealicense.com/licenses/gpl-2.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/broadinstitute/pilon', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Bruce Walker', 'type': 'person', 'email': 'bruce@quidnet.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'bruce', 'type': 'person', 'email': 'bw@ai.co', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Bruce Walker', 'type': 'person', 'email': 'bruce@w1bw.us', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Bruce Walker', 'type': 'person', 'email': 'bruce@appliedinvention.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Brad Langhorst', 'type': 'person', 'email': 'langhorst@neb.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Bruce Walker', 'type': 'person', 'email': 'bruce@broadinstitute.org', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/broadinstitute/pilon/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"piranha/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/piranha/cmd/1.2.1\",\n    \"name\": \"piranha\",\n    \"description\": [\n      \"Piranha is a peak-caller for CLIP- and RIP-Seq data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://smithlabresearch.org/software/piranha/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/piranha/cmd/0.1.0\",\n    \"name\": \"piranha\",\n    \"description\": [\n      \"Peak-caller for CLIP- and RIP-Seq data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Frnateam%2Fpiranha%2Fpiranha%2F0.1.0\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://smithlabresearch.org/software/piranha/:\n```\nContent:\n['Piranha is a peak-caller for CLIP- and RIP-Seq data. It takes input in BED or BAM format and identifies regions of statistically significant read enrichment. Additional covariates may optionally be provided to further inform the peak-calling process. ### **Download** The latest release of Piranha is [version 1.2.1](https://smithlabresearch.org/downloads/piranha-1.2.1.tar.gz) . This version introduces support for correctly identifying the strand each peak is on. The link to the latest development branch can be found in this [Github repo.](https://github.com/smithlabcode/piranha) ### **System requirements** 64-bit machine and GCC version \u2265 4.1 (to support TR1). Piranha requires a functioning installation of the GNU Scientific Library (GSL). If you don\u2019t already have this installed, you will need to download and install it from [here](http://www.gnu.org/software/gsl/) . This distribution has been tested with GSL version 1.14 Your installation of GSL must be built for a 64 bit architecture. Piranha uses BAMTools to provide support for BAM files. If you require BAM support, you must download and install [BAMTools](https://github.com/pezmaster31/bamtools/) . If you don\u2019t need BAM support, Piranha can be built without it (this is the default actually). The regression tests (which you don\u2019t need to run) require Python. ### **Install** To install Piranha, download the compressed archive, unpack it similar to: > tar -xf piranha-X.Y.Z.tar.gz To configure the installation, type the following, where \u2018>\u2019 is your prompt and the CWD is the root of the distribution: > ./configure To build the binaries, type: > make all To install the binaries, type the following, where \u2018>\u2019 is your prompt and the CWD is the root of the distribution: > make install This will place the binaries in the bin directory under the package root. They can be used directly from there without any additional steps. You can add that directory to your PATH environment variable to avoid having to specify their full paths, or you can copy the binaries to another directory of your choice in your PATH. You can verify that Piranha was built correctly by running the included unit and regression tests. At the command prompt (assuming your prompt is \u2018>\u2019) type the following: > make test ### **Basic usage of Piranha** Piranha has two main modes of operation. In the first, a single regular distribution is fit to the input data and each region is assigned a p-value based on this distribution. The default distribution is the zero-truncated negative binomial. To run Piranha in this way, use the following, where > is your prompt: > ./Piranha input.bed The second mode of operation fits a regression model relating counts to covariates. The default is a zero-truncated negative binomial regression. To run Piranha in this way, do the following, where > is your prompt: > ./Piranha input.bed covariate1.bed covariate2.bed Note that the first file is always assumed to be the counts, and the remaining files are assumed to be covariates. For further details on using Piranha, see the README file included with the distribution.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"mango/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/mango/cmd/None\",\n    \"name\": \"mango\",\n    \"description\": [\n      \"Complete ChIA-PET (Chromatin Interaction Analysis by Paired-End Tag sequencing) data analysis pipeline that provides statistical confidence estimates for interactions and corrects for major sources of bias including differential peak enrichment and genomic proximity.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/dphansti/mango\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"PHANSTIEL LAB\",\n        \"email\": \"dphansti@email.unc.edu\",\n        \"maintainer\": false,\n        \"url\": \"https://github.com/dphansti/mango\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c828d110e2f3aa0b3b7f9a\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/mango/cmd/0.0.5\",\n    \"name\": \"mango\",\n    \"description\": [\n      \"A scalable genomic visualization tool.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/bdgenomics/mango\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/bdgenomics/mango\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Apache-2.0\",\n        \"url\": \"https://spdx.org/licenses/Apache-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/dphansti/mango:\n```\nREADME content:\n[\"Mango ===== ChIA-PET Analysis Software Citation ======== Phanstiel DH, Boyle AP, Heidari N, Snyder MP. Mango: a bias-correcting ChIA-PET analysis pipeline. Bioinformatics. 2015 Oct 1;31(19):3092-8. doi:10.1093/bioinformatics/btv336. Epub 2015 Jun 1. PubMed PMID: 26034063; PubMed Central PMCID: PMC4592333. Mango Installation ================== 1. Mango depends on the following R packages. a) hash b) Rcpp c) optparse d) readr They can be installed throug CRAN. For example to install the package 'hash' open R and type the following :: install.packages('hash') install.packages('Rcpp') install.packages('optparse') install.packages('readr') 2. Mango depends on the following software pacakges which should be installed and included in the system PATH prior to using Mango. a) Bowtie (http://bowtie-bio.sourceforge.net) b) Bedtools >= 2.20.0 (https://github.com/arq5x/bedtools2) c) MACS2 (https://github.com/taoliu/MACS) 3. Once dependencies are installed Mango can be installed from the command line using the following command. :: git clone https://github.com/dphansti/mango.git R CMD INSTALL --no-multiarch --with-keep.source mango If installing on cluster where you have limited permissions you may need to setup a local library. An example is shown below. :: git clone https://github.com/dphansti/mango.git mkdir ~/R mkdir ~/R/library export R_LIBS=$R_LIBS:~/R/library R CMD INSTALL --no-multiarch --with-keep.source mango --library=~/R/library We recommend adding the $R_LIBS:~/R/library command to your bash_profile as well. Features ======== Mango uses fastq files generated by illumina sequencers to call peaks and interactions from ChIA-PET experiments. Arguments can be passed to Mango either by a configuration file, through the command line, or a combination of both. In cases where arguments at supplied both through the command line and a configuration file the values passed via command line arguments will take precidence. Usage of Mango ============== :: Rscript mango.R [-options] Example for regular interactions calling :: Rscript Mango.R --fastq1 samplename_1.fastq --fastq2 samplename_1.fastq --prefix samplename --argsfile argsfile.txt --chromexclude chrM,chrY --stages 1:5 Example of a argsfile :: bowtieref = /path/to/hg19 bedtoolsgenome = /path/to/human.hg19.genome !! Note if using tagmentation-generated libraries !! ============== If the libraries were generated using tagmentation instead of MmeI digestions and adapter ligation we recommend the following settings: :: --keepempty TRUE --shortreads FALSE --maxlength 1000 Parameters ---------- ALL STAGES ~~~~~~~~~~ ``argsfile`` The full path to a file containing any of the following parameters. See above for example. ``stages`` stages of the pipeline to execute. stage can be either a single stage (e.g 1 or a range of stagnes e.g 1:5). default = 1:5 ``prefix`` prefix for all output files. default = mango ``outdir`` The output direcoroy. default = NULL ``bowtieref`` genome reference file for bowtie ``bedtoolsgenome`` bedtools genome file ``chrominclude`` comma separated list of chromosomes to use (e.g. chr1,chr2,chr3,...). Only these chromosomes will be processed. If NULL all chromosomes with be processed. default = NULL ``chromexclude`` comma separated list of chromosomes to exclude (e.g. chrM,chrY). If NULL all chromosomes with be processed. !!chrM should always be excluded due to its extremely short length!! default = NULL ``bedtoolspath`` full path to bedtools (only required if not found in system PATH). default = NULL ``macs2path`` full path to macs2 (only required if not found in system PATH). default = NULL ``bowtiepath`` full path to bowtie (only required if not found in system PATH). default = NULL STAGE 1 PARAMETERS ~~~~~~~~~~ ``linkerA`` linker sequence to look for. default = GTTGGATAAG ``linkerB`` linker sequence to look for. default = GTTGGAATGT ``singlelinker`` Was only a single linker used? If TRUE Mango will only look for linkerA. LinkerB will be ignored. default = FALSE ``minlength`` min length of reads after linker trimming. default = 15 ``maxlength`` max length of reads after linker trimming. If libraries were generated via tagmentation this should be set to a value greater than the read length (i.e. 1000). default = 25 ``keepempty`` Should reads with no linker be kept (TRUE or FALSE). If libraries were generated via tagmentation this should be set to TRUE. default = FALSE STAGE 2 PARAMETERS ~~~~~~~~~~ ``shortreads`` should bowtie alignments be done using paramter for very short reads (~20 bp). If libraries were generated via tagmentation this should be set to FALSE. default = TRUE ``threads`` number of threads to be used for bowtie alignment. default = 1 (!! This option is currently disabled to due to errors. We are working on a solution !!) STAGE 2 PARAMETERS ~~~~~~~~~~ ``npets4dist`` the number of PETS to use to plot PET distance distribution. default = 1000000 (use -1 for all PETS). STAGE 4 PARAMETERS ~~~~~~~~~~ ``MACS_qvalue`` pvalue cutoff for peak calling in MACS2. default = 0.05 ``MACS_shiftsize`` MACS shiftize. NULL allows MACS to determine it ``peakslop`` Number of basespairs to extend peaks on both sides. default = 500 ``peakinput`` Name of user supplied peaks file. If NULL Mango will use peaks determined from MACS2 analysis. default = NULL ``blacklist`` BED file of regions to remove from MACS peaks ``gzize`` mappable genome size or effective genome size for MACS2.default = 'hs' STAGE 5 PARAMETERS ~~~~~~~~~~ ``distcutrangemin`` When Mango determines the self-ligation cutoff this is the minimum distance it will consider. Changing this setting is not recommended. default = 1000 ``distcutrangemax`` When Mango determines the self-ligation cutoff this is the maximum distance it will consider. Changing this setting is not recommended. default = 100000 ``biascut`` Mango exlcudes very short distance PETS since they tend to arise from self-ligation of a single DNA framgent as opposed to interligation of two interacting fragments. To determine this distnce cutoff Mango determines the fraction of PETs at each distance that come from self-ligation and sets the cutoff at the point where the fraction is less than or equal to BIASCUT. default = 0.05 ``FDR`` FDR cutoff for significant interactions. default = 0.05 ``numofbins`` number of bins to use for binomial p-value calculations. default = 50 ``corrMethod`` Method to use for correction of mulitply hypothesis testing. See (http://stat.ethz.ch/R-manual/R-devel/library/stats/html/p.adjust.html) for more details. default = BH ``maxinteractingdist`` The maximum disance (in basepairs) considered for interaction. Optimum sensitivity is generally acheived at values of 1000000-2000000. default = 1000000 ``extendreads`` how many bp to extend reads towards peak. default = 120 ``minPETS`` The minimum number of PETs required for an interaction (applied after FDR filtering). default = 2 ``reportallpairs`` Should all pairs be reported or just significant pairs (TRUE or FALSE). default = FALSE Intermediate Files ============ ``...same.fastq`` These files contain fastq formated sequences after linkers have been detected and removed. Only pairs of reads with the same linker sequences on both ends of the PET are reported in these files. These are the only reads used for subsequent steps. ``...chim.fastq`` These files contain fastq formated sequences after linkers have been detected and removed. Only pairs of reads with the different linker sequences on both ends of the PET are reported in these files. These reads are NOT used for subsequent steps. ``...bedpe`` This file desribes all aligned PETs. The columns are (chromosome1, start1, end1, chromosome2, start2, end2, readname, score, strand1, strand2) ``...rmdup.bedpe`` This file desribes all aligned PETs after removal of duplicate PETs. The columns are (chromosome1, start1, end1, chromosome2, start2, end2, readname, score, strand1, strand2) ``...tagAlign`` This file desribes all reads (PETs are split into two lines in this file) in standard tagAlign format. The columns are (chromosome, start, end, readname, score, strand) ``...slopPeak`` This file desribes peaks after peak calling, addition of a user defined number of basepairs (peakslop), and merging of overlapping peaks. The columns are (chromosome, start, end, peakname). Output Files ============ ``...interactions.fdr.mango`` This files contains all significant interactions. The columns are (chromosome1, start1, end1, chromosome2, start2, end2, PETs supporting the interaction, the adjusted P-value of the interaction)* ``...interactions.all.mango`` This files contains all tested interactions and is only generated if 'reportallpairs' is TRUE. The columns are the same as those for the file above. *More columns with column headers will be output if 'verboseoutput' is TRUE.\"]```"}, {"role": "user", "content": "Content from https://github.com/dphansti/mango:\n```\nRepository metadata:\n{'name': 'mango', 'label': ['mango'], 'description': [{'term': 'chia pet analysis software', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [], 'repository': [{'term': 'https://github.com/dphansti/mango', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Doug Phanstiel', 'type': 'person', 'email': 'dphansti@stanford.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'dphansti', 'type': 'person', 'email': 'douglas_phanstiel@med.unc.edu', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Lila14', 'type': 'person', 'email': 'lr65358@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'LILA HERK RIEBER', 'type': 'person', 'email': 'lur159@E0502303.bmb.psu.edu', 'maintainer': False}, 'id': 3}, {'term': {'name': 'DOUGLAS H PHANSTIEL', 'type': 'person', 'email': 'dphansti@DOUGLASs-MacBook-Pro-2.local', 'maintainer': False}, 'id': 4}, {'term': {'name': 'DOUGLAS H PHANSTIEL', 'type': 'person', 'email': 'dphansti@DN52em37.SUNet', 'maintainer': False}, 'id': 5}, {'term': {'name': 'dphansti', 'type': 'person', 'email': 'dphansti@DN2m1jekp.SUNet', 'maintainer': False}, 'id': 6}, {'term': {'name': 'DOUGLAS H PHANSTIEL', 'type': 'person', 'email': 'dphansti@DOUGLASs-MacBook-Pro.local', 'maintainer': False}, 'id': 7}, {'term': {'name': 'DOUGLAS H PHANSTIEL', 'type': 'person', 'email': 'dphansti@DN2m1jegj.SUNet', 'maintainer': False}, 'id': 8}, {'term': {'name': 'Alan Boyle', 'type': 'person', 'email': 'aboyle@stanford.edu', 'maintainer': False}, 'id': 9}, {'term': {'name': 'Alan Boyle', 'type': 'person', 'email': 'aboyle@gmail.com', 'maintainer': False}, 'id': 10}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/dphansti/mango/blob/main/argfile.txt'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bamhash/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/bamhash/cmd/None\",\n    \"name\": \"bamhash\",\n    \"description\": [\n      \"Checksum program for verifying the integrity of sequence data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/DecodeGenetics/BamHash\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"pmelsted@hi.is\",\n        \"maintainer\": false,\n        \"url\": \"http://www.decode.com/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a9c6d3b8acd1a2f9bb45\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/bamhash/cmd/1.1\",\n    \"name\": \"bamhash\",\n    \"description\": [\n      \"Hash BAM and FASTQ files to verify data integrity.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fbgruening%2Fbamhash%2Fbamhash%2F1.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/DecodeGenetics/BamHash:\n```\nREADME content:\n['# BamHash Hash BAM and FASTQ files to verify data integrity For each pair of reads in a BAM or FASTQ file we compute a hash value composed of the readname, whether it is first or last in pair, sequence and quality value. All the hash values are summed up so the result is independent of the ordering within the files. The result can be compared to verify that the pair of FASTQ files contain the same read information as the aligned BAM file. The program is written in C++ and uses SeqAnHTS v1.0 for parsing FASTQ, gzip compressed FASTQ and BAM files. SeqAnHTS is a fork of SeqAn library ( D\u00f6ring etal. , 2008 ) that uses htslib to read SAM/BAM/CRAM files. ## Manuscript Arna \u00d3skarsd\u00f3ttir, G\u00edsli M\u00e1sson and P\u00e1ll Melsted (2015) BamHash: a checksum program for verifying the integrity of sequence data. [Bioinformatics, btv539](http://bioinformatics.oxfordjournals.org/content/early/2015/10/01/bioinformatics.btv539.abstract). A preprint is available on [bioRxiv](http://biorxiv.org/content/early/2015/03/03/015867). ## Usage The program has three executables which are used for different filetypes. Running them with `--help` displays detailed help messages. ### Common options All programs work with sets of reads. The reads are made up of a read name, sequence and quality information. All of these components go into the hash, but the read name or quality information can be ignored if necessary. This would be the case if a pipeline mangled the names, quantizised the quality or after realigning quality scores. The default mode is to assume paired end reads. If you have single end reads you can supply the `--no-paired` option. A debug option `-d` prints the information and hash value of each read individually, this can be helpful if BamHash is not cooperating with your pipeline. Both multiline FASTA and FASTQ are supported and gzipped input for FASTA and FASTQ. ### BAM ~~~ bamhash_checksum_bam [OPTIONS] <in.bam> <in2.bam> ... bamhash_checksum_bam [OPTIONS] -r <reference-file> <in.cram> ~~~ processes a number of BAM files. BAM files are assumed to contain paired end reads. If you run with `--no-paired` it treats all reads as single end and displays a warning if any read is marked as \"second in pair\" in the BAM file. ### FASTQ ~~~ bamhash_checksum_fastq [OPTIONS] <in1.fastq.gz> [in2.fastq.gz ... ] ~~~ processes a number of FASTQ files. FASTQ files are assumed to contain paired end reads, such that the first two files contain the first pair of reads, etc. If any of the read names in the two pairs don\\'t match the program exits with failure. ### FASTA ~~~ bamhash_checksum_fasta [OPTIONS] <in1.fasta> [in2.fasta ... ] ~~~ processes a number of FASTA files. All FASTA files are assumed to be single end reads with no quality information. To compare to a BAM file, run `bamhash_checksum_bam --no-paired --no-quality` ## Compiling External dependencies are on: OpenSSL for the MD5 implementation htslib library (version 1.9)']```"}, {"role": "user", "content": "Content from https://github.com/DecodeGenetics/BamHash:\n```\nRepository metadata:\n{'name': 'BamHash', 'label': ['BamHash'], 'description': [], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v2.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/DecodeGenetics/BamHash', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Birte Kehr', 'type': 'person', 'email': 'birte.kehr@bihealth.de', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Pall Melsted', 'type': 'person', 'email': 'pmelsted@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'arnaos', 'type': 'person', 'email': 'arnaos@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Arna \u00d3skarsd\u00f3ttir', 'type': 'person', 'email': 'arnaos@decode.is', 'maintainer': False}, 'id': 3}, {'term': {'name': 'bkehr', 'type': 'person', 'email': 'bkehr@users.noreply.github.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Chris Cole', 'type': 'person', 'email': 'c.cole@dundee.ac.uk', 'maintainer': False}, 'id': 5}, {'term': {'name': 'dpryan79', 'type': 'person', 'email': 'dpryan@dpryan.com', 'maintainer': False}, 'id': 6}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/DecodeGenetics/BamHash/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/DecodeGenetics/BamHash/blob/main/astyle.txt'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"transtermhp/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/transtermhp/cmd/2.09\",\n    \"name\": \"transtermhp\",\n    \"description\": [\n      \"TransTermHP finds rho-independent transcription terminators in bacterial genomes.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://transterm.cbcb.umd.edu/index.php\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/transtermhp/cmd/2.09.1\",\n    \"name\": \"transtermhp\",\n    \"description\": [\n      \"Finds rho-independent transcription terminators in bacterial genomes.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Ftranstermhp%2Ftranstermhp%2F2.09.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://transterm.cbcb.umd.edu/index.php:\n```\nContent:\n['**TransTermHP** finds rho-independent transcription terminators in bacterial genomes. Each terminator found by the program is assigned a confidence value that estimates its probability of being a true terminator. TransTermHP is described in: [C. Kingsford](http://www.cbcb.umd.edu/~carlk/) , [K. Ayanbule](http://www.cbcb.umd.edu/~ayanbule/) and [S.L. Salzberg](http://www.cbcb.umd.edu/~salzberg/) . [Rapid, accurate, computational discovery of Rho-independent transcription terminators illuminates their relationship to DNA uptake.](http://genomebiology.com/2007/8/2/R22/abstract) _Genome Biology_ 8:R22 (2007). A predecessor system, TransTerm, that used very different search and scoring algorithms is described in: Maria D. Ermolaeva, Hanif G. Khalak, Owen White, Hamilton O. Smith and Steven L. Salzberg. Prediction of Transcription Terminators in Bacterial Genomes. _J Mol Biol_ **301** , (1), 27-33 (2000) ## Features Some features of TransTermHP include: * Fast due to new dynamic programming algorithm (a typical genome takes < 1 min) * New, accurate scoring scheme * Informative output * Reads gene annotations from either a simple list or .ptt files * Handling of overlapping genes & terminators * General handling of gaps in terminator hairpins * Fully in C++ ## Download TransTermHP Source code for version 2.09 (updated Feb 28, 2011) can be downloaded [here](transterm_hp_v2.09.zip) . This version has been tested on Linux, but should compile on most UNIX-like systems using GCC. See the included USAGE.txt file for how to use TransTermHP. (TransTermHP 1.0 was an internal release.) ## Current Predictions To download predictions by organism, visit the [Download Predictions](http://transterm.cbcb.umd.edu/cgi-bin/transterm/predictions.pl) tab. To search predicted terminators, visit the [Search Terminators](http://transterm.cbcb.umd.edu/query.php) tab. ## Contact Us For questions or to report bugs, email transterm [AT] umiacs.umd.edu. ## Acknowledgements This project is supported in part by [NSF](http://www.nsf.gov/) Award DBI-0234704 and by [NIH](http://www.nih.gov) Award R01-LM007938.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"modle/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/modle/cmd/None\",\n    \"name\": \"modle\",\n    \"description\": [\n      \"MoDLE is a computational tool for fast, stochastic modeling of molecular contacts from DNA loop extrusion capable of simulating realistic contact patterns genome wide in a few minutes.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://github.com/paulsengroup/modle\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Jonas Paulsen\",\n        \"email\": \"jonas.paulsen@ibv.uio.no\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Torbj\u00f8rn Rognes\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Anthony Mathelier\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Vipin Kumar\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Roberto Rossini\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8375210e2f3aa0b3bcb3f\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/modle/cmd/1.1.0\",\n    \"name\": \"modle\",\n    \"description\": [\n      \"High-performance stochastic modeling of DNA loop extrusion interactions.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/paulsengroup/MoDLE\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/paulsengroup/MoDLE\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dbfdac7ee6cb57891288\"\n      },\n      {\n        \"$oid\": \"67e2dc4fac7ee6cb578912d9\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://github.com/paulsengroup/modle:\n```\nREADME content:\n['<!-- Copyright (C) 2022 Roberto Rossini <roberros@uio.no> SPDX-License-Identifier: MIT --> # MoDLE [![License](https://img.shields.io/badge/license-MIT-green)](./LICENSE) [![Ubuntu CI](https://github.com/paulsengroup/modle/actions/workflows/ubuntu-ci.yml/badge.svg)](https://github.com/paulsengroup/modle/actions/workflows/ubuntu-ci.yml) [![MacOS CI](https://github.com/paulsengroup/modle/actions/workflows/macos-ci.yml/badge.svg)](https://github.com/paulsengroup/modle/actions/workflows/macos-ci.yml) [![Build Docker image](https://github.com/paulsengroup/modle/actions/workflows/build-docker-image.yml/badge.svg)](https://github.com/paulsengroup/modle/actions/workflows/build-docker-image.yml) [![Download from Bioconda](https://img.shields.io/conda/vn/bioconda/modle?label=bioconda&logo=Anaconda)](https://anaconda.org/bioconda/modle) [![MoDLE paper - Genome Biology 2022](https://img.shields.io/badge/CITE-Genome%20Biology%20(2022)-blue )](https://doi.org/10.1186/s13059-022-02815-7) [![Zenodo DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6424697.svg)](https://doi.org/10.5281/zenodo.6424697) --- MoDLE is a computational tool for fast, stochastic modeling of molecular contacts from DNA loop extrusion capable of simulating realistic contact patterns genome wide in a few minutes. ![Figure 1 from MoDLE\\'s paper](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs13059-022-02815-7/MediaObjects/13059_2022_2815_Fig1_HTML.png) ## Using MoDLE MoDLE is developed on Linux and tested on Linux and MacOS. Running MoDLE on Windows is only possible using Docker containers (see below). MoDLE can be installed or compiled with one of the following methods. ### Docker or Singularity/Apptainer First, make sure you follow the instructions on how to install Docker or Singularity/Apptainer on your OS. <details> <summary>Installing Docker</summary> The following instructions assume you have root/admin permissions. - [Linux](https://docs.docker.com/desktop/install/linux-install/#generic-installation-steps/) - [MacOS](https://docs.docker.com/desktop/install/mac-install/) - [Windows](https://docs.docker.com/desktop/install/windows-install/) On some Linux distributions just installing Docker is not enough. You also need to start (and optionally enable) the appropriate service(s). This is usually done with one of the following: ```bash sudo systemctl start docker sudo systemctl start docker.service ``` Refer to [Docker](https://docs.docker.com/engine/install/) or your distribution documentation for more details. </details> <details> <summary>Installing Singularity/Apptainer</summary> The following instructions assume you have root/admin permissions. Apptainer can be easily installed using your system package manager. [Here](https://apptainer.org/docs/admin/main/installation.html#install-from-pre-built-packages) you can find instructions for common Linux distributions such as Ubuntu. Even if your distribution is not listed in the above documentation, your system package manager likely includes a package for Singularity or Apptainer. If this is not the case, then you must install Apptainer from source (instructions available [here](https://github.com/apptainer/apptainer/blob/release-1.1/INSTALL.md)). </details> #### Pulling MoDLE Docker image MoDLE Docker images are available on [ghcr.io](https://github.com/paulsengroup/modle/pkgs/container/modle) and [dockerhub](https://hub.docker.com/repository/docker/paulsengroup/modle). Downloading and running the latest stable release can be done as follows: ```console # Using Docker, may require sudo user@dev:/tmp$ docker run ghcr.io/paulsengroup/modle:1.1.0 --help # Using Singularity/Apptainer user@dev:/tmp$ singularity run ghcr.io/paulsengroup/modle:1.1.0 --help High-performance stochastic modeling of DNA loop extrusion interactions. Usage: /usr/local/bin/modle [OPTIONS] SUBCOMMAND Options: -h,--help Print this help message and exit -V,--version Display program version information and exit --config Path to MoDLE\\'s config file (optional). Subcommands: simulate, sim Simulate loop extrusion and write resulting molecular contacts in a .cool file. ``` The above will print MoDLE\\'s help message, and is equivalent to running `modle --help` on the command line (assuming MoDLE is available on your machine). <details> <summary>Troubleshooting</summary> <!-- TODO: Should this section be moved elsewhere? --> __Errors regarding missing input file(s)__ Assuming you have placed the required input files inside a folder named `mydata` ```console user@dev:/tmp$ ls -lah mydata total 248K drwx------ 2 user group 80 Jan 5 12:00 . drwxrwxrwt 20 user group 540 Jan 5 12.00 .. -rw------- 1 user group 365 Jan 5 12:00 grch38.chrom.sizes -rw------- 1 user group 241K Jan 5 12:00 grch38_h1_extrusion_barriers.bed.xz ``` Running MoDLE using Docker as shown below fails due to missing input file(s) ```bash docker run ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes mydata/grch38.chrom.sizes \\\\ --extrusion-barrier-file mydata/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix mydata/output # Error message # --chrom-sizes: File does not exist: mydata/grch38.chrom.sizes # Run with --help for more information. ``` This issue is caused by Docker not mounting/binding the host filesystem (e.g. the filesystem where `mydata/` resides) inside the container. To make `mydata/` accessible from inside the container, use one of the following methods ```bash # Method 1: mydata/ folder on the host is mapped to /mydata (notice the /) inside the container docker run -v \"$(pwd -P)/mydata/:/data/\" \\\\ ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes /data/grch38.chrom.sizes \\\\ --extrusion-barrier-file /data/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix /data/output # Mehtod 2: use two different folders for input/output. Input folder is mounted in read-only mode docker run -v \"$(pwd -P)/mydata/:/input_data/:ro\" \\\\ -v \"$(pwd -P)/output/:/output_data/\" \\\\ ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes /input_data/grch38.chrom.sizes \\\\ --extrusion-barrier-file /input_data/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix /output_data/output # Methods for Singularity/Apptainer (note that manually mounting paths is usually not required when using Singularity/Apptainer) singularity run -B \"$(pwd -P)/mydata/:/data/\" \\\\ docker://ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes /data/grch38.chrom.sizes \\\\ --extrusion-barrier-file /data/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix /data/output singularity run -B \"$(pwd -P)/mydata/:/input_data/:ro\" \\\\ -B \"$(pwd -P)/output/:/output_data/\" \\\\ docker://ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes /input_data/grch38.chrom.sizes \\\\ --extrusion-barrier-file /input_data/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix /output_data/output ``` Output produced by Method 1: ```console user@dev:/tmp$ ls -lah mydata total 57M drwx------ 2 user group 140 Jan 5 12:20 . drwxrwxrwt 20 user group 560 Jan 5 12:20 .. -rw------- 1 user group 365 Jan 5 12:00 grch38.chrom.sizes -rw------- 1 user group 241K Jan 5 12:00 grch38_h1_extrusion_barriers.bed.xz -rw-r--r-- 1 root root 3.7K Jan 5 12:00 output_config.toml -rw-r--r-- 1 root root 2.4M Jan 5 12:05 output_lef_1d_occupancy.bw -rw-r--r-- 1 root root 57M Jan 5 12:05 output.cool -rw-r--r-- 1 root root 14K Jan 5 12:05 output.log ``` Output produced by Method 2: ```console user@dev:/tmp$ ls -lah mydata/ total 57M drwx------ 2 user group 140 Jan 5 12:20 . drwxrwxrwt 20 user group 560 Jan 5 12:20 .. -rw------- 1 user group 365 Jan 5 12:00 grch38.chrom.sizes -rw------- 1 user group 241K Jan 5 12:00 grch38_h1_extrusion_barriers.bed.xz user@dev:/tmp$ ls -lah output/ drwx------ 2 user group 140 Jan 5 12:20 . drwxrwxrwt 20 user group 560 Jan 5 12:20 .. -rw-r--r-- 1 root root 3.7K Jan 5 12:00 output_config.toml -rw-r--r-- 1 root root 2.4M Jan 5 12:05 output_lef_1d_occupancy.bw -rw-r--r-- 1 root root 57M Jan 5 12:05 output.cool -rw-r--r-- 1 root root 14K Jan 5 12:05 output.log ``` __Troubleshooting problems inside the container is a bit tedious. Any tips?__ Yes! For troubleshooting we recommend using an interactive shell running inside the container. ```bash docker run -v \"$(pwd -P)/mydata/:/input_data/:ro\" \\\\ -v \"$(pwd -P)/output/:/output_data/\" \\\\ -it --rm --entrypoint=/bin/bash \\\\ ghcr.io/paulsengroup/modle:1.1.0 ``` This will drop you in an interactive bash shell. Press `Ctrl+C`, `Ctrl+D` or type `exit` to leave the shell and stop the container. ```console root@25e8efe74294:/data# ls -lah total 4.0K drwxr-xr-x 2 root root 2 Jan 5 12:00 . drwxr-xr-x 20 root root 27 Jan 5 12:00 .. root@25e8efe74294:/data# cd /input_data/ root@25e8efe74294:/input_data# ls -lah total 250K drwx------ 2 1000 1000 80 Jan 5 12:00 . drwxr-xr-x 20 root root 27 Jan 5 12:00 .. -rw------- 1 1000 1000 365 Jan 5 12:00 grch38.chrom.sizes -rw------- 1 1000 1000 241K Jan 5 12:00 grch38_h1_extrusion_barriers.bed.xz root@25e8efe74294:/input_data# touch foo touch: cannot touch \\'foo\\': Read-only file system root@25e8efe74294:/input_data# cd /output_data/ root@25e8efe74294:/output_data# ls -lah total 2.0K drwxr-xr-x 2 1000 1000 40 Jan 5 12:00 . drwxr-xr-x 20 root root 27 Jan 5 12:00 .. root@25e8efe74294:/output_data# touch foo root@25e8efe74294:/output_data# ls -lah total 2.0K drwxr-xr-x 2 1000 1000 60 Jan 5 12:00 . drwxr-xr-x 20 root root 27 Jan 5 12:00 .. -rw-r--r-- 1 root root 0 Jan 5 12:01 foo root@25e8efe74294:/output_data# whereis modle modle: /usr/local/bin/modle root@25e8efe74294:/output_data# modle --help High-performance stochastic modeling of DNA loop extrusion interactions. Usage: modle [OPTIONS] SUBCOMMAND Options: -h,--help Print this help message and exit -V,--version Display program version information and exit --config Path to MoDLE\\'s config file (optional). Subcommands: simulate, sim Simulate loop extrusion and write resulting molecular contacts in a .cool file. root@25e8efe74294:/output_data# exit exit ``` For Singularity/Apptainer this is even easier: ```bash singularity shell docker://ghcr.io/paulsengroup/modle:1.1.0 ``` </details> ### Conda (bioconda) MoDLE package for Linux and MacOS is available on [bioconda](https://anaconda.org/bioconda/modle) and can be installed as follows: ```console user@dev:/tmp$ conda create -n modle -c conda-forge -c bioconda modle (modle) user@dev:/tmp$ conda activate modle (modle) user@dev:/tmp$ whereis modle modle: /home/user/.miniconda3/envs/modle/bin/modle (modle) user@dev:/tmp$ modle --version MoDLE-v1.1.0-bioconda ``` ### Pre-built Linux binaries (not recommended) Download file `modle-1.1.0-x86_64-linux.tar.xz` from the [latest release](https://github.com/paulsengroup/modle/releases/latest). Extract the archive and copy `modle` and/or `modle_tools` to a location in your `PATH` <details> <summary> click to expand </summary> <!-- TODO make sure the link is correct once the release goes live --> ```console user@dev:/tmp$ curl -LO \\'https://github.com/paulsengroup/modle/releases/download/v1.1.0/modle-1.1.0-x86_64-linux.tar.xz\\' user@dev:/tmp$ tar -xvf modle-1.1.0-x86_64-linux.tar.xz modle-1.1.0-x86_64-linux/ modle-1.1.0-x86_64-linux/share/ modle-1.1.0-x86_64-linux/share/licenses/ modle-1.1.0-x86_64-linux/share/licenses/modle_tools/ modle-1.1.0-x86_64-linux/share/licenses/modle_tools/LICENSE modle-1.1.0-x86_64-linux/share/licenses/modle/ modle-1.1.0-x86_64-linux/share/licenses/modle/LICENSE modle-1.1.0-x86_64-linux/bin/ modle-1.1.0-x86_64-linux/bin/modle_tools modle-1.1.0-x86_64-linux/bin/modle user@dev:/tmp modle-1.1.0-x86_64-linux/bin/modle --version MoDLE-v1.1.0 # Optional: add modle and modle_tools to your PATH (please ensure $HOME/.local/bin/ is in your PATH) user@dev:/tmp$ install -Dm0755 modle-1.1.0-x86_64-linux/bin/modle \"$HOME/.local/bin/\" user@dev:/tmp$ install -Dm0755 modle-1.1.0-x86_64-linux/bin/modle_tools \"$HOME/.local/bin/\" user@dev:/tmp$ whereis modle modle: /home/user/.local/bin/modle user@dev:/tmp$ modle --version MoDLE-v1.1.0 ``` </details> ### Installing from source MoDLE can be compiled on most UNIX-like systems, including many Linux distributions and MacOS (10.15+). [Build instructions](https://github.com/paulsengroup/modle/wiki/Building-from-source) ### Running MoDLE Here we assume MoDLE was correctly installed and is available in your PATH (i.e. running `whereis modle` prints something like `modle: /usr/local/bin/modle`). If you are using Docker or Singularity/Apptainer, please make sure input files can be reached from within the container (see troubleshooting steps from [Using MoDLE with Docker or Singularity/Apptainer](https://github.com/paulsengroup/modle/tree/update-readme#docker-or-singularityapptainer). Folder `examples/data/` from this repository contains the input files used throughout this section. <details> <summary> Data provenance </summary> - `examples/data/hg38.chrom.sizes` was generated from the [hg38.chrom.sizes](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.chrom.sizes) released by UCSC. Chromosmes were filtered and sorted using `grep -E \\'chr[0-9XY]+[[:space:]]\\' hg38.chrom.sizes | sort -V` - `examples/data/hg38_extrusion_barriers.bed.xz` was generated as described the Methods of MoDLE\\'s [paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02815-7) (third paragraph of section \"MoDLE simulations\"). Refer to code hosted on [github.com/paulsengroup/2021-modle-paper-001-data-analysis](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis) for more details (useful links: [link1](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis/blob/main/workflows/preprocess_data.nf#L41-L100), [link2](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis/blob/main/configs/preprocess_data.config), [link3](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis/blob/main/data/download_list.txt), [link4](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis/blob/main/scripts/convert_chip_signal_to_occupancy.py). Feel free to get in touch if you are struggling to navigate code hosted in the data analysis repository). </details> #### Required input files Running a simulation with default settings only requires two input files: - A [chrom.sizes](https://software.broadinstitute.org/software/igv/chromSizes) file with the list of chromosomes to be simulated - A [BED file](https://bedtools.readthedocs.io/en/latest/content/general-usage.html#bed-format) with the list of extrusion barriers to include as part of the simulation Input files using common compression formats (namely Bzip2, Gzip, LZ4, xz/LZMA and zstd) are supported. The extrusion barrier BED file should have at least the first 6 columns defined (i.e. chrom, start, end, name, score and strand. See [bedtools docs](https://bedtools.readthedocs.io/en/latest/content/general-usage.html#bed-format) for more details). Some remarks: - ___chrom___ should contain the name of one of the chromosome defined in the `.chrom.sizes` - The ___start___ and ___end___ fields are used to position barriers along chromosomes. MoDLE models extrusion barriers as 1bp entities. Thus, when $end - start \\\\gt 1$ the extrusion barrier will be placed in the middle of interval $[start, end)$. - The ___name___ field is required but ignored, and can be safely set to `.` - ___score___ is required and should be set to a number between 0 and 1. This number expresses the average occupancy of the extrusion barrier site defined by the current line. Occupancies between 0.7-0.9 can be used as a starting point. Check out [this](https://github.com/paulsengroup/modle/wiki/%5BTutorial%5D---Generating-a-list-of-extrusion-barrier-from-ChIP-seq-data) tutorial to learn how barrier occupancy can be inferred from CTCF/RAD21 ChIP-Seq data. More information regarding how extrusion barriers are modeled canm be found in [MoDLE\\'s paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02815-7) and [suppl. text](https://static-content.springer.com/esm/art%3A10.1186%2Fs13059-022-02815-7/MediaObjects/13059_2022_2815_MOESM1_ESM.pdf). - ___strand___ is required and is used to define the extrusion barrier direction (should be one of `-`, `+` or `.`). As of MoDLE v1.1.0, extrusion barriers are modeled after CTCF barriers. Thus, the strand field should be populated with the direction of the CTCF binding site that is being defined. Barriers without strand information (i.e. with strand `.`) are ignored. #### Running a simulation with default settings ```console user@dev:/tmp/modle$ modle simulate \\\\ --chrom-sizes examples/data/hg38.chrom.sizes \\\\ --extrusion-barrier-file examples/data/hg38_extrusion_barriers.bed.xz \\\\ --output-prefix examples/out/hg38_default [2023-05-11 11:47:15.718] [info]: running MoDLE v1.1.0 [2023-05-11 11:47:15.718] [info]: complete log will be written to file \"examples/out/hg38_default.log\" [2023-05-11 11:47:15.718] [info]: writing simulation parameters to config file \"examples/out/hg38_default_config.toml\" [2023-05-11 11:47:15.718] [info]: command: modle simulate --chrom-sizes examples/data/hg38.chrom.sizes --extrusion-barrier-file examples/data/hg38_extrusion_barriers.bed.xz --output-prefix examples/out/hg38_default [2023-05-11 11:47:15.719] [info]: simulation will use up to 16 out of 16 available CPU cores. [2023-05-11 11:47:15.719] [info]: using --target-contact-density=1.00 as stopping criterion. [2023-05-11 11:47:15.719] [info]: contact sampling strategy: tad-plus-loop-with-noise. [2023-05-11 11:47:15.719] [info]: contact matrix resolution: 5000bp [2023-05-11 11:47:15.719] [info]: importing chromosomes from \"examples/data/hg38.chrom.sizes\"... [2023-05-11 11:47:15.719] [info]: imported 24 chromosomes in 239.985us. [2023-05-11 11:47:15.719] [info]: importing extrusion barriers from \"examples/data/hg38_extrusion_barriers.bed.xz\"... [2023-05-11 11:47:15.761] [info]: imported 38815 barriers from \"examples/data/hg38_extrusion_barriers.bed.xz\" in 41.776052ms. [2023-05-11 11:47:15.762] [info]: spawning worker thread W0... [2023-05-11 11:47:15.762] [info]: spawning worker thread W1... [2023-05-11 11:47:15.762] [info]: spawning worker thread W3... ... [2023-05-11 11:47:15.764] [info]: begin processing chr1:0-248956422: simulating ~37485 epochs across 512 cells using 4979 LEFs and 3518 barriers (~73 epochs per cell)... [2023-05-11 11:47:37.627] [info]: begin processing chr2:0-242193529: simulating ~37501 epochs across 512 cells using 4844 LEFs and 2974 barriers (~73 epochs per cell)... [2023-05-11 11:47:43.592] [info]: [io] writing contacts for chr1:0-248956422 to file \"examples/out/hg38_default.cool\"... [2023-05-11 11:47:46.182] [info]: [io]: written 29875200 contacts for chr1:0-248956422 to file \"examples/out/hg38_default.cool\" in 2.590415163s (3.47M nnz out of 29.88M pixels). [2023-05-11 11:47:46.186] [info]: [io]: writing 1D LEF occupancy profile for chr1:0-248956422 to file \"examples/out/hg38_default_lef_1d_occupancy.bw\"... [2023-05-11 11:47:46.196] [info]: [io]: writing 1D LEF occupancy profile for chr1:0-248956422 to file \"examples/out/hg38_default_lef_1d_occupancy.bw\" took 9.964882ms [2023-05-11 11:47:57.200] [info]: begin processing chr3:0-198295559: simulating ~37474 epochs across 512 cells using 3966 LEFs and 2469 barriers (~73 epochs per cell)... ... [2023-05-11 11:51:49.400] [info]: [io] writing contacts for chrY:0-57227415 to file \"examples/out/hg38_default.cool\"... [2023-05-11 11:51:49.744] [info]: [io]: written 6867600 contacts for chrY:0-57227415 to file \"examples/out/hg38_default.cool\" in 344.473166ms (0.92M nnz out of 6.87M pixels). [2023-05-11 11:51:49.744] [info]: [io]: writing 1D LEF occupancy profile for chrY:0-57227415 to file \"examples/out/hg38_default_lef_1d_occupancy.bw\"... [2023-05-11 11:51:49.746] [info]: [io]: writing 1D LEF occupancy profile for chrY:0-57227415 to file \"examples/out/hg38_default_lef_1d_occupancy.bw\" took 1.565303ms', '[2023-05-11 11:51:51.925] [info]: simulation terminated without errors in 4m36.206746164s! Bye. ``` The above command will create folder `examples/out/` (if it doesn\\'t already exist), and write the following files inside it: ``` examples/out \u251c\u2500\u2500 hg38_default_config.toml \u251c\u2500\u2500 hg38_default.cool \u251c\u2500\u2500 hg38_default_lef_1d_occupancy.bw \u2514\u2500\u2500 hg38_default.log ``` Output files produced by the above command are available [here](https://doi.org/10.5281/zenodo.7924253). Simulated interactions are written to file `hg38_default.cool`. More information about the .cool format is available at [open2c/cooler](https://github.com/open2c/cooler). In case any of the output files already exist, MoDLE will refuse to run and print an error message listing the file name collisions. Passing the `--force` flag overrides this behavior and will cause MoDLE to overwrite existing files. To visualize `.cool` files we recommend using [cooler show](https://open2c.github.io/cooler/) or [higlass](https://docs.higlass.io/) (more specifically [higlass-docker](https://github.com/higlass/higlass-docker) or [higlass-manage](https://github.com/higlass/higlass-manage)). <details> <summary> <b> Tips for visualization </b> </summary> ##### Quickly visualizing interactions with Cooler ```bash cooler show examples/out/hg38_default.cool chr1:5000000-8000000 ``` ![chr1 5-8Mbp heatmap](examples/images/hg38_default_001.avif) ##### Visualizing interactions with HiGlass Before visualizing heatmaps with HiGlass, the single-resolution `.cool` file produced by MoDLE needs to be converted to a multi-resolution Cooler (`.mcool`) file. This is easily done with `cooler zoomify`: ```console user@dev:/tmp/modle$ cooler zoomify examples/out/hg38_default.cool INFO:cooler.cli.zoomify:Recursively aggregating \"examples/out/hg38_default.cool\" INFO:cooler.cli.zoomify:Writing to \"examples/out/hg38_default.mcool\" INFO:cooler.reduce:Copying base matrices and producing 12 new zoom levels. INFO:cooler.reduce:Bin size: 5000 INFO:cooler.reduce:Aggregating from 5000 to 10000. INFO:cooler.create:Creating cooler at \"examples/out/hg38_default.mcool::/resolutions/10000\" INFO:cooler.create:Writing chroms INFO:cooler.create:Writing bins INFO:cooler.create:Writing pixels INFO:cooler.reduce:0 10000066 INFO:cooler.reduce:10000066 20000079 INFO:cooler.reduce:20000079 30000056 INFO:cooler.reduce:30000056 40000018 INFO:cooler.reduce:40000018 43586992 INFO:cooler.create:Writing indexes INFO:cooler.create:Writing info INFO:cooler.reduce:Aggregating from 10000 to 20000. ... ``` Next, you need to start HiGlass server. If you are using `higlass-manage`, you may have to prefix commands with `sudo` or `sudo -E`. ```console # Using higlass-docker user@dev:/tmp/modle$ sudo docker pull higlass/higlass-docker Using default tag: latest latest: Pulling from higlass/higlass-docker Digest: sha256:44086069ee7d4d3f6f3f0012569789ec138f42b84aa44357826c0b6753eb28de Status: Image is up to date for higlass/higlass-docker:latest docker.io/higlass/higlass-docker:latest user@dev:/tmp/modle$ sudo docker run \\\\ --detach \\\\ --publish 8989:80 \\\\ --volume ~/hg-data:/data \\\\ --volume ~/hg-tmp:/tmp \\\\ --name higlass-modle \\\\ higlass/higlass-docker eb3fe07ea23b57eb844a590e7f1e939cf82a11e5d25db87c4b25024a41c9a546 # Using higlass-manage user@dev:/tmp/modle$ sudo higlass-manage start --use-redis --no-public-data Stopping previously running container Attempting to remove existing Docker network instance Stopping previously running Redis container Pulling redis:5.0.3-alpine done Pulling latest image... done Data directory: /home/user/hg-data Temp directory: () Starting... default 8989 Docker started: higlass-manage-container-default sending request 1 Waiting to start (tilesets)... sending request 2 ... public_data: False ret: b\\'{\"uid\": \"default_local\"}\\' ret: ExecResult(exit_code=0, output=b\\'\\') Replaced js file Started ``` Now if you visit [localhost:8989/app](http://localhost:8989/app) with your browser you should see an empty HiGlass page like the following: ![higlass empty page](examples/images/higlass_001.avif) Next, we need to ingest our `.mcool` into HiGlass: ```console user@dev:/tmp/modle$ mkdir ~/hg-data/media/modle user@dev:/tmp/modle$ cp examples/out/hg38_default.mcool ~/hg-data/media/modle # With higlass-docker user@dev:/tmp/modle$ sudo docker exec higlass-modle \\\\ python higlass-server/manage.py ingest_tileset \\\\ --project-name \"modle\" \\\\ --filename modle/hg38_default.mcool \\\\ --filetype cooler \\\\ --datatype matrix \\\\ --no-upload # With higlass-manage user@dev:/tmp/modle$ sudo higlass-manage ingest --project-name=modle modle/hg38_default.mcool --no-upload state True Inferred filetype: cooler Inferred datatype: matrix state True name_text: hg_name: default command: python higlass-server/manage.py ingest_tileset --filename modle/hg38_default.mcool --filetype cooler --datatype matrix --project-name \"modle\" --no-upload --uid emFfbA9tTCiBTt6b5vTG8Q ``` Finally, after refreshing page [localhost:8989/app](http://localhost:8989/app) you should be able to add tileset `hg38_default.mcool` to the center panel: ![higlass default settings](examples/images/higlass_002.avif) In case you are struggling to visualize MoDLE\\'s output feel free to reach out by starting a new [discussion](https://github.com/paulsengroup/modle/discussions). </details> <details> <summary> <b> Tips for programmatic access to Cooler files </b> </summary> Programmatic access to contacts stored in Cooler files is possible using the `cooler` [CLI interface](https://cooler.readthedocs.io/en/latest/cli.html) or Python [API](https://cooler.readthedocs.io/en/latest/api.html). ##### CLI interface ```console # Dumping interactions in BEDPE format user@dev:/tmp/modle$ cooler dump --table pixels --join examples/out/hg38_default.cool | head chr1 0 5000 chr1 0 5000 9 chr1 0 5000 chr1 5000 10000 5 chr1 0 5000 chr1 10000 15000 2 chr1 0 5000 chr1 15000 20000 2 chr1 0 5000 chr1 20000 25000 1 chr1 0 5000 chr1 25000 30000 2 chr1 0 5000 chr1 30000 35000 3 chr1 0 5000 chr1 35000 40000 1 chr1 0 5000 chr1 40000 45000 2 chr1 0 5000 chr1 45000 50000 1 user@dev:/tmp/modle$ cooler dump --table pixels --join examples/out/hg38_default.mcool::/resolutions/200000 | head chr1 0 20000 chr1 0 20000 114 chr1 0 20000 chr1 20000 40000 120 chr1 0 20000 chr1 40000 60000 69 chr1 0 20000 chr1 60000 80000 59 chr1 0 20000 chr1 80000 100000 34 chr1 0 20000 chr1 100000 120000 28 chr1 0 20000 chr1 120000 140000 25 chr1 0 20000 chr1 140000 160000 18 chr1 0 20000 chr1 160000 180000 20 chr1 0 20000 chr1 180000 200000 12 ``` ##### Python API ```python-console In [1]: import cooler In [2]: c = cooler.Cooler(\"examples/out/hg38_default.cool\") In [3]: selector = c.matrix(balance=False) In [4]: selector.fetch(\"chr1:0-50000\") Out[4]: array([[ 9, 5, 2, 2, 1, 2, 3, 1, 2, 1], [ 5, 9, 9, 15, 3, 3, 8, 1, 8, 1], [ 2, 9, 12, 26, 11, 7, 3, 8, 5, 2], [ 2, 15, 26, 25, 27, 16, 12, 14, 10, 7], [ 1, 3, 11, 27, 20, 42, 30, 17, 15, 10], [ 2, 3, 7, 16, 42, 21, 27, 17, 15, 12], [ 3, 8, 3, 12, 30, 27, 26, 27, 26, 17], [ 1, 1, 8, 14, 17, 17, 27, 23, 44, 23], [ 2, 8, 5, 10, 15, 15, 26, 44, 31, 35], [ 1, 1, 2, 7, 10, 12, 17, 23, 35, 23]], dtype=int32) In [5]: selector = c.matrix(balance=False, as_pixels=True, join=True) In [6]: selector.fetch(\"chr1:0-50000\") Out[6]: chrom1 start1 end1 chrom2 start2 end2 count 0 chr1 0 5000 chr1 0 5000 9 1 chr1 0 5000 chr1 5000 10000 5 2 chr1 0 5000 chr1 10000 15000 2 3 chr1 0 5000 chr1 15000 20000 2 4 chr1 0 5000 chr1 20000 25000 1 5 chr1 0 5000 chr1 25000 30000 2 6 chr1 0 5000 chr1 30000 35000 3 7 chr1 0 5000 chr1 35000 40000 1 8 chr1 0 5000 chr1 40000 45000 2 9 chr1 0 5000 chr1 45000 50000 1 ... ``` Refer to Cooler [documentation](https://cooler.readthedocs.io/en/latest/quickstart.html) for more details. </details> #### Running a simulation using config files The `hg38_default_config.toml` file produced by MoDLE in the [previous section](#running-a-simulation-with-default-settings) is a config file that can be used to repeat simulations using the same set of parameters and input files. Information found in config files is also embedded in the `.cool` file as a JSON string, and can be extracted using `cooler info`: <!-- TODO: Update once https://github.com/paulsengroup/modle/issues/75 is implemented --> ```console user@dev:/tmp/modle$ cooler info --metadata examples/out/hg38_default.cool { \"simulate\": { \"avg-lef-processivity\": 300000, \"burnin-extr-speed-coefficient\": 1, \"burnin-history-length\": 100, \"burnin-smoothing-window-size\": 5, \"burnin-target-epochs-for-lef-activation\": 320, \"chrom-sizes\": \"examples/data/hg38.chrom.sizes\", ... ``` The following will run a simulation with the same settings as the simulation from the [previous example](#Running-a-simulation-with-default-settings) ```bash modle simulate --config examples/out/hg38_default_config.toml --force ``` Parameters read from a config file have lower priority than parameter specified through the CLI, meaning that CLI options can be used to override parameters read from the config file. This can be useful when running a batch of simulations using a large number of optional parameters, and where only a handful of parameters need to change across simulation runs. The following command will run a simulation using parameters from the [previous example](#Running-a-simulation-with-default-settings) as starting point, but using a different number of cells and target contact density. ```bash modle simulate \\\\ --config examples/out/hg38_default_config.toml \\\\ --ncells 16 \\\\ --target-contact-density 10 \\\\ --output-prefix examples/out/hg38_deep ``` Output files produced by the above command are available [here](https://doi.org/10.5281/zenodo.7924253). ![Simulation w/ high contact density](examples/images/higlass_003.avif) MoDLE\\'s config files are in [TOML format](https://toml.io/en/). Adding a line like `my-option=my_value` to the config file has the same effect as passing `--my-option=my_value` on the CLI. For an up-to-date list of supported CLI options, please refer to MoDLE\\'s help message: ```console user@dev:/tmp/modle$ modle simulate --help Simulate loop extrusion and write resulting molecular contacts in a .cool file. Usage: modle simulate [OPTIONS] Options: -h,--help Print this help message and exit [Option Group: IO] Options controlling MoDLE input, output and logs. Options: -c,--chrom-sizes REQUIRED Path to file with chromosome sizes in chrom.sizes format. -b,--extrusion-barrier-file REQUIRED Path to a file in BED6+ format with the genomic coordinates of extrusion barriers to be simulated. The score field in a BED record should be a number between 0 and 1 and is interpreted as the extrusion barrier occupancy for the extrusion barrier described by the record. Barriers mapping on chromosomes not listed in the chrom.sizes file passed through the --chrom-sizes option are ignored. -g,--genomic-intervals Path to BED3+ file with the genomic regions to be simulated. Intervals listed in this file should be a subset of the chromosomes defined in the .chrom.sizes files. Intervals referring to chromosomes not listed in the .chrom.sizes file are ignored. -f,--force Overwrite existing files (if any). -o,--output-prefix REQUIRED Output prefix. Can be an absolute or relative path including the file name but without the extension. Example: running modle sim -o /tmp/my_simulation ... yields the following files: - /tmp/my_simulation.cool - /tmp/my_simulation.log - /tmp/my_simulation_config.toml - /tmp/my_simulation_lef_1d_occupancy.bw --assembly-name=unknown Name of the genome assembly to be simulated. This is only used to populate the \"assembly\" attribute in the output .cool file. ... ``` ## Citing If you use MoDLE in your research, please cite the following paper: Rossini, R., Kumar, V., Mathelier, A. _et al._ MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions. _Genome Biol_ __23__, 247 (2022). [https://doi.org/10.1186/s13059-022-02815-7](https://doi.org/10.1186/s13059-022-02815-7) <details> <summary>BibTex</summary> ```bibtex @article{rossini_2022, author = {Rossini, Roberto and Kumar, Vipin and Mathelier, Anthony and Rognes, Torbj\u00f8rn and Paulsen, Jonas}, doi = {10.1186/s13059-022-02815-7}, journal = {Genome Biology}, month = {11}, title = {{MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions}}, url = {https://doi.org/10.1186/s13059-022-02815-7}, volume = {23}, year = {2022} } ``` </details> In addition to the above reference, you can also cite specific version(s) of MoDLE using one of the DOIs from Zenodo: [10.5281/zenodo.6424697](https://doi.org/10.5281/zenodo.6424697)']```"}, {"role": "user", "content": "Content from http://github.com/paulsengroup/modle:\n```\nRepository metadata:\n{'name': 'modle', 'label': ['modle', 'MoDLE'], 'description': [{'term': 'High-performance stochastic modeling of DNA loop extrusion interactions', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://doi.org/10.1186/s13059-022-02815-7', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.1.0', 'v1.0.1', 'v1.0.0', 'v1.0.0-rc.7', 'v1.0.0-rc.6', 'v1.0.0-rc.5', 'v1.0.0-rc.4', 'v1.0.0-rc.3', 'v1.0.0-rc.2', 'v1.0.0-rc.1'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}, {'term': {'name': 'MIT', 'url': ''}, 'id': 1}], 'repository': [{'term': 'https://github.com/paulsengroup/modle', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/biology', 'term': 'biology', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/cpp', 'term': 'cpp', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/cpp17', 'term': 'cpp17', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/dna', 'term': 'dna', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/high-performance', 'term': 'high-performance', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/loop-extrusion', 'term': 'loop-extrusion', 'vocabulary': ''}, 'id': 6}, {'term': {'uri': 'https://github.com/topics/stochastic-model', 'term': 'stochastic-model', 'vocabulary': ''}, 'id': 7}, {'term': {'uri': 'https://github.com/topics/tad', 'term': 'tad', 'vocabulary': ''}, 'id': 8}, {'term': {'uri': 'https://github.com/topics/topologically-associated-domains', 'term': 'topologically-associated-domains', 'vocabulary': ''}, 'id': 9}, {'term': {'uri': 'https://github.com/topics/hic', 'term': 'hic', 'vocabulary': ''}, 'id': 10}], 'operations': [], 'authors': [{'term': {'name': 'Roberto Rossini', 'email': '', 'maintainer': False, 'type': 'person'}, 'id': 0}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/paulsengroup/modle/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/paulsengroup/modle/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/paulsengroup/modle/blob/main/conanfile.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [{'term': {'title': 'MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions', 'year': '2022', 'doi': '10.1186/s13059-022-02815-7', 'url': 'https://doi.org/10.1186/s13059-022-02815-7'}, 'id': 0}], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': '', 'citation': \"# Copyright (C) 2022 Roberto Rossini <roberros@uio.no>\\n#\\n# SPDX-License-Identifier: MIT\\n\\ncff-version: 1.2.0\\nmessage: 'If you use this software, please cite it using the metadata from this file.'\\nauthors:\\n  - given-names: Roberto\\n    family-names: Rossini\\n    orcid: 'https://orcid.org/0000-0003-3096-1470'\\n    email: roberros@uio.no\\n    affiliation: 'Department of Biosciences, University of Oslo'\\ntitle: MoDLE\\nabstract: 'High-performance stochastic modeling of DNA loop extrusion interactions.'\\ndoi: '10.5281/zenodo.6424697'\\nurl: 'https://github.com/paulsengroup/modle'\\nrepository-code: 'https://github.com/paulsengroup/modle'\\ntype: software\\nlicense: MIT\\npreferred-citation:\\n  type: article\\n  authors:\\n  - given-names: Roberto\\n    family-names: Rossini\\n    orcid: 'https://orcid.org/0000-0003-3096-1470'\\n    email: roberros@uio.no\\n    affiliation: 'Department of Biosciences, University of Oslo'\\n  - given-names: Vipin\\n    family-names: Kumar\\n    email: vipin.kumar@ncmm.uio.no\\n    affiliation: 'Centre for Molecular Medicine Norway (NCMM), Nordic EMBL Partnership, University of Oslo'\\n  - given-names: Anthony\\n    family-names: Mathelier\\n    orcid: 'https://orcid.org/0000-0001-5127-5459'\\n    email: anthony.mathelier@ncmm.uio.no\\n    affiliation: 'Centre for Molecular Medicine Norway (NCMM), Nordic EMBL Partnership, University of Oslo'\\n  - given-names: Torbj\u00f8rn\\n    family-names: Rognes\\n    orcid: 'https://orcid.org/0000-0002-9329-9974'\\n    email: torognes@ifi.uio.no\\n    affiliation: 'Centre for Bioinformatics, Department of Informatics, University of Oslo'\\n  - given-names: Jonas\\n    family-names: Paulsen\\n    orcid: 'https://orcid.org/0000-0002-7918-5495'\\n    email: jonas.paulsen@ibv.uio.no\\n    affiliation: 'Department of Biosciences, University of Oslo'\\n  doi: '10.1186/s13059-022-02815-7'\\n  url: 'https://doi.org/10.1186/s13059-022-02815-7'\\n  journal: 'Genome Biology'\\n  year: 2022\\n  month: 11\\n  volume: 23\\n  number: 1\\n  pages: 247\\n  issn: '1474-760X'\\n  title: 'MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions'\\n  abstract: 'DNA loop extrusion emerges as a key process establishing genome structure and function. We introduce MoDLE, a computational tool for fast, stochastic modeling of molecular contacts from DNA loop extrusion capable of simulating realistic contact patterns genome wide in a few minutes. MoDLE accurately simulates contact maps in concordance with existing molecular dynamics approaches and with Micro-C data and does so orders of magnitude faster than existing approaches. MoDLE runs efficiently on machines ranging from laptops to high performance computing clusters and opens up for exploratory and predictive modeling of 3D genome structure in a wide range of settings.'\\n\"}```"}, {"role": "user", "content": "Content from https://github.com/paulsengroup/MoDLE:\n```\nREADME content:\n['<!-- Copyright (C) 2022 Roberto Rossini <roberros@uio.no> SPDX-License-Identifier: MIT --> # MoDLE [![License](https://img.shields.io/badge/license-MIT-green)](./LICENSE) [![Ubuntu CI](https://github.com/paulsengroup/modle/actions/workflows/ubuntu-ci.yml/badge.svg)](https://github.com/paulsengroup/modle/actions/workflows/ubuntu-ci.yml) [![MacOS CI](https://github.com/paulsengroup/modle/actions/workflows/macos-ci.yml/badge.svg)](https://github.com/paulsengroup/modle/actions/workflows/macos-ci.yml) [![Build Docker image](https://github.com/paulsengroup/modle/actions/workflows/build-docker-image.yml/badge.svg)](https://github.com/paulsengroup/modle/actions/workflows/build-docker-image.yml) [![Download from Bioconda](https://img.shields.io/conda/vn/bioconda/modle?label=bioconda&logo=Anaconda)](https://anaconda.org/bioconda/modle) [![MoDLE paper - Genome Biology 2022](https://img.shields.io/badge/CITE-Genome%20Biology%20(2022)-blue )](https://doi.org/10.1186/s13059-022-02815-7) [![Zenodo DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6424697.svg)](https://doi.org/10.5281/zenodo.6424697) --- MoDLE is a computational tool for fast, stochastic modeling of molecular contacts from DNA loop extrusion capable of simulating realistic contact patterns genome wide in a few minutes. ![Figure 1 from MoDLE\\'s paper](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs13059-022-02815-7/MediaObjects/13059_2022_2815_Fig1_HTML.png) ## Using MoDLE MoDLE is developed on Linux and tested on Linux and MacOS. Running MoDLE on Windows is only possible using Docker containers (see below). MoDLE can be installed or compiled with one of the following methods. ### Docker or Singularity/Apptainer First, make sure you follow the instructions on how to install Docker or Singularity/Apptainer on your OS. <details> <summary>Installing Docker</summary> The following instructions assume you have root/admin permissions. - [Linux](https://docs.docker.com/desktop/install/linux-install/#generic-installation-steps/) - [MacOS](https://docs.docker.com/desktop/install/mac-install/) - [Windows](https://docs.docker.com/desktop/install/windows-install/) On some Linux distributions just installing Docker is not enough. You also need to start (and optionally enable) the appropriate service(s). This is usually done with one of the following: ```bash sudo systemctl start docker sudo systemctl start docker.service ``` Refer to [Docker](https://docs.docker.com/engine/install/) or your distribution documentation for more details. </details> <details> <summary>Installing Singularity/Apptainer</summary> The following instructions assume you have root/admin permissions. Apptainer can be easily installed using your system package manager. [Here](https://apptainer.org/docs/admin/main/installation.html#install-from-pre-built-packages) you can find instructions for common Linux distributions such as Ubuntu. Even if your distribution is not listed in the above documentation, your system package manager likely includes a package for Singularity or Apptainer. If this is not the case, then you must install Apptainer from source (instructions available [here](https://github.com/apptainer/apptainer/blob/release-1.1/INSTALL.md)). </details> #### Pulling MoDLE Docker image MoDLE Docker images are available on [ghcr.io](https://github.com/paulsengroup/modle/pkgs/container/modle) and [dockerhub](https://hub.docker.com/repository/docker/paulsengroup/modle). Downloading and running the latest stable release can be done as follows: ```console # Using Docker, may require sudo user@dev:/tmp$ docker run ghcr.io/paulsengroup/modle:1.1.0 --help # Using Singularity/Apptainer user@dev:/tmp$ singularity run ghcr.io/paulsengroup/modle:1.1.0 --help High-performance stochastic modeling of DNA loop extrusion interactions. Usage: /usr/local/bin/modle [OPTIONS] SUBCOMMAND Options: -h,--help Print this help message and exit -V,--version Display program version information and exit --config Path to MoDLE\\'s config file (optional). Subcommands: simulate, sim Simulate loop extrusion and write resulting molecular contacts in a .cool file. ``` The above will print MoDLE\\'s help message, and is equivalent to running `modle --help` on the command line (assuming MoDLE is available on your machine). <details> <summary>Troubleshooting</summary> <!-- TODO: Should this section be moved elsewhere? --> __Errors regarding missing input file(s)__ Assuming you have placed the required input files inside a folder named `mydata` ```console user@dev:/tmp$ ls -lah mydata total 248K drwx------ 2 user group 80 Jan 5 12:00 . drwxrwxrwt 20 user group 540 Jan 5 12.00 .. -rw------- 1 user group 365 Jan 5 12:00 grch38.chrom.sizes -rw------- 1 user group 241K Jan 5 12:00 grch38_h1_extrusion_barriers.bed.xz ``` Running MoDLE using Docker as shown below fails due to missing input file(s) ```bash docker run ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes mydata/grch38.chrom.sizes \\\\ --extrusion-barrier-file mydata/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix mydata/output # Error message # --chrom-sizes: File does not exist: mydata/grch38.chrom.sizes # Run with --help for more information. ``` This issue is caused by Docker not mounting/binding the host filesystem (e.g. the filesystem where `mydata/` resides) inside the container. To make `mydata/` accessible from inside the container, use one of the following methods ```bash # Method 1: mydata/ folder on the host is mapped to /mydata (notice the /) inside the container docker run -v \"$(pwd -P)/mydata/:/data/\" \\\\ ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes /data/grch38.chrom.sizes \\\\ --extrusion-barrier-file /data/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix /data/output # Mehtod 2: use two different folders for input/output. Input folder is mounted in read-only mode docker run -v \"$(pwd -P)/mydata/:/input_data/:ro\" \\\\ -v \"$(pwd -P)/output/:/output_data/\" \\\\ ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes /input_data/grch38.chrom.sizes \\\\ --extrusion-barrier-file /input_data/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix /output_data/output # Methods for Singularity/Apptainer (note that manually mounting paths is usually not required when using Singularity/Apptainer) singularity run -B \"$(pwd -P)/mydata/:/data/\" \\\\ docker://ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes /data/grch38.chrom.sizes \\\\ --extrusion-barrier-file /data/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix /data/output singularity run -B \"$(pwd -P)/mydata/:/input_data/:ro\" \\\\ -B \"$(pwd -P)/output/:/output_data/\" \\\\ docker://ghcr.io/paulsengroup/modle:1.1.0 simulate \\\\ --chrom-sizes /input_data/grch38.chrom.sizes \\\\ --extrusion-barrier-file /input_data/grch38_h1_extrusion_barriers.bed.xz \\\\ --output-prefix /output_data/output ``` Output produced by Method 1: ```console user@dev:/tmp$ ls -lah mydata total 57M drwx------ 2 user group 140 Jan 5 12:20 . drwxrwxrwt 20 user group 560 Jan 5 12:20 .. -rw------- 1 user group 365 Jan 5 12:00 grch38.chrom.sizes -rw------- 1 user group 241K Jan 5 12:00 grch38_h1_extrusion_barriers.bed.xz -rw-r--r-- 1 root root 3.7K Jan 5 12:00 output_config.toml -rw-r--r-- 1 root root 2.4M Jan 5 12:05 output_lef_1d_occupancy.bw -rw-r--r-- 1 root root 57M Jan 5 12:05 output.cool -rw-r--r-- 1 root root 14K Jan 5 12:05 output.log ``` Output produced by Method 2: ```console user@dev:/tmp$ ls -lah mydata/ total 57M drwx------ 2 user group 140 Jan 5 12:20 . drwxrwxrwt 20 user group 560 Jan 5 12:20 .. -rw------- 1 user group 365 Jan 5 12:00 grch38.chrom.sizes -rw------- 1 user group 241K Jan 5 12:00 grch38_h1_extrusion_barriers.bed.xz user@dev:/tmp$ ls -lah output/ drwx------ 2 user group 140 Jan 5 12:20 . drwxrwxrwt 20 user group 560 Jan 5 12:20 .. -rw-r--r-- 1 root root 3.7K Jan 5 12:00 output_config.toml -rw-r--r-- 1 root root 2.4M Jan 5 12:05 output_lef_1d_occupancy.bw -rw-r--r-- 1 root root 57M Jan 5 12:05 output.cool -rw-r--r-- 1 root root 14K Jan 5 12:05 output.log ``` __Troubleshooting problems inside the container is a bit tedious. Any tips?__ Yes! For troubleshooting we recommend using an interactive shell running inside the container. ```bash docker run -v \"$(pwd -P)/mydata/:/input_data/:ro\" \\\\ -v \"$(pwd -P)/output/:/output_data/\" \\\\ -it --rm --entrypoint=/bin/bash \\\\ ghcr.io/paulsengroup/modle:1.1.0 ``` This will drop you in an interactive bash shell. Press `Ctrl+C`, `Ctrl+D` or type `exit` to leave the shell and stop the container. ```console root@25e8efe74294:/data# ls -lah total 4.0K drwxr-xr-x 2 root root 2 Jan 5 12:00 . drwxr-xr-x 20 root root 27 Jan 5 12:00 .. root@25e8efe74294:/data# cd /input_data/ root@25e8efe74294:/input_data# ls -lah total 250K drwx------ 2 1000 1000 80 Jan 5 12:00 . drwxr-xr-x 20 root root 27 Jan 5 12:00 .. -rw------- 1 1000 1000 365 Jan 5 12:00 grch38.chrom.sizes -rw------- 1 1000 1000 241K Jan 5 12:00 grch38_h1_extrusion_barriers.bed.xz root@25e8efe74294:/input_data# touch foo touch: cannot touch \\'foo\\': Read-only file system root@25e8efe74294:/input_data# cd /output_data/ root@25e8efe74294:/output_data# ls -lah total 2.0K drwxr-xr-x 2 1000 1000 40 Jan 5 12:00 . drwxr-xr-x 20 root root 27 Jan 5 12:00 .. root@25e8efe74294:/output_data# touch foo root@25e8efe74294:/output_data# ls -lah total 2.0K drwxr-xr-x 2 1000 1000 60 Jan 5 12:00 . drwxr-xr-x 20 root root 27 Jan 5 12:00 .. -rw-r--r-- 1 root root 0 Jan 5 12:01 foo root@25e8efe74294:/output_data# whereis modle modle: /usr/local/bin/modle root@25e8efe74294:/output_data# modle --help High-performance stochastic modeling of DNA loop extrusion interactions. Usage: modle [OPTIONS] SUBCOMMAND Options: -h,--help Print this help message and exit -V,--version Display program version information and exit --config Path to MoDLE\\'s config file (optional). Subcommands: simulate, sim Simulate loop extrusion and write resulting molecular contacts in a .cool file. root@25e8efe74294:/output_data# exit exit ``` For Singularity/Apptainer this is even easier: ```bash singularity shell docker://ghcr.io/paulsengroup/modle:1.1.0 ``` </details> ### Conda (bioconda) MoDLE package for Linux and MacOS is available on [bioconda](https://anaconda.org/bioconda/modle) and can be installed as follows: ```console user@dev:/tmp$ conda create -n modle -c conda-forge -c bioconda modle (modle) user@dev:/tmp$ conda activate modle (modle) user@dev:/tmp$ whereis modle modle: /home/user/.miniconda3/envs/modle/bin/modle (modle) user@dev:/tmp$ modle --version MoDLE-v1.1.0-bioconda ``` ### Pre-built Linux binaries (not recommended) Download file `modle-1.1.0-x86_64-linux.tar.xz` from the [latest release](https://github.com/paulsengroup/modle/releases/latest). Extract the archive and copy `modle` and/or `modle_tools` to a location in your `PATH` <details> <summary> click to expand </summary> <!-- TODO make sure the link is correct once the release goes live --> ```console user@dev:/tmp$ curl -LO \\'https://github.com/paulsengroup/modle/releases/download/v1.1.0/modle-1.1.0-x86_64-linux.tar.xz\\' user@dev:/tmp$ tar -xvf modle-1.1.0-x86_64-linux.tar.xz modle-1.1.0-x86_64-linux/ modle-1.1.0-x86_64-linux/share/ modle-1.1.0-x86_64-linux/share/licenses/ modle-1.1.0-x86_64-linux/share/licenses/modle_tools/ modle-1.1.0-x86_64-linux/share/licenses/modle_tools/LICENSE modle-1.1.0-x86_64-linux/share/licenses/modle/ modle-1.1.0-x86_64-linux/share/licenses/modle/LICENSE modle-1.1.0-x86_64-linux/bin/ modle-1.1.0-x86_64-linux/bin/modle_tools modle-1.1.0-x86_64-linux/bin/modle user@dev:/tmp modle-1.1.0-x86_64-linux/bin/modle --version MoDLE-v1.1.0 # Optional: add modle and modle_tools to your PATH (please ensure $HOME/.local/bin/ is in your PATH) user@dev:/tmp$ install -Dm0755 modle-1.1.0-x86_64-linux/bin/modle \"$HOME/.local/bin/\" user@dev:/tmp$ install -Dm0755 modle-1.1.0-x86_64-linux/bin/modle_tools \"$HOME/.local/bin/\" user@dev:/tmp$ whereis modle modle: /home/user/.local/bin/modle user@dev:/tmp$ modle --version MoDLE-v1.1.0 ``` </details> ### Installing from source MoDLE can be compiled on most UNIX-like systems, including many Linux distributions and MacOS (10.15+). [Build instructions](https://github.com/paulsengroup/modle/wiki/Building-from-source) ### Running MoDLE Here we assume MoDLE was correctly installed and is available in your PATH (i.e. running `whereis modle` prints something like `modle: /usr/local/bin/modle`). If you are using Docker or Singularity/Apptainer, please make sure input files can be reached from within the container (see troubleshooting steps from [Using MoDLE with Docker or Singularity/Apptainer](https://github.com/paulsengroup/modle/tree/update-readme#docker-or-singularityapptainer). Folder `examples/data/` from this repository contains the input files used throughout this section. <details> <summary> Data provenance </summary> - `examples/data/hg38.chrom.sizes` was generated from the [hg38.chrom.sizes](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.chrom.sizes) released by UCSC. Chromosmes were filtered and sorted using `grep -E \\'chr[0-9XY]+[[:space:]]\\' hg38.chrom.sizes | sort -V` - `examples/data/hg38_extrusion_barriers.bed.xz` was generated as described the Methods of MoDLE\\'s [paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02815-7) (third paragraph of section \"MoDLE simulations\"). Refer to code hosted on [github.com/paulsengroup/2021-modle-paper-001-data-analysis](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis) for more details (useful links: [link1](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis/blob/main/workflows/preprocess_data.nf#L41-L100), [link2](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis/blob/main/configs/preprocess_data.config), [link3](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis/blob/main/data/download_list.txt), [link4](https://github.com/paulsengroup/2021-modle-paper-001-data-analysis/blob/main/scripts/convert_chip_signal_to_occupancy.py). Feel free to get in touch if you are struggling to navigate code hosted in the data analysis repository). </details> #### Required input files Running a simulation with default settings only requires two input files: - A [chrom.sizes](https://software.broadinstitute.org/software/igv/chromSizes) file with the list of chromosomes to be simulated - A [BED file](https://bedtools.readthedocs.io/en/latest/content/general-usage.html#bed-format) with the list of extrusion barriers to include as part of the simulation Input files using common compression formats (namely Bzip2, Gzip, LZ4, xz/LZMA and zstd) are supported. The extrusion barrier BED file should have at least the first 6 columns defined (i.e. chrom, start, end, name, score and strand. See [bedtools docs](https://bedtools.readthedocs.io/en/latest/content/general-usage.html#bed-format) for more details). Some remarks: - ___chrom___ should contain the name of one of the chromosome defined in the `.chrom.sizes` - The ___start___ and ___end___ fields are used to position barriers along chromosomes. MoDLE models extrusion barriers as 1bp entities. Thus, when $end - start \\\\gt 1$ the extrusion barrier will be placed in the middle of interval $[start, end)$. - The ___name___ field is required but ignored, and can be safely set to `.` - ___score___ is required and should be set to a number between 0 and 1. This number expresses the average occupancy of the extrusion barrier site defined by the current line. Occupancies between 0.7-0.9 can be used as a starting point. Check out [this](https://github.com/paulsengroup/modle/wiki/%5BTutorial%5D---Generating-a-list-of-extrusion-barrier-from-ChIP-seq-data) tutorial to learn how barrier occupancy can be inferred from CTCF/RAD21 ChIP-Seq data. More information regarding how extrusion barriers are modeled canm be found in [MoDLE\\'s paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02815-7) and [suppl. text](https://static-content.springer.com/esm/art%3A10.1186%2Fs13059-022-02815-7/MediaObjects/13059_2022_2815_MOESM1_ESM.pdf). - ___strand___ is required and is used to define the extrusion barrier direction (should be one of `-`, `+` or `.`). As of MoDLE v1.1.0, extrusion barriers are modeled after CTCF barriers. Thus, the strand field should be populated with the direction of the CTCF binding site that is being defined. Barriers without strand information (i.e. with strand `.`) are ignored. #### Running a simulation with default settings ```console user@dev:/tmp/modle$ modle simulate \\\\ --chrom-sizes examples/data/hg38.chrom.sizes \\\\ --extrusion-barrier-file examples/data/hg38_extrusion_barriers.bed.xz \\\\ --output-prefix examples/out/hg38_default [2023-05-11 11:47:15.718] [info]: running MoDLE v1.1.0 [2023-05-11 11:47:15.718] [info]: complete log will be written to file \"examples/out/hg38_default.log\" [2023-05-11 11:47:15.718] [info]: writing simulation parameters to config file \"examples/out/hg38_default_config.toml\" [2023-05-11 11:47:15.718] [info]: command: modle simulate --chrom-sizes examples/data/hg38.chrom.sizes --extrusion-barrier-file examples/data/hg38_extrusion_barriers.bed.xz --output-prefix examples/out/hg38_default [2023-05-11 11:47:15.719] [info]: simulation will use up to 16 out of 16 available CPU cores. [2023-05-11 11:47:15.719] [info]: using --target-contact-density=1.00 as stopping criterion. [2023-05-11 11:47:15.719] [info]: contact sampling strategy: tad-plus-loop-with-noise. [2023-05-11 11:47:15.719] [info]: contact matrix resolution: 5000bp [2023-05-11 11:47:15.719] [info]: importing chromosomes from \"examples/data/hg38.chrom.sizes\"... [2023-05-11 11:47:15.719] [info]: imported 24 chromosomes in 239.985us. [2023-05-11 11:47:15.719] [info]: importing extrusion barriers from \"examples/data/hg38_extrusion_barriers.bed.xz\"... [2023-05-11 11:47:15.761] [info]: imported 38815 barriers from \"examples/data/hg38_extrusion_barriers.bed.xz\" in 41.776052ms. [2023-05-11 11:47:15.762] [info]: spawning worker thread W0... [2023-05-11 11:47:15.762] [info]: spawning worker thread W1... [2023-05-11 11:47:15.762] [info]: spawning worker thread W3... ... [2023-05-11 11:47:15.764] [info]: begin processing chr1:0-248956422: simulating ~37485 epochs across 512 cells using 4979 LEFs and 3518 barriers (~73 epochs per cell)... [2023-05-11 11:47:37.627] [info]: begin processing chr2:0-242193529: simulating ~37501 epochs across 512 cells using 4844 LEFs and 2974 barriers (~73 epochs per cell)... [2023-05-11 11:47:43.592] [info]: [io] writing contacts for chr1:0-248956422 to file \"examples/out/hg38_default.cool\"... [2023-05-11 11:47:46.182] [info]: [io]: written 29875200 contacts for chr1:0-248956422 to file \"examples/out/hg38_default.cool\" in 2.590415163s (3.47M nnz out of 29.88M pixels). [2023-05-11 11:47:46.186] [info]: [io]: writing 1D LEF occupancy profile for chr1:0-248956422 to file \"examples/out/hg38_default_lef_1d_occupancy.bw\"... [2023-05-11 11:47:46.196] [info]: [io]: writing 1D LEF occupancy profile for chr1:0-248956422 to file \"examples/out/hg38_default_lef_1d_occupancy.bw\" took 9.964882ms [2023-05-11 11:47:57.200] [info]: begin processing chr3:0-198295559: simulating ~37474 epochs across 512 cells using 3966 LEFs and 2469 barriers (~73 epochs per cell)... ... [2023-05-11 11:51:49.400] [info]: [io] writing contacts for chrY:0-57227415 to file \"examples/out/hg38_default.cool\"... [2023-05-11 11:51:49.744] [info]: [io]: written 6867600 contacts for chrY:0-57227415 to file \"examples/out/hg38_default.cool\" in 344.473166ms (0.92M nnz out of 6.87M pixels). [2023-05-11 11:51:49.744] [info]: [io]: writing 1D LEF occupancy profile for chrY:0-57227415 to file \"examples/out/hg38_default_lef_1d_occupancy.bw\"... [2023-05-11 11:51:49.746] [info]: [io]: writing 1D LEF occupancy profile for chrY:0-57227415 to file \"examples/out/hg38_default_lef_1d_occupancy.bw\" took 1.565303ms', '[2023-05-11 11:51:51.925] [info]: simulation terminated without errors in 4m36.206746164s! Bye. ``` The above command will create folder `examples/out/` (if it doesn\\'t already exist), and write the following files inside it: ``` examples/out \u251c\u2500\u2500 hg38_default_config.toml \u251c\u2500\u2500 hg38_default.cool \u251c\u2500\u2500 hg38_default_lef_1d_occupancy.bw \u2514\u2500\u2500 hg38_default.log ``` Output files produced by the above command are available [here](https://doi.org/10.5281/zenodo.7924253). Simulated interactions are written to file `hg38_default.cool`. More information about the .cool format is available at [open2c/cooler](https://github.com/open2c/cooler). In case any of the output files already exist, MoDLE will refuse to run and print an error message listing the file name collisions. Passing the `--force` flag overrides this behavior and will cause MoDLE to overwrite existing files. To visualize `.cool` files we recommend using [cooler show](https://open2c.github.io/cooler/) or [higlass](https://docs.higlass.io/) (more specifically [higlass-docker](https://github.com/higlass/higlass-docker) or [higlass-manage](https://github.com/higlass/higlass-manage)). <details> <summary> <b> Tips for visualization </b> </summary> ##### Quickly visualizing interactions with Cooler ```bash cooler show examples/out/hg38_default.cool chr1:5000000-8000000 ``` ![chr1 5-8Mbp heatmap](examples/images/hg38_default_001.avif) ##### Visualizing interactions with HiGlass Before visualizing heatmaps with HiGlass, the single-resolution `.cool` file produced by MoDLE needs to be converted to a multi-resolution Cooler (`.mcool`) file. This is easily done with `cooler zoomify`: ```console user@dev:/tmp/modle$ cooler zoomify examples/out/hg38_default.cool INFO:cooler.cli.zoomify:Recursively aggregating \"examples/out/hg38_default.cool\" INFO:cooler.cli.zoomify:Writing to \"examples/out/hg38_default.mcool\" INFO:cooler.reduce:Copying base matrices and producing 12 new zoom levels. INFO:cooler.reduce:Bin size: 5000 INFO:cooler.reduce:Aggregating from 5000 to 10000. INFO:cooler.create:Creating cooler at \"examples/out/hg38_default.mcool::/resolutions/10000\" INFO:cooler.create:Writing chroms INFO:cooler.create:Writing bins INFO:cooler.create:Writing pixels INFO:cooler.reduce:0 10000066 INFO:cooler.reduce:10000066 20000079 INFO:cooler.reduce:20000079 30000056 INFO:cooler.reduce:30000056 40000018 INFO:cooler.reduce:40000018 43586992 INFO:cooler.create:Writing indexes INFO:cooler.create:Writing info INFO:cooler.reduce:Aggregating from 10000 to 20000. ... ``` Next, you need to start HiGlass server. If you are using `higlass-manage`, you may have to prefix commands with `sudo` or `sudo -E`. ```console # Using higlass-docker user@dev:/tmp/modle$ sudo docker pull higlass/higlass-docker Using default tag: latest latest: Pulling from higlass/higlass-docker Digest: sha256:44086069ee7d4d3f6f3f0012569789ec138f42b84aa44357826c0b6753eb28de Status: Image is up to date for higlass/higlass-docker:latest docker.io/higlass/higlass-docker:latest user@dev:/tmp/modle$ sudo docker run \\\\ --detach \\\\ --publish 8989:80 \\\\ --volume ~/hg-data:/data \\\\ --volume ~/hg-tmp:/tmp \\\\ --name higlass-modle \\\\ higlass/higlass-docker eb3fe07ea23b57eb844a590e7f1e939cf82a11e5d25db87c4b25024a41c9a546 # Using higlass-manage user@dev:/tmp/modle$ sudo higlass-manage start --use-redis --no-public-data Stopping previously running container Attempting to remove existing Docker network instance Stopping previously running Redis container Pulling redis:5.0.3-alpine done Pulling latest image... done Data directory: /home/user/hg-data Temp directory: () Starting... default 8989 Docker started: higlass-manage-container-default sending request 1 Waiting to start (tilesets)... sending request 2 ... public_data: False ret: b\\'{\"uid\": \"default_local\"}\\' ret: ExecResult(exit_code=0, output=b\\'\\') Replaced js file Started ``` Now if you visit [localhost:8989/app](http://localhost:8989/app) with your browser you should see an empty HiGlass page like the following: ![higlass empty page](examples/images/higlass_001.avif) Next, we need to ingest our `.mcool` into HiGlass: ```console user@dev:/tmp/modle$ mkdir ~/hg-data/media/modle user@dev:/tmp/modle$ cp examples/out/hg38_default.mcool ~/hg-data/media/modle # With higlass-docker user@dev:/tmp/modle$ sudo docker exec higlass-modle \\\\ python higlass-server/manage.py ingest_tileset \\\\ --project-name \"modle\" \\\\ --filename modle/hg38_default.mcool \\\\ --filetype cooler \\\\ --datatype matrix \\\\ --no-upload # With higlass-manage user@dev:/tmp/modle$ sudo higlass-manage ingest --project-name=modle modle/hg38_default.mcool --no-upload state True Inferred filetype: cooler Inferred datatype: matrix state True name_text: hg_name: default command: python higlass-server/manage.py ingest_tileset --filename modle/hg38_default.mcool --filetype cooler --datatype matrix --project-name \"modle\" --no-upload --uid emFfbA9tTCiBTt6b5vTG8Q ``` Finally, after refreshing page [localhost:8989/app](http://localhost:8989/app) you should be able to add tileset `hg38_default.mcool` to the center panel: ![higlass default settings](examples/images/higlass_002.avif) In case you are struggling to visualize MoDLE\\'s output feel free to reach out by starting a new [discussion](https://github.com/paulsengroup/modle/discussions). </details> <details> <summary> <b> Tips for programmatic access to Cooler files </b> </summary> Programmatic access to contacts stored in Cooler files is possible using the `cooler` [CLI interface](https://cooler.readthedocs.io/en/latest/cli.html) or Python [API](https://cooler.readthedocs.io/en/latest/api.html). ##### CLI interface ```console # Dumping interactions in BEDPE format user@dev:/tmp/modle$ cooler dump --table pixels --join examples/out/hg38_default.cool | head chr1 0 5000 chr1 0 5000 9 chr1 0 5000 chr1 5000 10000 5 chr1 0 5000 chr1 10000 15000 2 chr1 0 5000 chr1 15000 20000 2 chr1 0 5000 chr1 20000 25000 1 chr1 0 5000 chr1 25000 30000 2 chr1 0 5000 chr1 30000 35000 3 chr1 0 5000 chr1 35000 40000 1 chr1 0 5000 chr1 40000 45000 2 chr1 0 5000 chr1 45000 50000 1 user@dev:/tmp/modle$ cooler dump --table pixels --join examples/out/hg38_default.mcool::/resolutions/200000 | head chr1 0 20000 chr1 0 20000 114 chr1 0 20000 chr1 20000 40000 120 chr1 0 20000 chr1 40000 60000 69 chr1 0 20000 chr1 60000 80000 59 chr1 0 20000 chr1 80000 100000 34 chr1 0 20000 chr1 100000 120000 28 chr1 0 20000 chr1 120000 140000 25 chr1 0 20000 chr1 140000 160000 18 chr1 0 20000 chr1 160000 180000 20 chr1 0 20000 chr1 180000 200000 12 ``` ##### Python API ```python-console In [1]: import cooler In [2]: c = cooler.Cooler(\"examples/out/hg38_default.cool\") In [3]: selector = c.matrix(balance=False) In [4]: selector.fetch(\"chr1:0-50000\") Out[4]: array([[ 9, 5, 2, 2, 1, 2, 3, 1, 2, 1], [ 5, 9, 9, 15, 3, 3, 8, 1, 8, 1], [ 2, 9, 12, 26, 11, 7, 3, 8, 5, 2], [ 2, 15, 26, 25, 27, 16, 12, 14, 10, 7], [ 1, 3, 11, 27, 20, 42, 30, 17, 15, 10], [ 2, 3, 7, 16, 42, 21, 27, 17, 15, 12], [ 3, 8, 3, 12, 30, 27, 26, 27, 26, 17], [ 1, 1, 8, 14, 17, 17, 27, 23, 44, 23], [ 2, 8, 5, 10, 15, 15, 26, 44, 31, 35], [ 1, 1, 2, 7, 10, 12, 17, 23, 35, 23]], dtype=int32) In [5]: selector = c.matrix(balance=False, as_pixels=True, join=True) In [6]: selector.fetch(\"chr1:0-50000\") Out[6]: chrom1 start1 end1 chrom2 start2 end2 count 0 chr1 0 5000 chr1 0 5000 9 1 chr1 0 5000 chr1 5000 10000 5 2 chr1 0 5000 chr1 10000 15000 2 3 chr1 0 5000 chr1 15000 20000 2 4 chr1 0 5000 chr1 20000 25000 1 5 chr1 0 5000 chr1 25000 30000 2 6 chr1 0 5000 chr1 30000 35000 3 7 chr1 0 5000 chr1 35000 40000 1 8 chr1 0 5000 chr1 40000 45000 2 9 chr1 0 5000 chr1 45000 50000 1 ... ``` Refer to Cooler [documentation](https://cooler.readthedocs.io/en/latest/quickstart.html) for more details. </details> #### Running a simulation using config files The `hg38_default_config.toml` file produced by MoDLE in the [previous section](#running-a-simulation-with-default-settings) is a config file that can be used to repeat simulations using the same set of parameters and input files. Information found in config files is also embedded in the `.cool` file as a JSON string, and can be extracted using `cooler info`: <!-- TODO: Update once https://github.com/paulsengroup/modle/issues/75 is implemented --> ```console user@dev:/tmp/modle$ cooler info --metadata examples/out/hg38_default.cool { \"simulate\": { \"avg-lef-processivity\": 300000, \"burnin-extr-speed-coefficient\": 1, \"burnin-history-length\": 100, \"burnin-smoothing-window-size\": 5, \"burnin-target-epochs-for-lef-activation\": 320, \"chrom-sizes\": \"examples/data/hg38.chrom.sizes\", ... ``` The following will run a simulation with the same settings as the simulation from the [previous example](#Running-a-simulation-with-default-settings) ```bash modle simulate --config examples/out/hg38_default_config.toml --force ``` Parameters read from a config file have lower priority than parameter specified through the CLI, meaning that CLI options can be used to override parameters read from the config file. This can be useful when running a batch of simulations using a large number of optional parameters, and where only a handful of parameters need to change across simulation runs. The following command will run a simulation using parameters from the [previous example](#Running-a-simulation-with-default-settings) as starting point, but using a different number of cells and target contact density. ```bash modle simulate \\\\ --config examples/out/hg38_default_config.toml \\\\ --ncells 16 \\\\ --target-contact-density 10 \\\\ --output-prefix examples/out/hg38_deep ``` Output files produced by the above command are available [here](https://doi.org/10.5281/zenodo.7924253). ![Simulation w/ high contact density](examples/images/higlass_003.avif) MoDLE\\'s config files are in [TOML format](https://toml.io/en/). Adding a line like `my-option=my_value` to the config file has the same effect as passing `--my-option=my_value` on the CLI. For an up-to-date list of supported CLI options, please refer to MoDLE\\'s help message: ```console user@dev:/tmp/modle$ modle simulate --help Simulate loop extrusion and write resulting molecular contacts in a .cool file. Usage: modle simulate [OPTIONS] Options: -h,--help Print this help message and exit [Option Group: IO] Options controlling MoDLE input, output and logs. Options: -c,--chrom-sizes REQUIRED Path to file with chromosome sizes in chrom.sizes format. -b,--extrusion-barrier-file REQUIRED Path to a file in BED6+ format with the genomic coordinates of extrusion barriers to be simulated. The score field in a BED record should be a number between 0 and 1 and is interpreted as the extrusion barrier occupancy for the extrusion barrier described by the record. Barriers mapping on chromosomes not listed in the chrom.sizes file passed through the --chrom-sizes option are ignored. -g,--genomic-intervals Path to BED3+ file with the genomic regions to be simulated. Intervals listed in this file should be a subset of the chromosomes defined in the .chrom.sizes files. Intervals referring to chromosomes not listed in the .chrom.sizes file are ignored. -f,--force Overwrite existing files (if any). -o,--output-prefix REQUIRED Output prefix. Can be an absolute or relative path including the file name but without the extension. Example: running modle sim -o /tmp/my_simulation ... yields the following files: - /tmp/my_simulation.cool - /tmp/my_simulation.log - /tmp/my_simulation_config.toml - /tmp/my_simulation_lef_1d_occupancy.bw --assembly-name=unknown Name of the genome assembly to be simulated. This is only used to populate the \"assembly\" attribute in the output .cool file. ... ``` ## Citing If you use MoDLE in your research, please cite the following paper: Rossini, R., Kumar, V., Mathelier, A. _et al._ MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions. _Genome Biol_ __23__, 247 (2022). [https://doi.org/10.1186/s13059-022-02815-7](https://doi.org/10.1186/s13059-022-02815-7) <details> <summary>BibTex</summary> ```bibtex @article{rossini_2022, author = {Rossini, Roberto and Kumar, Vipin and Mathelier, Anthony and Rognes, Torbj\u00f8rn and Paulsen, Jonas}, doi = {10.1186/s13059-022-02815-7}, journal = {Genome Biology}, month = {11}, title = {{MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions}}, url = {https://doi.org/10.1186/s13059-022-02815-7}, volume = {23}, year = {2022} } ``` </details> In addition to the above reference, you can also cite specific version(s) of MoDLE using one of the DOIs from Zenodo: [10.5281/zenodo.6424697](https://doi.org/10.5281/zenodo.6424697)']```"}, {"role": "user", "content": "Content from https://github.com/paulsengroup/MoDLE:\n```\nRepository metadata:\n{'name': 'modle', 'label': ['modle', 'MoDLE'], 'description': [{'term': 'High-performance stochastic modeling of DNA loop extrusion interactions', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://doi.org/10.1186/s13059-022-02815-7', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.1.0', 'v1.0.1', 'v1.0.0', 'v1.0.0-rc.7', 'v1.0.0-rc.6', 'v1.0.0-rc.5', 'v1.0.0-rc.4', 'v1.0.0-rc.3', 'v1.0.0-rc.2', 'v1.0.0-rc.1'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}, {'term': {'name': 'MIT', 'url': ''}, 'id': 1}], 'repository': [{'term': 'https://github.com/paulsengroup/modle', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/biology', 'term': 'biology', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/cpp', 'term': 'cpp', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/cpp17', 'term': 'cpp17', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/dna', 'term': 'dna', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/high-performance', 'term': 'high-performance', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/loop-extrusion', 'term': 'loop-extrusion', 'vocabulary': ''}, 'id': 6}, {'term': {'uri': 'https://github.com/topics/stochastic-model', 'term': 'stochastic-model', 'vocabulary': ''}, 'id': 7}, {'term': {'uri': 'https://github.com/topics/tad', 'term': 'tad', 'vocabulary': ''}, 'id': 8}, {'term': {'uri': 'https://github.com/topics/topologically-associated-domains', 'term': 'topologically-associated-domains', 'vocabulary': ''}, 'id': 9}, {'term': {'uri': 'https://github.com/topics/hic', 'term': 'hic', 'vocabulary': ''}, 'id': 10}], 'operations': [], 'authors': [{'term': {'name': 'Roberto Rossini', 'email': '', 'maintainer': False, 'type': 'person'}, 'id': 0}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/paulsengroup/MoDLE/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/paulsengroup/MoDLE/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/paulsengroup/MoDLE/blob/main/conanfile.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [{'term': {'title': 'MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions', 'year': '2022', 'doi': '10.1186/s13059-022-02815-7', 'url': 'https://doi.org/10.1186/s13059-022-02815-7'}, 'id': 0}], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': '', 'citation': \"# Copyright (C) 2022 Roberto Rossini <roberros@uio.no>\\n#\\n# SPDX-License-Identifier: MIT\\n\\ncff-version: 1.2.0\\nmessage: 'If you use this software, please cite it using the metadata from this file.'\\nauthors:\\n  - given-names: Roberto\\n    family-names: Rossini\\n    orcid: 'https://orcid.org/0000-0003-3096-1470'\\n    email: roberros@uio.no\\n    affiliation: 'Department of Biosciences, University of Oslo'\\ntitle: MoDLE\\nabstract: 'High-performance stochastic modeling of DNA loop extrusion interactions.'\\ndoi: '10.5281/zenodo.6424697'\\nurl: 'https://github.com/paulsengroup/modle'\\nrepository-code: 'https://github.com/paulsengroup/modle'\\ntype: software\\nlicense: MIT\\npreferred-citation:\\n  type: article\\n  authors:\\n  - given-names: Roberto\\n    family-names: Rossini\\n    orcid: 'https://orcid.org/0000-0003-3096-1470'\\n    email: roberros@uio.no\\n    affiliation: 'Department of Biosciences, University of Oslo'\\n  - given-names: Vipin\\n    family-names: Kumar\\n    email: vipin.kumar@ncmm.uio.no\\n    affiliation: 'Centre for Molecular Medicine Norway (NCMM), Nordic EMBL Partnership, University of Oslo'\\n  - given-names: Anthony\\n    family-names: Mathelier\\n    orcid: 'https://orcid.org/0000-0001-5127-5459'\\n    email: anthony.mathelier@ncmm.uio.no\\n    affiliation: 'Centre for Molecular Medicine Norway (NCMM), Nordic EMBL Partnership, University of Oslo'\\n  - given-names: Torbj\u00f8rn\\n    family-names: Rognes\\n    orcid: 'https://orcid.org/0000-0002-9329-9974'\\n    email: torognes@ifi.uio.no\\n    affiliation: 'Centre for Bioinformatics, Department of Informatics, University of Oslo'\\n  - given-names: Jonas\\n    family-names: Paulsen\\n    orcid: 'https://orcid.org/0000-0002-7918-5495'\\n    email: jonas.paulsen@ibv.uio.no\\n    affiliation: 'Department of Biosciences, University of Oslo'\\n  doi: '10.1186/s13059-022-02815-7'\\n  url: 'https://doi.org/10.1186/s13059-022-02815-7'\\n  journal: 'Genome Biology'\\n  year: 2022\\n  month: 11\\n  volume: 23\\n  number: 1\\n  pages: 247\\n  issn: '1474-760X'\\n  title: 'MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions'\\n  abstract: 'DNA loop extrusion emerges as a key process establishing genome structure and function. We introduce MoDLE, a computational tool for fast, stochastic modeling of molecular contacts from DNA loop extrusion capable of simulating realistic contact patterns genome wide in a few minutes. MoDLE accurately simulates contact maps in concordance with existing molecular dynamics approaches and with Micro-C data and does so orders of magnitude faster than existing approaches. MoDLE runs efficiently on machines ranging from laptops to high performance computing clusters and opens up for exploratory and predictive modeling of 3D genome structure in a wide range of settings.'\\n\"}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"mapsplice/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/mapsplice/cmd/None\",\n    \"name\": \"mapsplice\",\n    \"description\": [\n      \"Highly sensitive and specific tool in the detection of splices as well as CPU and memory efficiency. It can be applied to both short (<75 bp) and long reads (>=75 bp) and it is not dependent on splice site features or intron length, consequently it can detect novel canonical as well as non-canonical splices. The tool leverages the quality and diversity of read alignments of a given splice to increase accuracy.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.netlab.uky.edu/p/bioinfo/MapSplice\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Support\",\n        \"email\": \"mapsplice@netlab.uky.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c828c310e2f3aa0b3b7f1b\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/mapsplice/cmd/2.2.1\",\n    \"name\": \"mapsplice\",\n    \"description\": [\n      \"MapSplice is a software for mapping RNA-seq data to reference genome for splice junction discovery that depends only on reference genome, and not on any further annotations.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.netlab.uky.edu/p/bioinfo/MapSplice2\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Custom\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"razers3/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/razers3/cmd/None\",\n    \"name\": \"razers3\",\n    \"description\": [\n      \"RazerS 3 is a tool for mapping millions of short genomic reads onto a\\nreference genome. It was designed with focus on mapping next-generation\\nsequencing reads onto whole DNA genomes. RazerS 3 searches for matches of\\nreads with a percent identity above a given threshold (-i X), whereby it\\ndetects alignments with mismatches as well as gaps.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/seqan/seqan/tree/master/apps/razers3\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/razers3/cmd/3.5.8\",\n    \"name\": \"razers3\",\n    \"description\": [\n      \"RazerS 3 - Faster, fully sensitive read mapping.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.seqan.de/projects/razers/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://www.seqan.de/projects/razers/:\n```\nContent:\n['## Abstract **Motivation:** During the last years NGS sequencing has become a key technology for many applications in the biomedical sciences. Throughput continues to increase and new protocols provide longer reads than currently available. In almost all applications, read mapping is a first step. Hence, it is crucial to have algorithms and implementations that perform fast, with high sensitivity, and are able to deal with long reads and a large absolute number of indels. **Results:** RazerS is a read mapping program with adjustable sensitivity based on counting q-grams. In this work we propose the successor RazerS 3 which now supports shared-memory parallelism, an additional seed-based filter with adjustable sensitivity, a much faster, banded version of the Myers\u2019 bit-vector algorithm for verification, memory saving measures and support for the SAM output format. This leads to a much improved performance for mapping reads, in particular long reads with many errors. We extensively compare RazerS 3 with other popular read mappers and show that its results are often superior to them in terms of sensitivity while exhibiting practical and often competetive run times. In addition, RazerS 3 works without a precomputed index. **Main Features:** * import of FASTA/FASTQ read and genome files * 5 output formats (including SAM) * reads can be of arbitrary length * supports Hamming and edit distance read mapping with configurable error rates * supports paired-end read mapping * configurable and predictable sensitivity (runtime/sensitivity tradeoff) * key improvements (compared to RazerS):multicore parallelizationadditional pigeonhole filter optimized for low error-rates with controllable sensitivitybanded Myers\u2019 algorithm for verificationfull sensitivity under the definition given in RabemaSAM outputAvailability and Implementation: Source code and binaries are freely available for download at https://www.seqan.de/projects/razers. RazerS 3 is implemented in C++ and OpenMP under a GPL license using the SeqAn library and supports Linux, Mac OS X, and Windows. ## Links ## Please Cite * D. Weese, M. Holtgrewe, K. Reinert,[\u201cRazerS 3: Faster, fully sensitive read mapping\u201d](http://bioinformatics.oxfordjournals.org/content/28/20/2592), vol. 28, iss. 20, 2012-08-24.cite this publication@article{fu_mi_publications1159,abstract={Motivation: During the last years NGS sequencing has become a key technology for many applications in the biomedical sciences. Throughput continues to increase and new protocols provide longer reads than currently available. In almost all applications, read mapping is a first step. Hence, it is crucial to have algorithms and implementations that perform fast, with high sensitivity, and are able to deal with long reads and a large absolute number of indels. Results: RazerS is a read mapping program with adjustable sensitivity based on counting q-grams. In this work we propose the successor RazerS 3 which now supports shared-memory parallelism, an additional seed-based filter with adjustable sensitivity, a much faster, banded version of the Myers? bit-vector algorithm for verification, memory saving measures and support for the SAM output format. This leads to a much improved performance for mapping reads, in particular long reads with many errors. We extensively compare RazerS 3 with other popular read mappers and show that its results are often superior to them in terms of sensitivity while exhibiting practical and often competetive run times. In addition, RazerS 3 works without a precomputed index. Availability and Implementation: Source code and binaries are freely available for download at http://www.seqan.de/projects/razers. RazerS 3 is implemented in C++ and OpenMP under a GPL license using the SeqAn library and supports Linux, Mac OS X, and Windows.},author={D. Weese and M. Holtgrewe and K. Reinert},journal={Bioinformatics},month={August},number={20},pages={2592--2599},publisher={Oxford University Press},title={RazerS 3: Faster, fully sensitive read mapping},url={http://publications.imp.fu-berlin.de/1159/},volume={28},year={2012}} * D. Weese, A.-K. Emde, T. Rausch, A. D\u00f6ring, K. Reinert,[\u201cRazerS - Fast Read Mapping with Sensitivity Control\u201d](http://genome.cshlp.org/content/19/9/1646.abstract), vol. 19, iss. 9, 2009-07-10.cite this publication@article{fu_mi_publications453,abstract={Second-generation sequencing technologies deliver DNA sequence data at unprecedented high throughput. Common to most biological applications is a mapping of the reads to an almost identical or highly similar reference genome. Due to the large amounts of data, e?cient algorithms and implementations are crucial for this task. We present an e?cient read mapping tool called RazerS. It allows the user to align sequencing reads of arbitrary length using either the Hamming distance or the edit distance. Our tool can work either lossless or with a user-de?ned loss rate at higher speeds. Given the loss rate, we present an approach that guarantees not to lose more reads than speci?ed. This enables the user to adapt to the problem at hand and provides a seamless tradeo? between sensitivity and running time.},author={D. Weese and A.-K. Emde and T. Rausch and A. D{\\\\\"o}ring and K. Reinert},journal={Genome Research},month={July},number={9},pages={1646--1654},title={RazerS - Fast Read Mapping with Sensitivity Control},url={http://publications.imp.fu-berlin.de/453/},volume={19},year={2009}} ## Contact For questions, comments, or suggestions please contact: David Weese [david.weese@fu-berlin.de](mailto:david.weese@fu-berlin.de)']```"}, {"role": "user", "content": "Content from https://github.com/seqan/seqan/tree/master/apps/razers3:\n```\nREADME content:\n['# SeqAn - The Library for Sequence Analysis [![build status][1]][2] [![license][3]][4] [![latest release][5]][6] [![platforms][7]][8] [![start][9]][10] [![twitter][11]][12] [1]: https://img.shields.io/github/actions/workflow/status/seqan/seqan/ci_linux.yml?branch=main&style=flat&logo=github&label=SeqAn%20CI \"Open GitHub actions page\" [2]: https://github.com/seqan/seqan/actions?query=branch%3Amain [3]: https://img.shields.io/badge/license-BSD-green.svg \"Open license file\" [4]: https://github.com/seqan/seqan/blob/main/LICENSE [5]: https://img.shields.io/github/release/seqan/seqan.svg \"Get the latest release\" [6]: https://github.com/seqan/seqan/releases/latest [7]: https://img.shields.io/badge/platform-linux%20%7C%20bsd%20%7C%20osx%20%7C%20win-informational.svg \"Open our API documentation\" [8]: https://docs.seqan.de/seqan/main/ [9]: https://img.shields.io/github/stars/seqan/seqan.svg?style=social \"See who starred us\" [10]: https://github.com/seqan/seqan/stargazers [11]: https://img.shields.io/twitter/follow/SeqAnLib.svg?label=follow&style=social \"Follow us on Twitter\" [12]: https://twitter.com/seqanlib | **NOTE <br> [SeqAn3 is out and hosted in a different repository](https://github.com/seqan/seqan3)** | |:----------------------------------------------------------------------------------------------------:| | We recommend using SeqAn3 for new applications. | ## What Is SeqAn? SeqAn is an open source C++ library of efficient algorithms and data structures for the analysis of sequences with the focus on biological data. Our library applies a unique generic design that guarantees high performance, generality, extensibility, and integration with other libraries. SeqAn is easy to use and simplifies the development of new software tools with a minimal loss of performance. ## License The SeqAn library itself, the tests and demos are licensed under the very permissive 3-clause BSD License. The licenses for the applications themselves can be found in the LICENSE files. ## Prerequisites Older compiler versions might work but are neither supported nor tested. ### Linux, macOS, FreeBSD * GCC \u2265 12 * Clang/LLVM \u2265 17 * Intel oneAPI C++ Compiler 2025.0 (IntelLLVM) ### Windows * Visual C++ \u2265 17.0 / Visual Studio \u2265 2022 ### Architecture support * Intel/AMD platforms, including optimisations for modern instruction sets (`POPCNT`, `SSE4`, `AVX2`, `AVX512`) * All Debian release architectures supported, including most ARM and all PowerPC platforms. ### Build system * To build tests, demos, and official SeqAn applications you also need CMake \u2265 3.12. Some official applications might have additional requirements or only work on a subset of platforms. ## Documentation Resources * [Getting Started](https://seqan.readthedocs.io/en/main/Tutorial/GettingStarted) * [Manual](https://seqan.readthedocs.io/en/main) * [Tutorial](https://seqan.readthedocs.io/en/main/index.html#tutorials) * [How-Tos](https://seqan.readthedocs.io/en/main/Tutorial/HowTo) * [API Documentation (stable)](https://docs.seqan.de/seqan/main/) ## Contact * [Mailing List](https://lists.fu-berlin.de/listinfo/seqan-dev#subscribe) * [GitHub Project (issues, source code)](https://github.com/seqan/seqan)']```"}, {"role": "user", "content": "Content from https://github.com/seqan/seqan/tree/master/apps/razers3:\n```\nRepository metadata:\n{'name': 'seqan', 'label': ['seqan'], 'description': [{'term': \"SeqAn's official repository.\", 'id': 0}], 'links': [], 'webpage': [{'term': 'https://www.seqan.de', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['seqan-v2.5.0', 'seqan-v2.5.0rc3', 'seqan-v2.5.0rc2', 'seqan-v2.5.0rc1', 'seqan-v2.4.0', 'seqan-v2.3.2', 'seqan-v2.3.1', 'seqan-v2.3.0', 'seqan-v2.2.0', 'seqan-v2.1.1', 'seqan-v2.1.0', 'seqan-v2.0.2', 'seqan-v2.0.1', 'seqan-v2.0.0', 'seqan-v2.0.0-rc1', 'seqan-v1.4.2'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/seqan/seqan', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/cpp14', 'term': 'cpp14', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/bioinfomatics', 'term': 'bioinfomatics', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/high-performance', 'term': 'high-performance', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/simd', 'term': 'simd', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/sequence-alignments', 'term': 'sequence-alignments', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/alignment', 'term': 'alignment', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/bwt', 'term': 'bwt', 'vocabulary': ''}, 'id': 6}, {'term': {'uri': 'https://github.com/topics/indexing', 'term': 'indexing', 'vocabulary': ''}, 'id': 7}, {'term': {'uri': 'https://github.com/topics/suffixarray', 'term': 'suffixarray', 'vocabulary': ''}, 'id': 8}, {'term': {'uri': 'https://github.com/topics/htslib', 'term': 'htslib', 'vocabulary': ''}, 'id': 9}, {'term': {'uri': 'https://github.com/topics/sam-bam', 'term': 'sam-bam', 'vocabulary': ''}, 'id': 10}, {'term': {'uri': 'https://github.com/topics/fasta', 'term': 'fasta', 'vocabulary': ''}, 'id': 11}, {'term': {'uri': 'https://github.com/topics/fastq-format', 'term': 'fastq-format', 'vocabulary': ''}, 'id': 12}, {'term': {'uri': 'https://github.com/topics/blast', 'term': 'blast', 'vocabulary': ''}, 'id': 13}, {'term': {'uri': 'https://github.com/topics/seqan', 'term': 'seqan', 'vocabulary': ''}, 'id': 14}], 'operations': [], 'authors': [{'term': {'name': 'Enrico Seiler', 'type': 'person', 'email': 'eseiler@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Enrico Seiler', 'type': 'person', 'email': 'enrico.seiler@hotmail.de', 'maintainer': False}, 'id': 1}, {'term': {'name': 'rrahn', 'type': 'person', 'email': 'rahn@molgen.mpg.de', 'maintainer': False}, 'id': 2}, {'term': {'name': 'rrahn', 'type': 'person', 'email': 'rene.rahn@fu-berlin.de', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Evelin', 'type': 'person', 'email': 'evelin.aasna@fu-berlin.de', 'maintainer': False}, 'id': 4}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'changelog', 'url': 'https://github.com/seqan/seqan/blob/main/CHANGELOG.md'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/seqan/seqan/blob/main/CMakeLists.txt'}, 'id': 1}, {'term': {'type': 'contributing', 'url': 'https://github.com/seqan/seqan/blob/main/CONTRIBUTING.md'}, 'id': 2}, {'term': {'type': 'readme', 'url': 'https://github.com/seqan/seqan/blob/main/README.md'}, 'id': 3}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"edena/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/edena/cmd/3\",\n    \"name\": \"edena\",\n    \"description\": [\n      \"A program for assembling de novo bacterial genomes from short reads produced by Illumina sequencing platform.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.genomic.ch/edena\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Dr David HERNANDEZ\",\n        \"email\": \"david.hernandez@genomic.ch\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8279a10e2f3aa0b3b7429\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/edena/cmd/3.131028\",\n    \"name\": \"edena\",\n    \"description\": [\n      \"De novo short reads assembler.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.genomic.ch/edena.php\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"sortmerna/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/sortmerna/cmd/None\",\n    \"name\": \"sortmerna\",\n    \"description\": [\n      \"Sequence analysis tool for filtering, mapping and OTU-picking NGS reads.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioinfo.cristal.univ-lille.fr/RNA/sortmerna/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"https://github.com/sortmerna/sortmerna/issues\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a5edd3b8acd1a2f9b5d7\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/sortmerna/cmd/4.3.6\",\n    \"name\": \"sortmerna\",\n    \"description\": [\n      \"SortMeRNA is a biological sequence analysis tool for filtering, mapping and OTU-picking NGS reads.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/biocore/sortmerna\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://bioinfo.lifl.fr/RNA/sortmerna\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"LGPL\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a5edd3b8acd1a2f9b5d7\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/biocore/sortmerna:\n```\nREADME content:\n['# sortmerna SortMeRNA is a local sequence alignment tool for filtering, mapping and clustering. The core algorithm is based on approximate seeds and allows for sensitive analysis of NGS reads. The main application of SortMeRNA is filtering rRNA from metatranscriptomic data. SortMeRNA takes as input files of reads (fasta, fastq, fasta.gz, fastq.gz) and one or multiple rRNA database file(s), and sorts apart aligned and rejected reads into two files. SortMeRNA works with Illumina, Ion Torrent and PacBio data, and can produce SAM and BLAST-like alignments. SortMeRNA is also available through [QIIME v1.9.1](http://qiime.org) and the [nf-core RNA-Seq pipeline v.3.9](https://nf-co.re/rnaseq/3.9). ## Table of Contents - [Getting Started](#getting-started) - [Using Conda package](#using-conda-package) - [Using GitHub release binaries on Linux](#using-github-release-binaries-on-linux) - [Running](#running) - [Execution trace](#execution-trace) - [Building from sources](#building-from-sources) - [User Manual](#user-manual) - [Databases](#databases) - [Taxonomies](#taxonomies) - [Citation](#citation) - [Contributors](#contributors) - [Support](#support) ## Getting Started SortMeRNA 4 is C++17 compliant, and mostly uses standard libraries. It uses CMake as the build system, and can be run/built on all major OS including Linux, Windows, and Mac, on AMD64 and ARM64 processors. ### Using Conda package Install conda - [official docs](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html) ``` wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh ``` The conda packages before Sortmerna 4.3.7 were hosted on Bioconda. Starting with 4.3.7 the packages are hosted on conda-forge. Erroneously an empty 4.3.7 package made its way to Bioconda, and should be ignored until removed (from Bioconda). Currently the build on conda-forge still waiting to be merged. Until it is ready, the local installation package can be used: ``` # == only for 4.3.7 until ready on conda-forge == # download the conda-build package into a directory of your choice e.g. Downloads/ wget https://github.com/sortmerna/sortmerna/releases/download/v4.3.7/sortmerna-4.3.7-conda-linux-64.tar.bz2 -P ~/Downloads/ # create a new environment and install SortMeRNA in it conda create --name sortmerna conda activate sortmerna conda install ~/Downloads/sortmerna-4.3.7-conda-linux-64.tar.bz2 which sortmerna # check the installed binary e.g. miniforge3/envs/sortmerna/bin/sortmerna sortmerna -h ``` For versions older then 4.3.7 per the [Bioconda guidelines](https://bioconda.github.io), add the following conda channels: ``` conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda config --set channel_priority strict conda search sortmerna Loading channels: done # Name Version Build Channel sortmerna 2.0 0 bioconda ... sortmerna 4.3.4 0 bioconda ... sortmerna 4.3.6 0 bioconda ... sortmerna 4.3.7 hdbdd923_1 bioconda <- (!) ignore - corrupt, see instructions above # create a new environment and install SortMeRNA in it conda create --name sortmerna_env conda activate sortmerna_env conda install sortmerna which sortmerna /home/biocodz/miniconda3/envs/sortmerna_env/bin/sortmerna # test the installation sortmerna --version SortMeRNA version 4.3.6 Build Date: Aug 16 2022 sortmerna_build_git_sha:@db8c1983765f61986b46ee686734749eda235dcc@ sortmerna_build_git_date:@2022/08/16 11:42:59@ # view help sortmerna -h ``` ### Using GitHub release binaries on Linux Visit [Sortmerna GitHub Releases](https://github.com/biocore/sortmerna/releases) Linux distribution is a Shell script with the embedded installation archive. Issue the following bash commands: ``` pushd ~ # get the distro wget https://github.com/biocore/sortmerna/releases/download/v4.3.6/sortmerna-4.3.6-Linux.sh # view the installer usage bash sortmerna-4.3.6-Linux.sh --help Options: [defaults in brackets after descriptions] --help print this message --version print cmake installer version --prefix=dir directory in which to install --include-subdir include the sortmerna-4.3.6-Linux subdirectory --exclude-subdir exclude the sortmerna-4.3.6-Linux subdirectory --skip-license accept license # run the installer bash sortmerna-4.3.6-Linux.sh --skip-license sortmerna Installer Version: 4.3.6, Copyright (c) Clarity Genomics This is a self-extracting archive. The archive will be extracted to: $HOME/sortmerna Using target directory: /home/biocodz/sortmerna Extracting, please wait... Unpacking finished successfully # check the installed binaries ls -lrt /home/biocodz/sortmerna/bin/ sortmerna # set PATH export PATH=$HOME/sortmerna/bin:$PATH # test the installation sortmerna --version SortMeRNA version 4.3.6 Build Date: Jul 17 2021 sortmerna_build_git_sha:@921fa40256760ea2d44c49b21eb326afda748d5e@ sortmerna_build_git_date:@2022/08/16 10:59:31@ # view help sortmerna -h ``` ### Running * The only required options are `--ref` and `--reads` * Options (any) can be specified usig a single dash e.g. `-ref` and `-reads` * Both plain `fasta/fastq` and archived `fasta.gz/fastq.gz` files are accepted * file extensions `.fastq, .fastq.gz, .fq, .fq.gz, .fasta, ...` are optional. The format and compression are automatically recognized * Relative paths are accepted for example ``` # single reference and single reads file sortmerna --ref REF_PATH --reads READS_PATH # for multiple references use multiple \\'--ref\\' sortmerna --ref REF_PATH_1 --ref REF_PATH_2 --ref REF_PATH_3 --reads READS_PATH # for paired reads use \\'--reads\\' twice sortmerna --ref REF_PATH_1 --ref REF_PATH_2 --ref REF_PATH_3 --reads READS_PATH_1 --reads READS_PATH_2 ``` More examples can be found in [test.jinja](https://github.com/biocore/sortmerna/blob/master/scripts/test.jinja) and [run.py](https://github.com/biocore/sortmerna/blob/master/scripts/run.py) #### Execution trace Here is a [sample execution trace](https://sortmerna.readthedocs.io/en/latest/trace4.3.2.html). `IMPORTANT` - Progressing execution trace showing the number of reads processed so far indicates a normally running program. - Non-progressing trace means a problem. Please, kill the process (no waiting for two days), and file an issue [here](https://github.com/biocore/sortmerna/issues) - please, provide the execution trace when filing issues. [Sample execution statistics](https://github.com/biocore/sortmerna/wiki/sample-execution-statistics) are provided to give an idea on what the execution time might be. ## Building from sources [Build instructions](https://sortmerna.readthedocs.io/en/latest/building.html) ## User Manual See [Sortmerna Read The Docs project](https://sortmerna.readthedocs.io/en/latest/index.html). In case you need PDF, any modern browser can print web pages to PDF. ## Databases Please, use [database.tar.gz](https://github.com/biocore/sortmerna/releases/download/v4.3.4/database.tar.gz) from release 4.3.4. We recommend to use smr_v4.3_default_db.fasta. Original source databases (clustering parameters given below): * Silva 138 SSURef NR99 (16S, 18S) * Silva 132 LSURef (23S, 28S) * RFAM v14.1 (5S, 5.8S) The difference between the databases is the % ID for clustering the sequences for each kingdom + rRNA component. Specifically, * smr_v4.3_fast_db.fasta * bac-16S 85%, 5S & 5.8S seeds, rest 90% (benchmark accuracy: 99.888%) * smr_v4.3_default_db.fasta * bac-16S 90%, 5S & 5.8S seeds, rest 95% (benchmark accuracy: 99.899%) * smr_v4.3_sensitive_db.fasta * all 97% (benchmark accuracy: 99.907%) * smr_v4.3_sensitive_db_rfam_seeds.fasta * all 97%, except RFAM database which includes the full seed database sequences The accuracy (based on sensitivity and selectivity) is very good for all databases, however the \"sensitive\" databases will run at least 2x slower. ## Taxonomies The folder `data/rRNA_databases/silva_ids_acc_tax.tar.gz` contains SILVA taxonomy strings (extracted from XML file generated by ARB) for each of the reference sequences in the representative databases. The format of the files is three tab-separated columns, the first being the reference sequence ID, the second being the accession number and the final column is the taxonomy. ## Citation If you use SortMeRNA, please cite: Kopylova E., No\u00e9 L. and Touzet H., \"SortMeRNA: Fast and accurate filtering of ribosomal RNAs in metatranscriptomic data\", Bioinformatics (2012), doi: 10.1093/bioinformatics/bts611. ## Contributors See [AUTHORS](./AUTHORS) for a list of contributors to this project. ## Support For questions and comments, feel free to file an [issue](https://github.com/sortmerna/sortmerna/issues), or start a [discussion](https://github.com/sortmerna/sortmerna/discussions).']```"}, {"role": "user", "content": "Content from https://github.com/biocore/sortmerna:\n```\nRepository metadata:\n{'name': 'sortmerna', 'label': ['sortmerna'], 'description': [{'term': 'SortMeRNA: next-generation sequence filtering and alignment tool', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://sortmerna.readthedocs.io', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v4.3.7', 'v4.3.7-beta.1', 'v4.3.7-beta.0', 'v4.3.6', 'v4.3.6-pre', 'v4.3.5-pre', 'v4.3.4', 'v4.3.4-pre', 'v4.3.3', 'v4.3.3-pre', 'v4.3.2', 'v4.3.1', 'v4.2.0', 'v4.1.0', 'v4.0.0', 'v3.0.4', 'v3.0.3', 'v3.0.2', 'v3.0.1', 'v3.0.0', '2.1b', '2.1', '2.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/sortmerna/sortmerna', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/cpp', 'term': 'cpp', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/python', 'term': 'python', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/alignment', 'term': 'alignment', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/metatranscriptomics', 'term': 'metatranscriptomics', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/ngs', 'term': 'ngs', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/sequencing', 'term': 'sequencing', 'vocabulary': ''}, 'id': 6}], 'operations': [], 'authors': [{'term': {'name': 'biocodz', 'type': 'person', 'email': 'biocodz@protonmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Evguenia Kopylova', 'type': 'person', 'email': 'jenya.kopylov@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Martin Tzvetanov Grigorov', 'type': 'person', 'email': 'mgrigorov@apache.org', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Pierre Pericard', 'type': 'person', 'email': 'pierre.pericard@univ-lille.fr', 'maintainer': False}, 'id': 3}, {'term': {'name': 'cmith44', 'type': 'person', 'email': '101819511+cmith44@users.noreply.github.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'cmith44', 'type': 'person', 'email': 'nathaniel.smith44@hotmail.com', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Pierre Pericard', 'type': 'person', 'email': 'pierre.pericard@gmail.com', 'maintainer': False}, 'id': 6}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'changelog', 'url': 'https://github.com/sortmerna/sortmerna/blob/main/CHANGELOG.md'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/sortmerna/sortmerna/blob/main/CMakeLists.txt'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/sortmerna/sortmerna/blob/main/COPYING.LESSER.txt'}, 'id': 2}, {'term': {'type': 'root', 'url': 'https://github.com/sortmerna/sortmerna/blob/main/LICENSE.LESSER.txt'}, 'id': 3}, {'term': {'type': 'license', 'url': 'https://github.com/sortmerna/sortmerna/blob/main/LICENSE.txt'}, 'id': 4}, {'term': {'type': 'readme', 'url': 'https://github.com/sortmerna/sortmerna/blob/main/README.md'}, 'id': 5}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"gffread/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/gffread/cmd/0.12.7\",\n    \"name\": \"gffread\",\n    \"description\": [\n      \"GFF/GTF utility providing format conversions, region filtering, FASTA sequence extraction and more.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/gpertea/gffread\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://ccb.jhu.edu/software/stringtie/gff.shtml\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/gffread/cmd/2.2.1.1\",\n    \"name\": \"gffread\",\n    \"description\": [\n      \"Filters and/or converts GFF3/GTF2 records.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fdevteam%2Fgffread%2Fgffread%2F2.2.1.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/gpertea/gffread:\n```\nREADME content:\n['## GffRead GFF/GTF utility providing format conversions, filtering, FASTA sequence extraction and more. More details and usage examples can be found in the paper [DOI: 10.12688/f1000research.23297.1](http://dx.doi.org/10.12688/f1000research.23297.1) which can be also used to cite this software. The official webpage with download packages for this utility can be found online here: http://ccb.jhu.edu/software/stringtie/gff.shtml#gffread Use `gffread -h` to see the command line usage options. ## Installation Building this program from source requires the [GCLib](../../../gclib) source code library. The `make` command should automatically fetch the latest gclib version from the repository if no `../gclib` directory is found. ``` cd /some/build/dir git clone https://github.com/gpertea/gffread cd gffread make release ``` This should create the **gffread** binary in the current directory.']```"}, {"role": "user", "content": "Content from https://github.com/gpertea/gffread:\n```\nRepository metadata:\n{'name': 'gffread', 'label': ['gffread'], 'description': [{'term': 'GFF/GTF utility providing format conversions, region filtering, FASTA sequence extraction and more', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.12.7', 'v0.12.6', 'v0.12.4', 'v0.12.3', 'v0.12.2', 'v0.12.1', 'v0.11.8', 'v0.11.7'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/gpertea/gffread', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Geo Pertea', 'type': 'person', 'email': 'geo.pertea@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'alephreish', 'type': 'person', 'email': 'alephreish@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Maarten-vd-Sande', 'type': 'person', 'email': 'maartenvandersande@hotmail.com', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/gpertea/gffread/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://ccb.jhu.edu/software/stringtie/gff.shtml:\n```\nContent:\n['## GFF files Many bioinformatics programs represent genes and transcripts in GFF format ( **G** eneral **F** eature **F** ormat) which simply describes the locations and the attributes of gene and transcript features on the genome (chromosome or scaffolds/contigs). GFF has many versions, but the two most popular that are **GTF2** (Gene Transfer Format, described [here](http://mblab.wustl.edu/GTF2.html) ) and **GFF3** (defined [here](http://www.sequenceontology.org/gff3.shtml) ). Here are a few details about the way these formats are interpreted by StringTie and other bioinformatics programs. ##### GTF files As seen in the [GTF2 specification](http://mblab.wustl.edu/GTF2.html) , the **transcript_id** attribute is required, so our GFF parser also expects it, while a **gene_id** attribute, though not strictly required by our GFF parser, is very useful for grouping alternative transcripts under a gene/locus identifier. An optional **gene_name** attribute, if found, will be taken and shown as a symbolic gene name or short-form abbreviation (e.g. gene symbols from HGNC or Entrez Gene). Some annotation sources (e.g. Ensembl) place a \"human readable\" gene name/symbol in the **gene_name** attribute, like a HUGO symbol (while **gene_id** might be just an automatically generated numeric identifier for the gene). Most bioinformatics programs generally expect at least the _exon_ features which are enough to define the overall transcript structure, with optional _CDS_ features to specify the coding segments. Our GFF reader will ignore redundant features like start_codon, stop_codon when whole _CDS_ features were provided, and similarly _UTR_ features are ignored when whole _exon_ features were given. However, it is still possible to provide only _CDS_ and _*UTR_ features and our GFF parser will reassemble the exonic structure accordingly (internally converting these segments to exon segments). Example of a GTF2 transcript record with minimal attributes: The GTF output of programs like StringTie and Cufflinks also have an additional _transcript_ feature line acting as a parent feature for the _exon_ and _CDS_ features which define the transcript structure and have the same **transcript_id** attribute. This is not required by the GTF2 specification (and makes such files more similar to GFF3, which always have a \"parent\" feature, see below) but may be useful for holding the global attributes of each transcript (instead of having them duplicated for each _exon_ feature). ##### GFF3 files As defined by the [GFF3 specification](http://www.sequenceontology.org/gff3.shtml) , the parent features (usually transcripts, i.e. \" _mRNA_ \" features) are required to have an **ID** attribute, but here again an optional **gene_name** attribute can be used to specify a common gene name abbreviation. If gene_name is not given, it can be also inferred from the Name or ID attributes of the parent _gene_ feature of the current parent mRNA feature (if given in the input file). _CDS_ and _exon_ features are required to have a **Parent** attribute whose value must match the value of the **ID** attribute of a parent transcript feature (usually a \" _mRNA_ \" feature). Example of a GFF3 transcript record with minimal attributes: ##### Feature restrictions For various reasons our GFF parser currently assumes the following limits (maximum values) for the genomic length (span) of gene and transcript features: * genes and transcripts should not span more than 7 Megabases on the genomic sequence * exons should not be longer than 30 Kilobases * introns should not be larger than 6 Megabases Also, transcript IDs are expected to be unique per GFF input file (though we relaxed this restriction by limiting it to a chromosome/contig scope). ## The gffread utility The program **gffread** can be used to validate, filter, convert and perform various other operations on GFF files (use gffread -h to see the various usage options). Because the program shares the same GFF parser code with Cufflinks, Stringtie, and gffcompare, it could be used to verify that a GFF file from a certain annotation source is correctly \"understood\" by these programs. Thus the gffread utility can be used to simply read the transcripts from the file, and optionally print these transcripts back, in either GFF3 (default) or GTF2 format (with the -T option), while discarding any non-essential attributes, optionally fixing some potential issues with the input file(s). The command line for such a quick cleanup and a quick visual inspection of a given GFF file could be: gffread -E annotation.gtf -o- | less This will show the minimalist GFF3 re-formatting of the transcript records found in the input file (annotation.gtf in this example) which could be given in either GFF3 or GTF2 format. The -E option directs gffread to \"expose\" (display warnings about) any potential issues encountered while parsing the input file. In order to see the GTF2 version of the same transcripts, the -T option should be added: gffread -E annotation.gff -T -o- | more The examples above also show that gffread can be used to convert a file between GTF2 and GFF3 file formats. ##### Extracting transcript sequences gffread can also be used to generate a FASTA file with the DNA sequences for all transcripts in a GFF file. For this operation a fasta file with the genomic sequences have to be provided as well. For example, one might want to extract the sequence of all transfrags (defined as transcripts or transcript fragments that result from the assembly process) assembled from a StringTie or Cufflinks assembly session. This can be accomplished with a command line like this: gffread -w transcripts.fa -g /path/to/genome.fa transcripts.gtf The file genome.fa in this example would be a multi-fasta file with the genomic sequences of the target genome. This also requires that every contig or chromosome name found in the 1st column of the input GFF file (transcript.gtf in this example) must have a corresponding sequence entry in chromosomes.fa. This should be the case in our example if genome.fa is the file corresponding to the same genome (index) that was used for mapping the reads with Tophat. Note that the retrieval of the transcript sequences this way is going to be much faster if a fasta index file (genome.fa.fai in this example) is found in the same directory with the genomic fasta file. Such an index file can be created with the [samtools](http://samtools.sourceforge.net/) utility prior to running gffread, like this: samtools faidx genome.fa Then in subsequent runs using the -g option gffread will find this FASTA index and use it to speed up the extraction of transcript sequences. ##### Obtaining gffread ## The gffcompare utility The program [gffcompare](gffcompare.shtml) can be used to compare, merge, annotate and estimate accuracy of one or more GFF files (the \"query\" files), when compared with a reference annotation (also provided as GFF/GTF). A more detailed documentation for the program and its output files can be found [here (gffcompare documentation page)](gffcompare.shtml) ##### Example: evaluating transcript discovery accuracy Gffcompare can be used to evaluate and compare the accuracy of transcript assemblers - in terms of their structural correctness (exon/intron coordinates). This assessment can even be performed in case of more generic \"transcript discovery\" programs like gene finders. The best way to do this would be to use a simulated data set (where the \"reference annotation\" is also the set of the expressed transcripts being simulated), but for well annotated reference genomes (human, mouse etc.), gffcompare can be used to evaluate and compare the general accuracy of isoform discovery programs on a real data set, using just the known (reference) annotation of that genome. As a practical example, let\\'s assume we ran both Cufflinks and StringTie on a mouse RNA-Seq sample and we want to compare the overall accuracy of the two programs. In order to compare the baseline _de novo_ transcript assembly accuracy, both Cufflinks and StringTie should be run without using any reference annotation data (i.e. no -G or -g options were used). Assuming that Cufflinks\\' transcript assembly output file name is cufflinks_asm.gtf and StringTie\\'s output is in stringtie_asm.gtf , while the reference annotation would be in a file called mm10.gff, the gffcompare commands would be: gffcompare -R -r mm10.gff -o cuffcmp cufflinks_asm.gtf gffcompare -R -r mm10.gff -o strtcmp stringtie_asm.gtf The -R option is used here in order to adjust the sensitivity calculation as to only consider the \"expressed\" genes, which are those reference genes for which gffcompare found at least one overlapping transfrag in the given assembly data (*_asm.gtf file). (Of course this option would not be needed in the case of simulated RNA-Seq experiments where the reference transcripts would be all \"expressed\"). Multiple output files will be generated by gffcompare, with the given prefix - in the example above, for the stringtie assemblies, the output files will be: strtcmp.combined.gtf strtcmp.loci strtcmp.stats strtcmp.tracking strtcmp.stringtie_asm.gtf.refmap strtcmp.stringtie_asm.gtf.tmap Comparing the transcript assembly accuracy of the two programs is done by looking at the Sensitivity and Precision values in the *.stats output files for each program (strtcmp.stats vs cuffcmp.stats in the example above). That output looks like this (partially): #= Summary for dataset: stringtie_asm.gtf # Query mRNAs : 23555 in 17628 loci (17231 multi-exon transcripts) # (3731 multi-transcript loci, ~1.3 transcripts per locus) # Reference mRNAs : 16628 in 12062 loci (15850 multi-exon) # Super-loci w/ reference transcripts: 11552 #-----------------| Sensitivity | Precision | Base level: 82.4 | 76.5 | Exon level: 81.2 | 82.9 | Intron level: 86.1 | 94.8 | Intron chain level: 56.9 | 52.4 | Transcript level: 55.2 | 38.9 | Locus level: 70.1 | 48.0 | ##### Obtaining gffcompare ### Publication Pertea G and Pertea M. [GFF Utilities: GffRead and GffCompare](https://doi.org/10.12688/f1000research.23297.1) . _F1000Research_ 2020, 9:304 [DOI: 10.12688/f1000research.23297.1](https://doi.org/10.12688/f1000research.23297.1)']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"agotool/soap": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/agotool/soap/0.2\",\n    \"name\": \"agotool\",\n    \"description\": [\n      \"A Gene Ontology enrichment tool, specifically designed for the abundance bias in Mass Spectrometry (MS) based proteomics data. This site can be used for functional annotation enrichment for proteomics data. It contains tools for abundance corrected functional enrichment and supports the following categories: GO-terms, UniProt-keywords, KEGG-pathways, Reactome, WikiPathways, InterPro, Pfam, PubMed Publications, Diseaeses, Tissues, and Compartments.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://agotool.org/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"David Lyon\",\n        \"email\": \"dblyon@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a8bfd3b8acd1a2f9b980\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/agotool/soap/0.1\",\n    \"name\": \"agotool\",\n    \"description\": [\n      \"A Gene Ontology enrichment tool, specifically designed for the abundance bias in Mass Spectrometry (MS) based proteomics data. This site can be used for functional annotation enrichment for proteomics data. It contains tools for abundance corrected GO-term, UniProt-keyword, and KEGG-pathway enrichment.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://agotool.sund.ku.dk/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"David Lyon\",\n        \"email\": \"david.lyon@cpr.ku.dk\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a8bfd3b8acd1a2f9b980\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"stringtie/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/stringtie/cmd/2.2.1\",\n    \"name\": \"stringtie\",\n    \"description\": [\n      \"StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/gpertea/stringtie\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://ccb.jhu.edu/software/stringtie/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dbfdac7ee6cb57891288\"\n      },\n      {\n        \"$oid\": \"67c7a59dd3b8acd1a2f9b5c0\"\n      },\n      {\n        \"$oid\": \"67c827fd10e2f3aa0b3b780d\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/stringtie/cmd/1.3.3.1\",\n    \"name\": \"stringtie\",\n    \"description\": [\n      \"Transcript assembly and quantification.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fstringtie%2Fstringtie%2F1.3.3.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/gpertea/stringtie:\n```\nREADME content:\n['![alt text](https://img.shields.io/badge/License-MIT-blue.svg \"MIT License\") ## StringTie: efficient transcript assembly and quantitation of RNA-Seq data Stringtie employs efficient algorithms for transcript structure recovery and abundance estimation from bulk RNA-Seq reads aligned to a reference genome. It takes as input spliced alignments in coordinate-sorted SAM/BAM/CRAM format and produces a GTF output which consists of assembled transcript structures and their estimated expression levels (FPKM/TPM and base coverage values). For additional StringTie documentation and the latest official source and binary packages please refer to the official website: <https://ccb.jhu.edu/software/stringtie> ## Obtaining and installing StringTie Source and binary packages for this software can be directly downloaded from the [Releases](https://github.com/gpertea/stringtie/releases) page for this repository. StringTie is compatible with a wide range of Linux and Apple OS systems. The main program (StringTie) does not have any other library dependencies (besides zlib) and in order to compile it from source it requires a C++ compiler which supports the C++ 11 standard (GCC 4.8 or newer). ### Building the latest version from the repository In order to compile the StringTie source in this GitHub repository the following steps can be taken: ``` git clone https://github.com/gpertea/stringtie cd stringtie make -j4 release ``` During the first run of the above make command a few library dependencies will be downloaded and compiled, but any subsequent stringtie updates (using `git pull`) should rebuild much faster. To complete the installation, the resulting `stringtie` binary can then be copied to a programs directory of choice (preferably one that is in the current shell\\'s PATH). Building and installing of StringTie this way should take less than a minute on a regular Linux or Apple MacOS desktop. Note that simply running `make` would produce a less optimized executable which is suitable for debugging and runtime checking but that is significantly slower than the optimized version which is built by using the `make release` command as instructed above. ### Using pre-compiled (binary) releases Instead of compiling from source, some users may prefer to download an already compiled binary for Linux and Apple MacOS, ready to run. These binary package releases are compiled on older versions of these operating systems in order to provide compatibility with a wide range of OS versions not just the most recent distributions. These precompiled packages are made available on the <a href=\"https://github.com/gpertea/stringtie/releases\">Releases</a> page for this repository. Please note that these binary packages do not include the optional [super-reads module](#the-super-reads-module), which currently can only be built on Linux machines from the source made available in this repository. ## Running StringTie The generic command line for the default usage has this format: ``` stringtie [-o <output.gtf>] [other_options] <read_alignments.bam> ``` The main input of the program (_<read_alignments.bam>_) must be a SAM, BAM or CRAM file with RNA-Seq read alignments sorted by their genomic location (for example the `accepted_hits.bam` file produced by TopHat, or HISAT2 output sorted with `samtools sort` etc.). The main output is a GTF file containing the structural definitions of the transcripts assembled by StringTie from the read alignment data. The name of the output file should be specified with the `-o` option. If this `-o` option is not used, the output GTF with the assembled transcripts will be printed to the standard output (and can be captured into a file using the `>` output redirect operator). __Note__: if the `--mix` option is used, StringTie expects two alignment files to be given as positional parameters, in a specific order: the short read alignments must be the first file given while the long read alignments must be the second input file. Both alignment files must be sorted by genomic location. ``` stringtie [-o <output.gtf>] --mix [other_options] <short_read_alns.bam> <long_read_alns.bam> ``` Note that the command line parser in StringTie allows arbitrary order and mixing of the positional parameters with the other options of the program, so the input alignment files can also precede or be given in between the other options -- the following command line is equivalent to the one above: ``` stringtie <short_read_alns.bam> <long_read_alns.bam> --mix [other_options] [-o <output.gtf>] ``` ### Running StringTie on the provided test data When building from this source repository, after the program was compiled with `make release` as instructed above, the generated binary can be tested on a small data set with a command like this: ``` make test ``` This will run the included `run_tests.sh` script which downloads a small test data set and runs a few simple tests to ensure that the program works and generates the expected output. If a pre-compiled package is used instead of compiling the program from source, the `run_tests.sh` script is included in the binary package as well and it can be run immediately after unpacking the binary package: ``` tar -xvzf stringtie-3.0.0.Linux_x86_64.tar.gz cd stringtie-3.0.0.Linux_x86_64 ./run_tests.sh ``` These small test/demo data set can be downloaded as <a href=\"https://github.com/gpertea/stringtie/raw/test_data/tests_3.tar.gz\">tests_3.tar.gz</a> along with the source package and pre-compiled packages on the <a href=\"https://github.com/gpertea/stringtie/releases\">Releases</a> page of this repository. The tests can also be run manually as shown below (after changing to the _test_data_ directory, `cd test_data`): #### Test 1: Input consists of only alignments of short reads ``` stringtie -o short_reads.out.gtf short_reads.bam ``` #### Test 2: Input consists of alignments of short reads and superreads ``` stringtie -o short_reads_and_superreads.out.gtf short_reads_and_superreads.bam ``` #### Test 3: Input consists of alignments of long reads ``` stringtie -L -o long_reads.out.gtf long_reads.bam ``` #### Test 4: Input consists of alignments of long reads and reference annotation (guides) ``` stringtie -L -G human-chr19_P.gff -o long_reads_guided.out.gtf long_reads.bam ``` #### Test 5: Input consists of alignments of short reads and alignments of long reads (using `--mix` option) ``` stringtie --mix -o mix_reads.out.gtf mix_short.bam mix_long.bam ``` #### Test 6: Input consists of alignments of short reads, alignments of long reads and a reference annotation (guides) ``` stringtie --mix -G mix_guides.gff -o mix_reads_guided.out.gtf mix_short.bam mix_long.bam ``` For version 3.0.0, three additional tests have been added, please see the `run_tests.sh` scripts for the details. For large data sets one can expect up to one hour of processing time. A minimum of 8GB of RAM is recommended for running StringTie on regular size RNA-Seq samples, with 16 GB or more being strongly advised for larger data sets. ### StringTie options The following optional parameters can be specified (use `-h` or `--help` to get the usage message): ``` --version : print just the version at stdout and exit --conservative : conservative transcript assembly, same as -t -c 1.5 -f 0.05 --mix : both short and long read data alignments are provided (long read alignments must be the 2nd BAM/CRAM input file) --rf : assume stranded library fr-firststrand --fr : assume stranded library fr-secondstrand -G reference annotation to use for guiding the assembly process (GTF/GFF) --ptf : load point-features from a given 4 column feature file <f_tab> -o output path/file name for the assembled transcripts GTF (default: stdout) -l name prefix for output transcripts (default: STRG) -f minimum isoform fraction (default: 0.01) -L long reads processing; also enforces -s 1.5 -g 0 (default:false) -R if long reads are provided, just clean and collapse the reads but do not assemble -m minimum assembled transcript length (default: 200) -a minimum anchor length for junctions (default: 10) -j minimum junction coverage (default: 1) -t disable trimming of predicted transcripts based on coverage (default: coverage trimming is enabled) -c minimum reads per bp coverage to consider for multi-exon transcript (default: 1) -s minimum reads per bp coverage to consider for single-exon transcript (default: 4.75) -v verbose (log bundle processing details) -g maximum gap allowed between read mappings (default: 50) -M fraction of bundle allowed to be covered by multi-hit reads (default:1) -p number of threads (CPUs) to use (default: 1) -A gene abundance estimation output file -N nascent aware assembly: use this option with rRNA-depleted RNAseq libraries (e.g. Total RNA, Ribo-Zero) --nasc : enables -N but also outputs the assembled nascent transcripts -E define window around possibly erroneous splice sites from long reads to look out for correct splice sites (default: 25) -B enable output of Ballgown table files which will be created in the same directory as the output GTF (requires -G, -o recommended) -b enable output of Ballgown table files but these files will be created under the directory path given as <dir_path> -e only estimate the abundance of given reference transcripts (requires -G) --viral only relevant for long reads from viral data where splice sites do not follow consensus (default:false) -x do not assemble any transcripts on the given reference sequence(s) -u no multi-mapping correction (default: correction enabled) -h print this usage message and exit --ref/--cram-ref reference genome FASTA file for CRAM input Transcript merge usage mode: stringtie --merge [Options] { gtf_list | strg1.gtf ...} With this option StringTie will assemble transcripts from multiple input files generating a unified non-redundant set of isoforms. In this mode the following options are available: -G <guide_gff> reference annotation to include in the merging (GTF/GFF3) -o <out_gtf> output file name for the merged transcripts GTF (default: stdout) -m <min_len> minimum input transcript length to include in the merge (default: 50) -c <min_cov> minimum input transcript coverage to include in the merge (default: 0) -F <min_fpkm> minimum input transcript FPKM to include in the merge (default: 1.0) -T <min_tpm> minimum input transcript TPM to include in the merge (default: 1.0) -f <min_iso> minimum isoform fraction (default: 0.01) -g <gap_len> gap between transcripts to merge together (default: 250) -i keep merged transcripts with retained introns; by default these are not kept unless there is strong evidence for them -l <label> name prefix for output transcripts (default: MSTRG) ``` More details about StringTie options can be found in the [online manual](http://ccb.jhu.edu/software/stringtie/index.shtml?t=manual). ## Input files StringTie takes as input a SAM, BAM or CRAM file sorted by coordinate (genomic location). This file should contain spliced RNA-seq read alignments such as the ones produced by TopHat or HISAT2. TopHat output is already sorted. Unsorted SAM or BAM files generated by other aligners should be sorted using the `samtools` program: ``` samtools sort -o alns.sorted.bam alns.sam ``` The file resulted from the above command (alns.sorted.bam) can be used as input to StringTie. Any SAM record with a spliced alignment (i.e. having a read alignment across at least one junction) should have the `XS` tag to indicate the transcription strand, i.e. the genomic strand from which the RNA that produced this read originated. TopHat and HISAT2 alignments already include this tag, but if you use a different read mapper you should check that this tag is also included for spliced alignment records. STAR aligner should be run with the option `--outSAMstrandField intronMotif` in order to generate this tag. The `XS` tags are not necessary in the case of long RNA-seq reads aligned with `minimap2` using the `-ax splice` option. `minimap2` adds the `ts` tags to splice alignments to indicate the transcription strand (albeit in a different manner than the `XS` tag), and StringTie can recognize the `ts` tag as well, if the `XS` tag is missing. Thus the long read spliced alignments produced by `minimap2` can be also assembled by StringTie (with the option `-L` or as the 2nd input file for the `--mix` option). As explained above, the alignments must be sorted by coordinate before they can be used as input for StringTie. When CRAM files are used as input, the original reference genomic sequence can be provided with the `--ref` (`--cram-ref`) option as a multi-FASTA file with the same chromosome sequences that were used when aligning the reads. This is optional but recommended because StringTie can better estimate the quality of some spliced alignments (e.g. noticing mismatches around junctions) and that data can be retrieved in the case of some CRAM files only when the reference genome sequence is also provided. ### Reference transcripts (guides) A reference annotation file in GTF or GFF3 format can be provided to StringTie using the `-G` option which can be used as \\'guides\\' for the assembly process. When the `-e` option is used (i.e. expression estimation only), this option is required, and in that case StringTie will not attempt to assemble the read alignments but instead it will only estimate the expression levels of all the transcripts provided in this file Note that when a reference transcript is fully covered by reads, the original transcript ID from the reference annotation file will be shown in StringTie\\'s output record in the _`reference_id`_ GTF attribute. Output transcripts that lack such `reference_id` attribute can be considered \"novel\" transcript structures with respect to the given reference annotation. ## The super-reads module This optional module can be used to de-novo assemble, align and pre-process RNA-Seq reads, preparing them to be used as \"super-reads\" by Stringtie. More usage information is provided in <a href=\"https://github.com/gpertea/stringtie/blob/master/SuperReads_RNA/README.md\">SuperReads_RNA/README.md</a>. Quick installation instructions for this module from the source available on this repository (assuming main Stringtie installation was already completed as described above): ``` cd SuperReads_RNA ./install.sh ``` ### Using super-reads with Stringtie After running the super-reads module (see the <a href=\"https://github.com/gpertea/stringtie/blob/master/SuperReads_RNA/README.md\">SuperReads_RNA</a> module documentation for usage details), there is a BAM file created which contains sorted alignment for both short reads and super-reads, called *`sr_merge.bam`*, created in the selected output directory. This file can be directly given as the main input file to StringTie as described in the [Running StringTie](#running-stringtie) section above. ## License StringTie is free, open source software released under an <a href=\"https://opensource.org/licenses/MIT\">MIT License</a>. ## Publications Shumate A, Wong B, Pertea G, Pertea M [**Improved transcriptome assembly using a hybrid of long and short reads with StringTie**](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009730), _PLOS Computational Biology_ 18, 6 (2022), doi.org/10.1371/journal.pcbi.1009730 Kovaka S, Zimin AV, Pertea GM, Razaghi R, Salzberg SL, Pertea M [**Transcriptome assembly from long-read RNA-seq alignments with StringTie2**](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1910-1), _Genome Biology_ 20, 278 (2019), doi:10.1186/s13059-019-1910-1 Pertea M, Kim D, Pertea GM, Leek JT, Salzberg SL [**Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie and Ballgown**](http://www.nature.com/nprot/journal/v11/n9/full/nprot.2016.095.html), _Nature Protocols_ 11, 1650-1667 (2016), doi:10.1038/nprot.2016.095 Pertea M, Pertea GM, Antonescu CM, Chang TC, Mendell JT & Salzberg SL [**StringTie enables improved reconstruction of a transcriptome from RNA-seq reads**](http://www.nature.com/nbt/journal/vaop/ncurrent/full/nbt.3122.html), _Nature Biotechnology_ 2015, doi:10.1038/nbt.3122']```"}, {"role": "user", "content": "Content from https://github.com/gpertea/stringtie:\n```\nRepository metadata:\n{'name': 'stringtie', 'label': ['stringtie'], 'description': [{'term': 'Transcript assembly and quantification for RNA-Seq', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v3.0.0', 'v2.2.3', 'v2.2.2', 'v2.2.1', 'v2.2.0', 'v2.1.7', 'v2.1.6', 'v2.1.4', 'v2.1.3b', 'v2.1.3', 'v2.1.2', 'v2.1.1', 'v2.1.0', 'v2.0.6', 'v2.0.5', 'v2.0.4', 'v2.0.3', 'v2.0', 'v1.3.4d'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/gpertea/stringtie', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Geo Pertea', 'type': 'person', 'email': 'geo.pertea@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Ela Pertea', 'type': 'person', 'email': 'mpertea@jhu.edu', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Gryte Satas', 'type': 'person', 'email': 'gsatas7@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Chit Tong Lio', 'type': 'person', 'email': 'chit@OwO.local', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Mihaela Pertea', 'type': 'person', 'email': 'mpertea@jhu.edu\u00df', 'maintainer': False}, 'id': 4}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/gpertea/stringtie/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/gpertea/stringtie/blob/main/TODO.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://ccb.jhu.edu/software/stringtie/:\n```\nContent:\n[\"#### Overview **StringTie** is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts. It uses a novel network flow algorithm as well as an optional _de novo_ assembly step to assemble and quantitate full-length transcripts representing multiple splice variants for each gene locus. Its input can include not only alignments of short reads that can also be used by other transcript assemblers, but also alignments of longer sequences that have been assembled from those reads. In order to identify differentially expressed genes between experiments, StringTie's output can be processed by specialized software like [Ballgown](https://github.com/alyssafrazee/ballgown) , [Cuffdiff](http://cole-trapnell-lab.github.io/cufflinks/cuffdiff/index.html) or other programs (DESeq2, edgeR, etc.). #### News * **1/6/2025 - v3.0.0**release:increase of accuracy in many cases, as StringTie can now better handle poor or inconsistent guide coverage by short or long reads-N/--nascoptions can now be used to assemble and better handle incompletely processed (nascent) RNAs that are abundant in the case of rRNA-depletion sequencing samples (e.g. Total RNA Ribo-Zero libraries)fixes and improvements for the-eexpression quantification mode, for both long and short RNAseq reads * **5/7/2024 - v2.2.3**release:fixes for long-read assembly and build scripts * **4/20/2024 - v2.2.2**release:fixes an out-of-bounds issue when a large number of predictions are generatedfixes other rare situations causing a program crash * **1/26/2022 - v2.2.1**release:addressing an issue causing-eoption to not output low coverage transcriptsfixed a--ptfdata loading issue * **12/3/2021 - v2.2.0**release hilights:--mixoption allows StringTie to take both short and long read alignments; when this option is used, the 2nd BAM (or CRAM) file in the command line must be a long reads alignment file (the 1st being the short-reads alignment file);-Loption should not be used in this case.improved transcriptome assembly on mixed data with annotationadded support for CRAM input files, as StringTie is now built using[HTSlib](http://www.htslib.org/) * **3/9/2021 - v2.1.5**release changes:adjusted the calculation of coverage for long reads assemblynew trimming procedure implemented * **7/7/2020 - v2.1.4**release is a maintenance release with the following fixes:fixed an issue with--mergesometimes incorrectly processing the order of the reference sequencesfixed-eissue sometimes entering an infinite loopfixed compatibility issue between long transfrag and path assembled so farsmall bug fix in computing long read assemblies * **5/12/2020 - v2.1.3 release**new features and fixesadded the--viraloption for long reads from viral data where splice sites do not follow consensusadjustments to the assembly of long read alignment datafixed an occasional issue with the--mergeoptionmade the-ecompatible with long reads (-Loption) _See entire[release history here](history.shtml)._ [Back to top](#top) #### Obtaining and installing StringTie The current version of StringTie can be downloaded as precompiled binary or as a source package: In order to build and install StringTie from the source package the following steps can be taken: * Unpack the downloaded StringTie source archive in a directory of your choice, e.g.:cd ~/src/ tar xvfz ~/Downloads/stringtie-VER.tar.gzA directory calledstringtie-VER(where VER is the current numeric version of the program) will be created in the current directory. * Change directory and build thestringtieexecutable:cd stringtie-VER make release Alternatively, the source tree can be downloaded from [GitHub](https://github.com/gpertea/stringtie) and built in a similar fashion: git clone https://github.com/gpertea/stringtie cd stringtie make release * Optionally, thestringtieexecutable can be copied to one of the shell's PATH directories for easy access. For evaluating and further processing the GTF output of StringTie, the utility [gffcompare](/software/stringtie/gff.shtml#gffcompare) can be downloaded from the [GFF utilities](/software/stringtie/gff.shtml#gffcompare) page. [Back to top](#top) #### Publications Shumate A, Wong B, Pertea G, Pertea M [Improved transcriptome assembly using a hybrid of long and short reads with StringTie](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009730) , _PLOS Computational Biology_ 18, 6 (2022), doi.org/10.1371/journal.pcbi.1009730 Kovaka S, Zimin AV, Pertea GM, Razaghi R, Salzberg SL, Pertea M [Transcriptome assembly from long-read RNA-seq alignments with StringTie2](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1910-1) , _Genome Biology_ 20, 278 (2019), doi:10.1186/s13059-019-1910-1 Pertea M, Kim D, Pertea GM, Leek JT, Salzberg SL [Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie and Ballgown](http://www.nature.com/nprot/journal/v11/n9/full/nprot.2016.095.html) , _Nature Protocols_ 11, 1650-1667 (2016), doi:10.1038/nprot.2016.095 Pertea M, Pertea GM, Antonescu CM, Chang TC, Mendell JT & Salzberg SL. [StringTie enables improved reconstruction of a transcriptome from RNA-seq reads](http://www.nature.com/nbt/journal/vaop/ncurrent/full/nbt.3122.html) [Nature Biotechnology](http://www.nature.com/nbt/index.html) 2015, doi:10.1038/nbt.3122 [Back to top](#top)\"]```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"xtandem/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/xtandem/cmd/None\",\n    \"name\": \"xtandem\",\n    \"description\": [\n      \"Matches tandem mass spectra with peptide sequences.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.thegpm.org/TANDEM/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"contact@thegpm.org\",\n        \"email\": \"contact@thegpm.org\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"contact@thegpm.org\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a6c8d3b8acd1a2f9b634\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/xtandem/cmd/15.12.15.2\",\n    \"name\": \"xtandem\",\n    \"description\": [\n      \"X! Tandem open source is software that can match tandem mass spectra with peptide sequences, in a process that has come to be known as protein identification.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://www.thegpm.org/TANDEM/index.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Artistic License\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a6c8d3b8acd1a2f9b634\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://www.thegpm.org/TANDEM/index.html:\n```\nContent:\n['[Home](/TANDEM/index.html) | [Installation](/TANDEM/instructions.html) | [FAQ](/TANDEM/tandem_install_faq.html) | [Release notes](/TANDEM/release.html) | [Mutations](/TANDEM/pam.html) | [API](/TANDEM/api/index.html) _**Note**: For versions of X! Tandem that have been modified to run on cluster computer systems, please see[Parallel Tandem](/parallel/)or[X!!Tandem](http://wiki.thegpm.org/wiki/X!!Tandem)._ X! Tandem open source is software that can match tandem mass spectra with peptide sequences, in a process that has come to be known as protein identification. This software has a very simple, sophisticated application programming interface (API): it simply takes an XML file of instructions on its command line, and outputs the results into an XML file, which has been specified in the input XML file. The output format is described [here (PDF)](/docs/X_series_output_form.pdf) . This format is used for all of the X! series search engines, as well as the GPM and GPMDB. Unlike some earlier generation search engines, all of the X! Series search engines calculate statistical confidence (expectation values) for all of the individual spectrum-to-sequence assignments. They also reassemble all of the peptide assignments in a data set onto the known protein sequences and assign the statistical confidence that this assembly and alignment is non-random. The formula for which can be found [here](/docs/peptide_protein_expect.pdf) . Therefore, separate assembly and statistical analysis software, e.g. PeptideProphet and ProteinProphet, do not need to be used. This release has numerous small changes to reduce the amount of memory used by the application when utilizing its expert systems methods for finding PTMs and SAVs. It also adds several mechanisms to reduce the number of false positive assignments, particularly when testing for SAVs. * The handling of expert systems information that is loaded from files but is not altered during a search has been changed so that a single, global data structure is used, rather than individual data structures in each thread. This feature required significant alteration to the code, so any projects that are \"forks\" of the main X! Tandem project should take some care to assure that their changes are not effected. * A parent ion mass peak tolerance detection system has been added that can detect the minimum necessary tolerance as part of the report generation phase of the search. * Protein SAVs that can be assigned to non-variant peptides are now excluded from the output. This release improves the precision of handling variable modifications. Several new commands and notational add-ons make specifying how to test modification significantly more nuanced. The methods for handling variable modifications have been extensively re-written. * The value of the command \"protein, ptm complexity\" (C, a floating point number 0.0\u201312.0) sets the maximum number of variable modification alternatives that will be tested for a particular peptide. The number of alternatives is 2.0C. If this number is not specified, the default value C = 6.0 will be used. * The specification of a variable modification can include a value for the maximum number of modification sites to be considered in a single peptide. For example, the modification specification 15.994915@M would normally be used to test for M oxidation. If you wish only to consider one such modification per peptide, you can now write \"15.994915@**1**M\". Any number from 1\u201310 can be used in this notation. If not specified, a default value of 10 is used. * It is possible to specify that a variable modification NOT occur at the C-terminus of a peptide. For example, previously \"42.010565@K\" would have been used to test for K acetylation. Using the new notation, \"42.010565@**]**K\" can be used, which will not test C-terminal lysines for acetylation (which are chemically impossible for tryptic peptides). This notation is useful for most lysine post-translational modifications, as well as dimethyl-arginine. Note: monomethyl-arginine and -lysine are both susceptible to trypsin cleavage, so this notation is not recommended for monomethyl variable modifications. It is also not recommended for use with carbamylation \u2014 a urea artifact that can occur during tryptic digestion \u2014 although reducing the number of carbamylations allowed per peptide,_e.g._, \"43.005814@**1**K\", can be quite useful. * The legacy command \"spectrum, use noise suppression\" has been removed from the project: the original method was created for LCQ spectra and it no longer had any practical utility. * Limits have been introduced to the length of peptide that will be considered to be a solution to a mass spectrum. Previous limits had only been based on the parent ion mass of a fragment ion spectrum. The new limits require a peptide to be 6\u201350 residues in length, regardless of the parent ion mass. * The Windows version of the code has been updated and adapted for use with_Microsoft Visual Studio Community 2015_. It has been fully tested for_Windows 8, 8.1 & 10_. * The Linux version of the code has been updated and adapted for use with_Red Hat Enterprise Linux Workstation v.6.7_, using_gcc v. 4.4.7_. * This version was designed and tested to work with the_BI GPM Fury_version of the generic GPM interface. This release adds a new output format (mzIdentML) and several variants of the mzML input format (MSNumPress compression). It also corrects an undesired behavior when searching for protein N-terminal and C-terminal modifications when using a protein modification specification XML file. * New files (MSNumPress.cpp, MSNumPress.hpp) were added to the project (Johan Teleman) to implement the compression modes that have been added to the mzML specification. * New files (mzid_report.cpp, mzid_report.hpp) were added to the project to implement the output of an mzIdentML file, in addition to the existing BIOML output. To generate an mzIdentML output, set the new parameter:\"output, mzid\": if \"yes\" the file will be created with the extention .mzid. * The \"score_terminus_single\" method has been removed from mprocess and replaced by an altered version of \"score_terminus\", which corrects the bad behavior associated with searching for protein N- and C-terminal modifications when using a protein modification specification file. It also improves the display progress reporting for this type of search. _Copyright \u00a9 2004-2013, The Global Proteome Machine Organization_ _Use of all documentation for X! Tandem, X! P3 and X! Hunter is governed by the[Artistic License](http://opensource.org/licenses/artistic-license-1.0)._']```"}, {"role": "user", "content": "Content from http://www.thegpm.org/TANDEM/:\n```\nContent:\n['[Home](/TANDEM/index.html) | [Installation](/TANDEM/instructions.html) | [FAQ](/TANDEM/tandem_install_faq.html) | [Release notes](/TANDEM/release.html) | [Mutations](/TANDEM/pam.html) | [API](/TANDEM/api/index.html) _**Note**: For versions of X! Tandem that have been modified to run on cluster computer systems, please see[Parallel Tandem](/parallel/)or[X!!Tandem](http://wiki.thegpm.org/wiki/X!!Tandem)._ X! Tandem open source is software that can match tandem mass spectra with peptide sequences, in a process that has come to be known as protein identification. This software has a very simple, sophisticated application programming interface (API): it simply takes an XML file of instructions on its command line, and outputs the results into an XML file, which has been specified in the input XML file. The output format is described [here (PDF)](/docs/X_series_output_form.pdf) . This format is used for all of the X! series search engines, as well as the GPM and GPMDB. Unlike some earlier generation search engines, all of the X! Series search engines calculate statistical confidence (expectation values) for all of the individual spectrum-to-sequence assignments. They also reassemble all of the peptide assignments in a data set onto the known protein sequences and assign the statistical confidence that this assembly and alignment is non-random. The formula for which can be found [here](/docs/peptide_protein_expect.pdf) . Therefore, separate assembly and statistical analysis software, e.g. PeptideProphet and ProteinProphet, do not need to be used. This release has numerous small changes to reduce the amount of memory used by the application when utilizing its expert systems methods for finding PTMs and SAVs. It also adds several mechanisms to reduce the number of false positive assignments, particularly when testing for SAVs. * The handling of expert systems information that is loaded from files but is not altered during a search has been changed so that a single, global data structure is used, rather than individual data structures in each thread. This feature required significant alteration to the code, so any projects that are \"forks\" of the main X! Tandem project should take some care to assure that their changes are not effected. * A parent ion mass peak tolerance detection system has been added that can detect the minimum necessary tolerance as part of the report generation phase of the search. * Protein SAVs that can be assigned to non-variant peptides are now excluded from the output. This release improves the precision of handling variable modifications. Several new commands and notational add-ons make specifying how to test modification significantly more nuanced. The methods for handling variable modifications have been extensively re-written. * The value of the command \"protein, ptm complexity\" (C, a floating point number 0.0\u201312.0) sets the maximum number of variable modification alternatives that will be tested for a particular peptide. The number of alternatives is 2.0C. If this number is not specified, the default value C = 6.0 will be used. * The specification of a variable modification can include a value for the maximum number of modification sites to be considered in a single peptide. For example, the modification specification 15.994915@M would normally be used to test for M oxidation. If you wish only to consider one such modification per peptide, you can now write \"15.994915@**1**M\". Any number from 1\u201310 can be used in this notation. If not specified, a default value of 10 is used. * It is possible to specify that a variable modification NOT occur at the C-terminus of a peptide. For example, previously \"42.010565@K\" would have been used to test for K acetylation. Using the new notation, \"42.010565@**]**K\" can be used, which will not test C-terminal lysines for acetylation (which are chemically impossible for tryptic peptides). This notation is useful for most lysine post-translational modifications, as well as dimethyl-arginine. Note: monomethyl-arginine and -lysine are both susceptible to trypsin cleavage, so this notation is not recommended for monomethyl variable modifications. It is also not recommended for use with carbamylation \u2014 a urea artifact that can occur during tryptic digestion \u2014 although reducing the number of carbamylations allowed per peptide,_e.g._, \"43.005814@**1**K\", can be quite useful. * The legacy command \"spectrum, use noise suppression\" has been removed from the project: the original method was created for LCQ spectra and it no longer had any practical utility. * Limits have been introduced to the length of peptide that will be considered to be a solution to a mass spectrum. Previous limits had only been based on the parent ion mass of a fragment ion spectrum. The new limits require a peptide to be 6\u201350 residues in length, regardless of the parent ion mass. * The Windows version of the code has been updated and adapted for use with_Microsoft Visual Studio Community 2015_. It has been fully tested for_Windows 8, 8.1 & 10_. * The Linux version of the code has been updated and adapted for use with_Red Hat Enterprise Linux Workstation v.6.7_, using_gcc v. 4.4.7_. * This version was designed and tested to work with the_BI GPM Fury_version of the generic GPM interface. This release adds a new output format (mzIdentML) and several variants of the mzML input format (MSNumPress compression). It also corrects an undesired behavior when searching for protein N-terminal and C-terminal modifications when using a protein modification specification XML file. * New files (MSNumPress.cpp, MSNumPress.hpp) were added to the project (Johan Teleman) to implement the compression modes that have been added to the mzML specification. * New files (mzid_report.cpp, mzid_report.hpp) were added to the project to implement the output of an mzIdentML file, in addition to the existing BIOML output. To generate an mzIdentML output, set the new parameter:\"output, mzid\": if \"yes\" the file will be created with the extention .mzid. * The \"score_terminus_single\" method has been removed from mprocess and replaced by an altered version of \"score_terminus\", which corrects the bad behavior associated with searching for protein N- and C-terminal modifications when using a protein modification specification file. It also improves the display progress reporting for this type of search. _Copyright \u00a9 2004-2013, The Global Proteome Machine Organization_ _Use of all documentation for X! Tandem, X! P3 and X! Hunter is governed by the[Artistic License](http://opensource.org/licenses/artistic-license-1.0)._']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"hisat2/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/hisat2/cmd/2.2.1\",\n    \"name\": \"hisat2\",\n    \"description\": [\n      \"HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes as well as to a single reference genome.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/DaehwanKimLab/hisat2\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://daehwankimlab.github.io/hisat2\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dc25ac7ee6cb578912af\"\n      },\n      {\n        \"$oid\": \"67c7a0e1d3b8acd1a2f9b4b3\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/hisat2/cmd/2.1.0\",\n    \"name\": \"hisat2\",\n    \"description\": [\n      \"A fast and sensitive alignment program.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fhisat2%2Fhisat2%2F2.1.0\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/DaehwanKimLab/hisat2:\n```\nREADME content:\n[\"# HISAT2 ## Contact [Daehwan Kim](https://kim-lab.org) (infphilo@gmail.com) and [Chanhee Park](https://www.linkedin.com/in/chanhee-park-97677297/) (parkchanhee@gmail.com) ## Overview HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (whole-genome, transcriptome, and exome sequencing data) to a population of human genomes (as well as to a single reference genome). Based on an extension of BWT for a graph [1], we designed and implemented a graph FM index (GFM), an original approach and its first implementation to the best of our knowledge. In addition to using one global GFM index that represents general population, HISAT2 uses a large set of small GFM indexes that collectively cover the whole genome (each index representing a genomic region of 56 Kbp, with 55,000 indexes needed to cover human population). These small indexes (called local indexes) combined with several alignment strategies enable effective alignment of sequencing reads. This new indexing scheme is called Hierarchical Graph FM index (HGFM). We have developed HISAT2 based on the HISAT [2] and Bowtie 2 [3] implementations. See the [HISAT2 website](https://daehwankimlab.github.io/hisat2/) for more information. ![](HISAT2.png) Getting started ============ HISAT2 requires a 64-bit computer running either Linux or Mac OS X and at least 8 GB of RAM. A few notes: 1) HISAT2's index (HGFM) size for the human reference genome and 12.3 million common SNPs is 6.2 GB. The SNPs consist of 11 million single nucleotide polymorphisms, 728,000 deletions, and 555,000 insertions. Insertions and deletions used in this index are small (usually <20bp). We plan to incorporate structural variations (SV) into this index. 2) The memory footprint of HISAT2 is relatively low, 6.7 GB. 3) The runtime of HISAT2 is estimated to be slightly slower than HISAT (30\u2013100% slower for some data sets). 4) HISAT2 provides greater accuracy for alignment of reads containing SNPs. 5) Use [HISAT-3N](https://daehwankimlab.github.io/hisat2/hisat-3n/) to align nucleotide converted sequencing reads include [BS-seq], [SLAM-seq], [scBS-seq], [scSLAM-seq], [TAB-seq], [oxBS-seq], [TAPS] and [EM-seq]. This alignment process requires about 10 GB of RAM. 6) HISAT2 repository is seperated with HISAT-genotype repository. Please see the link below for [HISAT-genotype repository](https://github.com/DaehwanKimLab/hisat-genotype) and [HISAT-genotype homepage](https://daehwankimlab.github.io/hisat-genotype/). ## Install git clone https://github.com/DaehwanKimLab/hisat2.git cd hisat2 make Usage ============ ## Building an index `hisat2-build` builds a HISAT2 index from a set of DNA sequences. `hisat2-build` outputs a set of 6 files with suffixes `.1.ht2`, `.2.ht2`, `.3.ht2`, `.4.ht2`, `.5.ht2`, `.6.ht2`, .`7.ht2`, and `.8.ht2`. In the case of a large index these suffixes will have a `ht2l` termination. These files together constitute the index: they are all that is needed to align reads to that reference. The original sequence FASTA files are no longer used by HISAT2 once the index is built. Example for HISAT2 index building: hisat2-build genome.fa genome ## Alignment with HISAT2 Examples alignment with HISAT2: # for single-end FASTA reads DNA alignment hisat2 -f -x genome -U reads.fa -S output.sam --no-spliced-alignment # for paired-end FASTQ reads alignment hisat2 -x genome -1 reads_1.fq -2 read2_2.fq -S output.sam ## For more information, see the following websites: * [HISAT2 website](https://daehwankimlab.github.io/hisat2) * [HISAT-3N website](https://daehwankimlab.github.io/hisat2/hisat-3n/) ## License [GPL-3.0](LICENSE) ## References [1] Sir\u00e9n J, V\u00e4lim\u00e4ki N, M\u00e4kinen V (2014) Indexing graphs for path queries with applications in genome research. IEEE/ACM Transactions on Computational Biology and Bioinformatics 11: 375\u2013388. doi: 10.1109/tcbb.2013.2297101 [2] Kim D, Langmead B, and Salzberg SL HISAT: a fast spliced aligner with low memory requirements, Nature methods, 2015 [3] Langmead B, Salzberg SL: Fast gapped-read alignment with Bowtie 2. Nat Methods 2012, 9:357-359 [HISAT2]:https://github.com/DaehwanKimLab/hisat2 [BS-seq]: https://en.wikipedia.org/wiki/Bisulfite_sequencing [SLAM-seq]: https://www.nature.com/articles/nmeth.4435 [scBS-seq]: https://www.nature.com/articles/nmeth.3035 [scSLAM-seq]: https://www.nature.com/articles/s41586-019-1369-y [TAPS]: https://www.nature.com/articles/s41587-019-0041-2 [oxBS-seq]: https://science.sciencemag.org/content/336/6083/934 [TAB-seq]: https://www.cell.com/fulltext/S0092-8674%2812%2900534-X [EM-seq]: https://genome.cshlp.org/cgi/content/long/gr.266551.120 Publication ============ * ### HISAT2 and HISAT-genotype Kim, D., Paggi, J.M., Park, C. et al. [Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype](https://www.nature.com/articles/s41587-019-0201-4). Nat Biotechnol 37, 907\u2013915 (2019) * ### HISAT-3N Zhang, Y., Park, C., Bennett, C., Thornton, M. and Kim, D [Rapid and accurate alignment of nucleotide conversion sequencing reads with HISAT-3N](https://doi.org/10.1101/gr.275193.120) Genome Research 31(7): 1290-1295 (2021)\"]```"}, {"role": "user", "content": "Content from https://github.com/DaehwanKimLab/hisat2:\n```\nRepository metadata:\n{'name': 'hisat2', 'label': ['hisat2'], 'description': [{'term': 'Graph-based alignment (Hierarchical Graph FM index)', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v2.2.1', 'v2.2.0', '0f01dc6397a', 'cba6e8cb'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/DaehwanKimLab/hisat2', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Li Song', 'type': 'person', 'email': 'mourisl@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'mourisl', 'type': 'person', 'email': 'mourisl@hotmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Yun (Leo) Zhang', 'type': 'person', 'email': 'imzhangyun@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Chanhee Park', 'type': 'person', 'email': 'parkchanhee@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Daehwan Kim', 'type': 'person', 'email': 'infphilo@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Kim Lab', 'type': 'person', 'email': '46194401+DaehwanKimLab20191011@users.noreply.github.com', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/DaehwanKimLab/hisat2/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/DaehwanKimLab/hisat2/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/DaehwanKimLab/hisat2/blob/main/docs/README.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://daehwankimlab.github.io/hisat2:\n```\nContent:\n['**HISAT2** is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes as well as to a single reference genome. Based on an extension of BWT for graphs ( [Sir\u00e9n et al. 2014](http://dl.acm.org/citation.cfm?id=2674828) ), we designed and implemented a graph FM index (GFM), an original approach and its first implementation. In addition to using one global GFM index that represents a population of human genomes, **HISAT2** uses a large set of small GFM indexes that collectively cover the whole genome. These small indexes (called local indexes), combined with several alignment strategies, enable rapid and accurate alignment of sequencing reads. This new indexing scheme is called a Hierarchical Graph FM index (HGFM). ### [The HISAT-3N paper](https://genome.cshlp.org/content/31/7/1290.abstract)published at_Genome Research_. 7/1/2021 ### HISAT-3N beta release 12/14/2020 HISAT-3N is a software system for analyzing nucleotide conversion sequencing reads. See the [HISAT-3N](/hisat2/hisat-3n/) for more details. ### Index files are moved to the AWS Public Dataset Program. 9/3/2020 We have moved HISAT2 index files to the AWS Public Dataset Program. See the [link](https://registry.opendata.aws/jhu-indexes/) for more details. ### HISAT 2.2.1 release 7/24/2020 This patch version includes the following changes. ### HISAT 2.2.0 release 2/6/2020 This major version update includes a new feature to handle \u201crepeat\u201d reads. Based on sets of 100-bp simulated and 101-bp real reads that we tested, we found that 2.6-3.4% and 1.4-1.8% of the reads were mapped to >5 locations and >100 locations, respectively. Attempting to report all alignments would likely consume a prohibitive amount of disk space. In order to address this issue, our repeat indexing and alignment approach directly aligns reads to repeat sequences, resulting in one repeat alignment per read. HISAT2 provides application programming interfaces (API) for C++, Python, and JAVA that rapidly retrieve genomic locations from repeat alignments for use in downstream analyses. Other minor bug fixes are also included as follows: * Fixed occasional sign (+ or -) issues of template lengths in SAM file * Fixed duplicate read alignments in SAM file * Skip a splice site if exon\u2019s last base or first base is ambiguous (N) ### Index files are moved to a different location. 8/30/2019 Due to a high volume of index downloads, we have moved HISAT2 index files to a different location in order to provide faster download speed. If you use wget or curl to download index files, then you may need to use the following commands to get the correct file name. * wget --content-disposition_download_link_ * curl -OJ_download_link_ ### [The HISAT2 paper](https://www.nature.com/articles/s41587-019-0201-4)is out in_Nature Biotechnology_. 8/2/2019 ### HISAT 2.1.0 release 6/8/2017 * This major version includes the first release of HISAT-genotype, which currently performs HLA typing, DNA fingerprinting analysis, and CYP typing on whole genome sequencing (WGS) reads. We plan to extend the system so that it can analyze not just a few genes, but a whole human genome. Please refer to[the HISAT-genotype website](https://daehwankimlab.github.io/hisat-genotype)for more details. * HISAT2 can be directly compiled and executed on Windows system using Visual Studio, thanks to[Nigel Dyer](http://www2.warwick.ac.uk/fac/sci/systemsbiology/staff/dyer/). * Implemented--new-summaryoption to output a new style of alignment summary, which is easier to parse for programming purposes. * Implemented--summary-fileoption to output alignment summary to a file in addition to the terminal (e.g. stderr). * Fixed discrepancy in HISAT2\u2019s alignment summary. * Implemented--no-templatelen-adjustmentoption to disable automatic template length adjustment for RNA-seq reads. ### HISAT2 2.0.5 release 11/4/2016 Version 2.0.5 is a minor release with the following changes. * Due to a policy change (HTTP to HTTPS) in using SRA data (--sra-option), users are strongly encouraged to use this version. As of 11/9/2016, NCBI will begin a permanent redirect to HTTPS, which means the previous versions of HISAT2 no longer works with--sra-accoption soon. * Implemented-Iand-Xoptions for specifying minimum and maximum fragment lengths. The options are valid only when used with--no-spliced-alignment, which is used for the alignment of DNA-seq reads. * Fixed some cases where reads with SNPs on their 5\u2019 ends were not properly aligned. * Implemented--no-softclipoption to disable soft-clipping. * Implemented--max-seedsto specify the maximum number of seeds that HISAT2 will try to extend to full-length alignments (see[the manual](/hisat2/manual/)for details). ### HISAT2 2.0.4 Windows binary available[here](http://www.di.fc.ul.pt/~afalcao/hisat2_windows.html), thanks to[Andre Osorio Falcao](http://www.di.fc.ul.pt/~afalcao/)5/24/2016 ### HISAT2 2.0.4 release 5/18/2016 Version 2.0.4 is a minor release with the following changes. * Improved template length estimation (the 9th column of the SAM format) of RNA-seq reads by taking introns into account. * Introduced two options,--remove-chrnameand--add-chrname, to remove \u201cchr\u201d from reference names or add \u201cchr\u201d to reference names in the alignment output, respectively (the 3rd column of the SAM format). * Changed the maximum of mapping quality (the 5th column of the SAM format) from 255 to 60. Note that 255 is an undefined value according to the SAM manual and some programs would not work with this value (255) properly. * Fixed NH (number of hits) in the alignment output. * HISAT2 allows indels of any length pertaining to minimum alignment score (previously, the maximum length of indels was 3 bp). * Fixed several cases that alignment goes beyond reference sequences. * Fixed reporting duplicate alignments. ### HISAT2 2.0.3-beta release 3/28/2016 Version 2.0.3-beta is a minor release with the following changes. * Fixed graph index building when using both SNPs and transcripts. As a result, genome_snp_tran indexes here on the HISAT2 website have been rebuilt. * Included some missing files needed to follow the small test example (see[the manual](/hisat2/manual/)for details). ### HISAT2 2.0.2-beta release 3/17/2016 **Note (3/19/2016):** this version is slightly updated to handle reporting splice sites with the correct chromosome names. Version 2.0.2-beta is a major release with the following changes. * Memory mappaped IO (--mmoption) works now. * Building linear index can be now done using multi-threads. * Changed the minimum score for alignment in keeping with read lengths, so it\u2019s now--score-min L,0.0,-0.2, meaning a minimum score of -20 for 100-bp reads and -30 for 150-bp reads. * Fixed a bug that the same read was written into a file multiple times when--un-concwas used. * Fixed another bug that caused reads to map beyond reference sequences. * Introduced--haplotypeoption in the hisat2-build (index building), which is used with--snpoption together to incorporate those SNP combinations present in the human population. This option also prevents graph construction from exploding due to exponential combinations of SNPs in small genomic regions. * Provided a new python script to extract SNPs and haplotypes from VCF files,_hisat2_extract_snps_haplotypes_VCF.py_ * Changed several python script names as follows<_extract_splice_sites.py_to_hisat2_extract_splice_sites.py__extract_exons.py_to_hisat2_extract_exons.py__extract_snps.py_to_hisat2_extract_snps_haplotypes_UCSC.py_ ### HISAT2 2.0.1-beta release 11/19/2015 Version 2.0.1-beta is a maintenance release with the following changes. * Fixed a bug that caused reads to map beyond reference sequences. * Fixed a deadlock issue that happened very rarely. * Fixed a bug that led to illegal memory access when reading SNP information. * Fixed a system-specific bug related to popcount instruction. ### HISAT2 2.0.0-beta release 9/8/2015 - first release We extended the BWT/FM index to incorporate genomic differences among individuals into the reference genome, while keeping memory requirements low enough to fit the entire index onto a desktop computer. Using this novel Hierarchical Graph FM index (HGFM) approach, we built a new alignment system, HISAT2, with an index that incorporates ~12.3M common SNPs from the dbSNP database. HISAT2 provides greater alignment accuracy for reads containing SNPs. * HISAT2\u2019s index size for the human reference genome and 12.3 million common SNPs is 6.2GB (the memory footprint of HISAT2 is 6.7GB). The SNPs consist of 11 million single nucleotide polymorphisms, 728,000 deletions, and 555,000 insertions. The insertions and deletions used in this index are small (usually <20bp). * HISAT2 comes with several index types:Hierarchical FM index (HFM) for a reference genome (index base:_genome_)Hierarchical Graph FM index (HGFM) for a reference genome plus SNPs (index base:_genome_snp_)Hierarchical Graph FM index (HGFM) for a reference genome plus transcripts (index base:_genome_tran_)Hierarchical Graph FM index (HGFM) for a reference genome plus SNPs and transcripts (index base:_genome_snp_tran_) * HISAT2 is a successor to both[HISAT](http://ccb.jhu.edu/software/hisat)and[TopHat2](http://ccb.jhu.edu/software/tophat). We recommend that HISAT and TopHat2 users switch to HISAT2.HISAT2 can be considered an enhanced version of HISAT with many improvements and bug fixes. The alignment speed and memory requirements of HISAT2 are virtually the same as those of HISAT when using the HFM index (_genome_).When using graph-based indexes (HGFM), the runtime of HISAT2 is slightly slower than HISAT (30~80% additional CPU time).HISAT2 allows for mapping reads directly against transcripts, similar to that of TopHat2 (use_genome_tran_or_genome_snp_tran_). * When reads contain SNPs, the SNP information is provided as an optional field in the SAM output of HISAT2 (e.g.,**Zs:Z:1|S|rs3747203,97|S|rs16990981**- see[the manual](/hisat2/manual/)for details). This feature enables fast and sensitive genotyping in downstream analyses. Note that there is no alignment penalty for mismatches, insertions, and deletions if they correspond to known SNPs. * HISAT2 provides options for transcript assemblers (e.g., StringTie and Cufflinks) to work better with the alignment from HISAT2 (see options such as--dtaand--dta-cufflinks). * Some slides about HISAT2 are found[here](/hisat2/assets/data/HISAT2-first_release-Sept_8_2015.pdf)and we are preparing detailed documention. * We plan to incorporate a larger set of SNPs and structural variations (SV) into this index (e.g., long insertions/deletions, inversions, and translocations). ### The HISAT2 source code is available in a[public GitHub repository](https://github.com/DaehwanKimLab/hisat2)(5/30/2015).']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"export2graphlan/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/export2graphlan/cmd/0.19\",\n    \"name\": \"export2graphlan\",\n    \"description\": [],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fexport2graphlan%2Fexport2graphlan%2F0.19\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/export2graphlan/cmd/0.22\",\n    \"name\": \"export2graphlan\",\n    \"description\": [\n      \"Export2graphlan is a conversion software tool for producing both annotation and tree file for GraPhlAn.\\nIt automatically generate the input tree and the annotation file for GraPhlAn, starting from the\\ninput/output of MetaPhlAn, LEfSe, and HUMAnN. It supports also the biom file format. The annotation file\\nwill highlight specific sub-trees/clades automatically inferred from input file(s) provided. The two output\\nfile of export2graphlan should then be used with GraPhlAn.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/segatalab/export2graphlan\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/segatalab/export2graphlan\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/segatalab/export2graphlan:\n```\nREADME content:\n['**export2graphlan** is a conversion software tool for producing both annotation and tree file for GraPhlAn. In particular, the annotation file tries to highlight specific sub-trees deriving automatically from input file what nodes are important. The two output file of **export2graphlan** should then be used to run ``graphlan_annotate.py``, in order to attach to the tree the derived annotations, and finally, by executing ``graphlan.py`` the user can get the output image. # PREREQUISITES # **export2graphlan** requires the following additional library: * pandas ver. 0.13.1 ([pandas](http://pandas.pydata.org/index.html)) * BIOM ver. 2.0.1 ([biom-format](http://biom-format.org), only if you have input files in BIOM format) * SciPy ([scipy](http://www.scipy.org), required by hclust2) # INSTALLATION # **export2graphlan** is available in GitHub here: [export2graphlan repository](https://github.com/SegataLab/export2graphlan) and can be obtained using: 1. [Bioconda](https://bioconda.github.io/recipes/export2graphlan/README.html) ``` $ conda install export2graphlan ``` 2. [Pypi](https://pypi.org/project/export2graphlan/) ``` $ pip install export2graphlan ``` 3. Repository ``` $ git clone git@github.com:SegataLab/export2graphlan.git ``` This will download the **export2graphlan** repository locally in the ``export2graphlan`` subfolder. You then have to put this subfolder into the system path, so that you can use **export2graphlan** from anywhere in your system: ``` $ export PATH=`pwd`/export2graphlan/:$PATH ``` Adding the above line into the bash configuration file will make the path addition permanent. For Windows or MacOS systems a similar procedure should be followed. # USAGE # ``` usage: export2graphlan.py [-h] [-i LEFSE_INPUT] [-o LEFSE_OUTPUT] -t TREE -a ANNOTATION [--annotations ANNOTATIONS] [--external_annotations EXTERNAL_ANNOTATIONS] [--background_levels BACKGROUND_LEVELS] [--background_clades BACKGROUND_CLADES] [--background_colors BACKGROUND_COLORS] [--title TITLE] [--title_font_size TITLE_FONT_SIZE] [--def_clade_size DEF_CLADE_SIZE] [--min_clade_size MIN_CLADE_SIZE] [--max_clade_size MAX_CLADE_SIZE] [--def_font_size DEF_FONT_SIZE] [--min_font_size MIN_FONT_SIZE] [--max_font_size MAX_FONT_SIZE] [--annotation_legend_font_size ANNOTATION_LEGEND_FONT_SIZE] [--abundance_threshold ABUNDANCE_THRESHOLD] [--most_abundant MOST_ABUNDANT] [--least_biomarkers LEAST_BIOMARKERS] [--discard_otus] [--internal_levels] [--biomarkers2colors BIOMARKERS2COLORS] [--sep SEP] [--out_table OUT_TABLE] [--fname_row FNAME_ROW] [--sname_row SNAME_ROW] [--metadata_rows METADATA_ROWS] [--skip_rows SKIP_ROWS] [--sperc SPERC] [--fperc FPERC] [--stop STOP] [--ftop FTOP] [--def_na DEF_NA] export2graphlan.py (ver. 0.2.1 of 27 October 2018). Convert MetaPhlAn, LEfSe, and/or HUMAnN output to GraPhlAn input format. Authors: Francesco Asnicar (f.asnicar@unitn.it) optional arguments: -h, --help show this help message and exit --annotations ANNOTATIONS List which levels should be annotated in the tree. Use a comma separate values form, e.g., --annotation_levels 1,2,3. Default is None --external_annotations EXTERNAL_ANNOTATIONS List which levels should use the external legend for the annotation. Use a comma separate values form, e.g., --annotation_levels 1,2,3. Default is None --background_levels BACKGROUND_LEVELS List which levels should be highlight with a shaded background. Use a comma separate values form, e.g., --background_levels 1,2,3. Default is None --background_clades BACKGROUND_CLADES Specify the clades that should be highlight with a shaded background. Use a comma separate values form and surround the string with \" if there are spaces. Example: --background_clades \"Bacteria.Actinobacteria, Bacteria.Bacteroidetes.Bacteroidia, Bacteria.Firmicutes.Clostridia.Clostridiales\". Default is None --background_colors BACKGROUND_COLORS Set the color to use for the shaded background. Colors can be either in RGB or HSV (using a semi-colon to separate values, surrounded with ()) format. Use a comma separate values form and surround the string with \" if it contains spaces. Example: --background_colors \"#29cc36, (150; 100; 100), (280; 80; 88)\". To use a fixed set of colors associated to a fixed set of clades, you can specify a mapping file in a tab-separated format, where the first column is the clade (using the same format as for the \"-- background_clades\" param) and the second colum is the color associated. Default is None --title TITLE If specified set the title of the GraPhlAn plot. Surround the string with \" if it contains spaces, e.g., --title \"Title example\" --title_font_size TITLE_FONT_SIZE Set the title font size. Default is 15 --def_clade_size DEF_CLADE_SIZE Set a default size for clades that are not found as biomarkers by LEfSe. Default is 10 --min_clade_size MIN_CLADE_SIZE Set the minimum value of clades that are biomarkers. Default is 20 --max_clade_size MAX_CLADE_SIZE Set the maximum value of clades that are biomarkers. Default is 200 --def_font_size DEF_FONT_SIZE Set a default font size. Default is 10 --min_font_size MIN_FONT_SIZE Set the minimum font size to use. Default is 8 --max_font_size MAX_FONT_SIZE Set the maximum font size. Default is 12 --annotation_legend_font_size ANNOTATION_LEGEND_FONT_SIZE Set the font size for the annotation legend. Default is 10 --abundance_threshold ABUNDANCE_THRESHOLD Set the minimun abundace value for a clade to be annotated. Default is 20.0 --most_abundant MOST_ABUNDANT When only lefse_input is provided, you can specify how many clades highlight. Since the biomarkers are missing, they will be chosen from the most abundant. Default is 10 --least_biomarkers LEAST_BIOMARKERS When only lefse_input is provided, you can specify the minimum number of biomarkers to extract. The taxonomy is parsed, and the level is choosen in order to have at least the specified number of biomarkers. Default is 3 --discard_otus If specified the OTU ids will be discarde from the taxonmy. Default is True, i.e. keep OTUs IDs in taxonomy --internal_levels If specified sum-up from leaf to root the abundances values. Default is False, i.e. do not sum-up abundances on the internal nodes --biomarkers2colors BIOMARKERS2COLORS Mapping file that associates biomarkers to a specific color... I\\'ll define later the specific format of this file! input parameters: You need to provide at least one of the two arguments -i LEFSE_INPUT, --lefse_input LEFSE_INPUT LEfSe input data. A file that can be given to LEfSe for biomarkers analysis. It can be the result of a MetaPhlAn or HUMAnN analysis -o LEFSE_OUTPUT, --lefse_output LEFSE_OUTPUT LEfSe output result data. The result of LEfSe analysis performed on the lefse_input file output parameters: -t TREE, --tree TREE Output filename where save the input tree for GraPhlAn -a ANNOTATION, --annotation ANNOTATION Output filename where save GraPhlAn annotation Input data matrix parameters: --sep SEP --out_table OUT_TABLE Write processed data matrix to file --fname_row FNAME_ROW row number containing the names of the features [default 0, specify -1 if no names are present in the matrix --sname_row SNAME_ROW column number containing the names of the samples [default 0, specify -1 if no names are present in the matrix --metadata_rows METADATA_ROWS Row numbers to use as metadata[default None, meaning no metadata --skip_rows SKIP_ROWS Row numbers to skip (0-indexed, comma separated) from the input file[default None, meaning no rows skipped --sperc SPERC Percentile of sample value distribution for sample selection --fperc FPERC Percentile of feature value distribution for sample selection --stop STOP Number of top samples to select (ordering based on percentile specified by --sperc) --ftop FTOP Number of top features to select (ordering based on percentile specified by --fperc) --def_na DEF_NA Set the default value for missing values [default None which means no replacement] ``` *Note*: the last input parameters (``Input data matrix parameters``) refer to the **DataMatrix** class contained in the [hclust2](https://github.com/SegataLab/hclust2) repository. # EXAMPLES # The ``examples`` folder contains the following sub-folders: ``hmp_aerobiosis``, ``hmp_metahit_metabolic``, and ``hmp_metahit_mp2``. Each example should work just by typing in a terminal window (provided that you are inside one of the example folder) the following command: ``` #!bash $ ./PIPELINE.sh ``` If everything goes well you should find in the same folder of the example six new files: ``annot.txt``, ``outimg.png``, ``outimg_annot.png``, ``outimg_legend.png``, ``outtree.txt``, and ``tree.txt``. Where: * ``annot.txt``: contains the annotation that will be used by GraPhlAn, produced by the export2graphlan.py script * ``outimg.png``: is the circular tree produced by GraPhlAn * ``outimg_annot.png``: contains the annotation legend of the circular tree * ``outimg_legend.png``: contains the legends of the highlighted biomarkers in the circular tree * ``outtree.txt``: is the annotated tree produced by graphlan_annotate.py * ``tree.txt``: is the tree produced by the export2graphlan.py script # Support # If you should find problems in using **export2graphlan** please report them in [The bioBakery help forum](https://forum.biobakery.org/).']```"}, {"role": "user", "content": "Content from https://github.com/segatalab/export2graphlan:\n```\nRepository metadata:\n{'name': 'export2graphlan', 'label': ['export2graphlan'], 'description': [{'term': 'Conversion software tool for producing both annotation and tree files for GraPhlAn', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://segatalab.github.io/tools/graphlan', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['0.22'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/SegataLab/export2graphlan', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/circular-visualization', 'term': 'circular-visualization', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/python', 'term': 'python', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/tools', 'term': 'tools', 'vocabulary': ''}, 'id': 2}], 'operations': [], 'authors': [{'term': {'name': 'fasnicar', 'type': 'person', 'email': 'francesco.asnicar@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Francesco Beghini', 'type': 'person', 'email': 'francesco.beghini@unitn.it', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Francesco Beghini', 'type': 'person', 'email': 'fbeghini@outlook.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Francesco Asnicar', 'type': 'person', 'email': 'francescoasnicar@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'duytin', 'type': 'person', 'email': 'duytin@localhost', 'maintainer': False}, 'id': 4}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/segatalab/export2graphlan/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'license', 'url': 'https://github.com/segatalab/export2graphlan/blob/main/license.txt'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/segatalab/export2graphlan/blob/main/todo-list.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"pfp/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/pfp/cmd/0.3.9\",\n    \"name\": \"pfp\",\n    \"description\": [\n      \"Prefix Free Parsing.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/marco-oliva/pfp\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/marco-oliva/pfp\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/pfp/cmd/None\",\n    \"name\": \"pfp\",\n    \"description\": [\n      \"Web-based tool for extracting and displaying continuous electrostatic positive patches on protein surfaces.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://pfp.technion.ac.il/index.html\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Inbal Paz\",\n        \"email\": \"inbalos@tx.technion.ac.il\",\n        \"maintainer\": false,\n        \"url\": \"http://yaelab.technion.ac.il/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8295310e2f3aa0b3b842c\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/marco-oliva/pfp:\n```\nREADME content:\n[\"# Prefix-Free Parsing # [![CMake](https://github.com/marco-oliva/pfp/actions/workflows/cmake.yml/badge.svg?branch=master)](https://github.com/marco-oliva/pfp/actions/workflows/cmake.yml) [![Conda](https://anaconda.org/bioconda/pfp/badges/version.svg)](https://anaconda.org/bioconda/pfp) ### Citing ### This is an implementation of prefix-free parsing that was originally proposed by: ``` @article{boucher2019prefix, title={Prefix-free parsing for building big BWTs}, author={Boucher, Christina and Gagie, Travis and Kuhnle, Alan and Langmead, Ben and Manzini, Giovanni and Mun, Taher}, journal={Algorithms for Molecular Biology}, volume={14}, number={1}, pages={1--15}, year={2019}, publisher={BioMed Central} } ``` This has been modified to allow for optimization and allows for Variant Call Format (VCF) files to be used in replace of fasta files. The publication corresponding to this work is as follows: ``` @inproceedings{oliva2022csts, title={CSTs for Terabyte-Sized Data}, author={Oliva, Marco and Cenzato, Davide and Rossi, Massimiliano and Lipt{\\\\'a}k, Zsuzsanna and Gagie, Travis and Boucher, Christina}, booktitle={2022 Data Compression Conference (DCC)}, pages={93--102}, year={2022}, organization={IEEE} } ``` This work was supported by NIH R01AI141810 and made publicly available under GNU license. If you use any parts of the repository, please acknowledge via citation of the above publications and this repository. ### Install and Use ### This tool produces the same result as running `bigbwt` on the fasta file generated as follow: ``` cat reference.fa | bcftools consensus calls.vcf.gz -H 1 > consensus.fa ``` Symbolic alleles are currently not supported, e.g. `<CN1>`. ### Bioconda ### PFP is available on `bioconda`: ```bash conda install -c bioconda -c conda-forge pfp pfp++ --help ``` ### Docker ### PFP is available on docker: ```bash docker pull moliva3/pfp:latest docker run moliva3/pfp:latest pfp++ --help ``` If using singularity: ```bash singularity pull pfp_sif docker://moliva3/pfp:latest ./pfp_sif pfp++ --help ``` ### Build ### #### Dependencies #### * Htslib * OpenMP #### Build Instructions #### ``` git clone https://github.com/marco-oliva/pfp.git cd pfp mkdir build && cd build cmake .. make ``` ### Usage ### ``` PFP++ Usage: pfp++ [OPTIONS] Options: -h,--help Print this help message and exit. -v,--vcf TEXT ... List of comma ',' separated vcf files. Assuming in genome order! -r,--ref TEXT ... List of comma ',' separated reference files. Assuming in genome order! -f,--fasta TEXT:FILE Fasta file to parse. -i,--int32t TEXT:FILE Integers file to parse. --int-shift UINT:INT in [0 - 200] Each integer i in int32t input is interpreted as (i + int-shift). -H,--haplotype TEXT Haplotype: [1,2,12]. -t,--text TEXT:FILE Text file to parse. -o,--out-prefix TEXT Output prefix. -m,--max UINT Max number of samples to analyze. -S,--samples TEXT File containing the list of samples to parse. -w,--window-size UINT:INT in [3 - 200] Sliding window size. -p,--modulo UINT:INT in [5 - 20000] Modulo used during parsing. -j,--threads UINT Number of threads. --tmp-dir TEXT:DIR Temporary files directory. -c,--compress-dictionary Also output compressed the dictionary. --use-vcf-acceleration Use reference parse to avoid re-parsing. --print-statistics Print out csv containing stats. --output-occurrences Output count for each dictionary phrase. --output-sai Output sai array. --output-last Output last array. --acgt-only Convert all non ACGT characters from a VCF or FASTA file to N. --verbose Verbose output. --version Version number. --configure Read an ini file. ```\"]```"}, {"role": "user", "content": "Content from https://github.com/marco-oliva/pfp:\n```\nRepository metadata:\n{'name': 'pfp', 'label': ['pfp'], 'description': [{'term': 'Prefix Free Parsing', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.3.9', 'v0.3.8', 'v0.3.7', 'v0.3.6', 'v0.3.5'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/marco-oliva/pfp', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Marco Oliva', 'type': 'person', 'email': '55564108+marco-oliva@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Adam Novak', 'type': 'person', 'email': 'anovak@soe.ucsc.edu', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Marco Oliva', 'type': 'person', 'email': 'marco@Marcos-MacBook-Pro.local', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Marco Oliva', 'type': 'person', 'email': 'marco.oliva@ufl.edu', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/marco-oliva/pfp/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/marco-oliva/pfp/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [{'term': {'title': 'Prefix-free parsing for building big BWTs', 'year': '2019', 'doi': '', 'url': ''}, 'id': 0}, {'term': {'title': 'CSTs for Terabyte-Sized Data', 'year': '2022', 'doi': '', 'url': ''}, 'id': 1}], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"hardklor/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/hardklor/cmd/2.3.2\",\n    \"name\": \"hardklor\",\n    \"description\": [\n      \"Analyze mass spectra.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/mhoopmann/hardklor\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/mhoopmann/hardklor\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Apache-2.0\",\n        \"url\": \"https://spdx.org/licenses/Apache-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a00dd3b8acd1a2f9b47e\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/hardklor/cmd/2.30.1\",\n    \"name\": \"hardklor\",\n    \"description\": [\n      \"Identification of features from mass spectra.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fgalaxyp%2Fhardklor%2Fhardklor%2F2.30.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/mhoopmann/hardklor:\n```\nREADME content:\n['App: Hardklor Purpose: Analyze mass spectra Version History ------------------------------------------ 1.0 - Apr 17 2007 - Stable release 1.01 - May 17 2007 - Recompiled with latest MSToolkit(1.1) 1.02 - Jun 15 2007 - Fixed monoisotopic mass bug 1.03 - Aug 29 2007 - Added sensitivity level parameter - much better low abundance peptide performance. Some changes made to Mercury 1.1 - Sep 29 2007 - Added mzXML file support and compressed .cms1 & .cms2 support. Several bugfixes, including better S/N processing. 1.11 - Jan 16 2008 - Fixed bugs in MSToolkit 64-bit file support. Modified default Hardklor settings. 1.20 - Feb 6 2008 * Fixed user-defined scan window bug. * Allows reading of centroid data (-c). No signal-to-noise processing is done for this option. * Can set a static S/N cutoff for entire scan. This optimizes Orbitrap and LTQ-FT data analysis. See documentation. * Allows substitution of a user-defined molecule for averagine (-mol). Note: still in testing stages. * Toggle isotope distribution area for output in place of base isotope peak instensity (-da). * Minor speed improvements. 1.22 - Apr 24 2008 * Minor Bug Fixes * Upgraded MSToolkit software to 2.42 1.23 - Oct 24 2008 * Improvements to spectrum dividing functions 1.24 - Dec 11 2008 * Bugfixes for zoomscan and ultrazoomscan files 1.25 - Mar 3 2009 * Additional support for ms2 file format * Bugfix to MSToolkit (forked from SVN) 1.26 - Apr 30 2009 * Upgraded MSToolkit * Bugfix for rare S/N ratio calculation error * Potentially fixed rare, irregular, and odd 64-bit linux behavior 1.27 - Aug 13 2009 * Filenames and their paths can have spaces if entire path and filename is contained in quotes 1.28 - Sep 2 2009 * Writes results in XML format with the following parameter: -xml true 1.30 - Sep 13 2009 * Bugfix to QuickCharge algorithm 1.31 - Oct 1 2009 * Added new signal detection algorithm. It operates on the assumption valid signal peaks persist over multiple adjacent spectra. Detailed description of its usage can be found in the online documentation.']```"}, {"role": "user", "content": "Content from https://github.com/mhoopmann/hardklor:\n```\nRepository metadata:\n{'name': 'hardklor', 'label': ['hardklor'], 'description': [{'term': 'Automatically exported from code.google.com/p/hardklor', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['2.3.2', '2.3.0'], 'license': [], 'repository': [{'term': 'https://github.com/mhoopmann/hardklor', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'mhoopmann', 'type': 'person', 'email': 'michael.hoopmann@systemsbiology.org', 'maintainer': False}, 'id': 0}, {'term': {'name': 'mhoopmann', 'type': 'person', 'email': 'mhoopmann@systemsbiology.org', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Michael.Hoopmann@gmail.com', 'type': 'person', 'email': 'Michael.Hoopmann@gmail.com@2478e377-7440-2193-8850-81e9d7b1eff6', 'maintainer': False}, 'id': 2}, {'term': {'name': 'hoopmann%u.washington.edu@gtempaccount.com', 'type': 'person', 'email': 'hoopmann%u.washington.edu@gtempaccount.com@2478e377-7440-2193-8850-81e9d7b1eff6', 'maintainer': False}, 'id': 3}, {'term': {'name': 'hoopmann@u.washington.edu', 'type': 'person', 'email': 'hoopmann@u.washington.edu@2478e377-7440-2193-8850-81e9d7b1eff6', 'maintainer': False}, 'id': 4}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/mhoopmann/hardklor/blob/main/README.txt'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bioconductor-deseq2/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bioconductor-deseq2/lib/1.42.0\",\n    \"name\": \"bioconductor-deseq2\",\n    \"description\": [\n      \"Estimate variance-mean dependence in count data from high-throughput sequencing assays and test for differential expression based on a model using the negative binomial distribution.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/3.18/bioc/html/DESeq2.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"LGPL-3.0-or-later\",\n        \"url\": \"https://spdx.org/licenses/LGPL-3.0-or-later.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dbfdac7ee6cb57891288\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/deseq2/cmd/2.11.40.2\",\n    \"name\": \"deseq2\",\n    \"description\": [\n      \"Determines differentially expressed features from count tables.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fdeseq2%2Fdeseq2%2F2.11.40.2\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/3.18/bioc/html/DESeq2.html:\n```\nContent:\n['This package is for version 3.18 of Bioconductor; for the stable, up-to-date release version, see [DESeq2](/packages/DESeq2/) . **Bioconductor version:** 3.18 Estimate variance-mean dependence in count data from high-throughput sequencing assays and test for differential expression based on a model using the negative binomial distribution. **Author:** Michael Love [aut, cre], Constantin Ahlmann-Eltze [ctb], Kwame Forbes [ctb], Simon Anders [aut, ctb], Wolfgang Huber [aut, ctb], RADIANT EU FP7 [fnd], NIH NHGRI [fnd], CZI [fnd] **Maintainer:** Michael Love <michaelisaiahlove at gmail.com> **Citation (from within R, entercitation(\"DESeq2\")):** To view documentation for the version of this package installed in your system, start R and enter:']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"preseq/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/preseq/cmd/3.2.0\",\n    \"name\": \"preseq\",\n    \"description\": [\n      \"Predicting library complexity and genome coverage in high-throughput sequencing.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/smithlabcode/preseq\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/smithlabcode/preseq\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-or-later\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-or-later.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/preseq/cmd/None\",\n    \"name\": \"preseq\",\n    \"description\": [\n      \"This package is aimed at predicting and number of distinct reads and how many will be expected from additional sequencing using an initial sequencing experiment. The estimates can then be used to examine the utility of further sequencing, optimize the sequencing depth, or to screen multiple libraries to avoid low complexity samples.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://smithlabresearch.org/software/preseq/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"http://smithlabresearch.org/people/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c829f210e2f3aa0b3b8989\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/smithlabcode/preseq:\n```\nREADME content:\n['[![GitHub all releases](https://img.shields.io/github/downloads/smithlabcode/preseq/total?label=GitHub%20downloads)](https://github.com/smithlabcode/preseq/releases) [![Anaconda-Server Badge](https://anaconda.org/bioconda/preseq/badges/version.svg)](https://anaconda.org/bioconda/preseq) [![Install with Conda](https://anaconda.org/bioconda/preseq/badges/platforms.svg)](https://anaconda.org/bioconda/preseq) [![Install with Conda](https://img.shields.io/conda/dn/bioconda/preseq?label=Conda%20downloads)](https://anaconda.org/bioconda/preseq) [![Documentation Status](https://readthedocs.org/projects/preseq/badge/?version=latest)](https://preseq.readthedocs.io/en/latest/?badge=latest) [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) This is the README file for the preseq package. The preseq package is aimed at predicting the yield of distinct reads from a genomic library from an initial sequencing experiment. The estimates can then be used to examine the utility of further sequencing, optimize the sequencing depth, or to screen multiple libraries to avoid low complexity samples. SYSTEM REQUIREMENTS ======================================================================== The preseq software will only run on 64-bit UNIX-like operating systems and was developed on both Linux and Mac. The preseq software requires a C++ compiler that supports C++11. INSTALLATION ======================================================================== ### Installing from a release 1. Download `preseq-x.tar.gz` from the releases tab of this repository. 2. Unpack the archive: ```console $ tar -zxvf preseq-x.tar.gz ``` 3. Move into the preseq directory and create a build directory: ```console $ cd preseq-x $ mkdir build && cd build ``` 4. Run the configuration script: ```console $ ../configure ``` If you do not want to install preseq system-wide, or if you do not have admin privileges, specify a prefix directory: ```console $ ../configure --prefix=/some/reasonable/place ``` Finally, if you want to build with HTSlib support (for the `to-mr` program) then you need to specify the following: ```console $ ../configure --enable-hts ``` And if you installed HTSlib yourself in some non-standard directory, you must specify the location like this: ```console $ ../configure --enable-hts CPPFLAGS=\\'-I /path/to/htslib/headers\\' \\\\ LDFLAGS=\\'-L/path/to/htslib/lib\\' ``` 5. Compile and install the tools: ```console $ make $ make install ``` ### Installing from source Developers looking to use the latest commits can compile the cloned repository using the `Makefile` within the `src` directory. The process is simple: ```console $ cd src/ $ make ``` If the desired input is in `.bam` format, `htslib` is required. Type ```console $ make HAVE_HTSLIB=1 all ``` The HTSLib library can be obtained here: http://www.htslib.org/download. INPUT FILE FORMATS ======================================================================== The input to preseq can be in 3 general formats: 1. Mapped read locations in BED or BAM file format. The file should be sorted by chromosome, start position, end position, and finally strand if in BED format. If the file is in BAM format, then the file should be sorted using `bamtools` or `samtools sort`. 2. The \"counts histogram\" which will have, for each count 1,2,..., the number of unique \"species\" (e.g. reads, or anything else) that appear with that count. Examples can be found in the data directory within the preseqR subdirectory. Note these should not have a count for \"0\", and they should not have any header above the counts. Just two columns of numbers, with the first column sorted and unique. 3. The counts themselves, so just a file with one count on each line. These will be made into the \"counts histogram\" inside preseq right away. USAGE EXAMPLES ======================================================================== Each program included in this software package will print a list of options if executed without any command line arguments. Many of the programs use similar options (for example, output files are specified with \\'-o\\'). We have provided a data directory to test each of our programs. Change to the `data` directory and try some of our commands. To predict the yield of a future experiment, use `lc_extrap`. For the most basic usage of `lc_extrap` to compute the expected yield, use the command on the following data: ```console $ preseq lc_extrap -o yield_estimates.txt SRR1003759_5M_subset.mr ``` If the input file is in `.bam` format, use the `-B` flag: ```console $ preseq lc_extrap -B -o yield_estimates.txt SRR1106616_5M_subset.bam ``` For the counts histogram format, use the `-H` flag: ```console $ preseq lc_extrap -H -o yield_estimates.txt SRR1301329_1M_read.txt ``` The yield estimates will appear in yield_estimates.txt, and will be a column of future experiment sizes in `TOTAL_READS`, a column of the corresponding expected distinct reads in `EXPECTED_DISTINCT`, followed by two columns giving the corresponding confidence intervals. To investigate the past yield of an experiment, use `c_curve`. `c_curve` can take in the same file formats as `lc_extrap` by using the same flags. The estimates will appear in estimates.txt with two columns. The first column gives the total number of reads in a theoretically smaller experiment and the second gives the corresponding number of distinct reads. `bound_pop` provides an estimate for the species richness of the sampled population. The input file formats and corresponding flags are identical to `c_curve` and `lc_extrap`. The output provides the median species richness in the first column and the confidence intervals in the next two columns. Finally, `gc_extrap` predicts the expected genomic coverage for a future experiment. It produces the coverage in an output format identical to `lc_extrap`. `gc_extrap` can only take in files in BED and mapped reads format (using the `-B` flag for BED): ```console $ preseq gc_extrap -B -o coverage_estimates.txt SRR1003759_5M_subset.mr ``` More data is available in the `additional_data.txt` file in the `data` directory. For an extended write-up on our programs, please read the manual in the `docs` directory. HISTORY ======================================================================== Preseq was originally developed by Timothy Daley and Andrew D. Smith at University of Southern California. **v3.2.0:** Updates to the repo in preparation for putting preseq in conda **v3.1.2:** Two headers were added. **v3.1.0:** A mode `pop_size` has been added that uses the continued fraction approximation to the Good-Toulmin model and extrapolates as far as possible. Although `bound_pop` provides a good and reliable lower-bound, this new mode will give a more accurate estimate of the population size (e.g. total number of distinct molecules). It\\'s not perfect yet, and in some cases if the population is more than a billion times larger than the sample, it will still only give a lower bound. But it works well on most data sets. **v3.0.2:** GSL has been completely removed, and a data directory has been added for users to test our programs. **v3.0.1:** We no longer require users to have GSL for all modules except for `bound_pop`. Users interested in using `bound_pop` can install GSL and follow the instructions above to configure with GSL. **v3.0.0:** The main change to this version is that if BAM/SAM format will be used as input, the HTSLib library must be installed on the system when preseq is built. Installation instructions above have been updated correspondingly. We also updated to use C++11, so a more recent compiler is required, but these days C++11 is usually supported. **v2.0.3:** A bug in defect mode was fixed and a rng seed was added to allow for reproducibility. **v2.0.0:** We have added a new module, `bound_pop`, to estimate a lower bound of the population sampled from. Interpolation is calculated by expectation rather than subsampling, dramatically improving the speed. **v1.0.2:** We have switched the dependency on the BamTools API to SAMTools, which we believe will be more convenient for most users of preseq. Minor bugs have been fixed, and algorithms have been refined to more accurately construct counts histograms and extrapolate the complexity curve. More options have been added to `lc_extrap`. `c_curve` and `lc_extrap` are now both under a single binary for easier use, and commands will now be written as `preseq lc_extrap [OPTIONS]` Furthermore, there are updates to the manual for any minor issues encountered when compiling the preseq binary. We released an R package called [preseqR](http://cran.r-project.org/web/packages/preseqR/index.html) along with preseq. This makes most of the preseq functionality available in the R statistical environment, and includes some new functionality. The preseqR directory contains all required source code to build this R package. CONTACT INFORMATION ======================================================================== Andrew D. Smith <andrewds@usc.edu> and Timothy Daley <tdaley@stanford.edu> http://smithlabresearch.org LICENSE ======================================================================== ```txt The preseq software for estimating library complexity Copyright (C) 2014-2022 Timothy Daley, Andrew D Smith, Chao Deng University of Southern California This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. ```']```"}, {"role": "user", "content": "Content from https://github.com/smithlabcode/preseq:\n```\nRepository metadata:\n{'name': 'preseq', 'label': ['preseq'], 'description': [{'term': 'Software for predicting library complexity and genome coverage in high-throughput sequencing.', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://preseq.readthedocs.io', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v3.2.0', 'v3.1.2', 'v3.1.1', 'v3.1.0', 'v3.0.2', 'v3.0.1', 'v3.0', 'v2.0.3', 'v2.0.2', 'v2.0.1', 'v2.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/smithlabcode/preseq', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/capture-recapture', 'term': 'capture-recapture', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/capture-recapture-models', 'term': 'capture-recapture-models', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/library-complexity', 'term': 'library-complexity', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/sequencing', 'term': 'sequencing', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/sequencing-coverage', 'term': 'sequencing-coverage', 'vocabulary': ''}, 'id': 5}], 'operations': [], 'authors': [{'term': {'name': 'Andrew D Smith', 'type': 'person', 'email': 'andrewds@usc.edu', 'maintainer': False}, 'id': 0}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/smithlabcode/preseq/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'docs', 'url': 'https://github.com/smithlabcode/preseq/blob/main/docs/RELEASE_NOTES.txt'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/smithlabcode/preseq/blob/main/documentation/README.md'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://smithlabresearch.org/software/preseq/:\n```\nContent:\n['The preseq package is aimed at predicting and estimating the complexity of a genomic sequencing library, equivalent to predicting and estimating the number of redundant reads from a given sequencing depth and how many will be expected from additional sequencing using an initial sequencing experiment. The estimates can then be used to examine the utility of further sequencing, optimize the sequencing depth, or to screen multiple libraries to avoid low complexity samples. Here is a link to the latest development branch at [Github.](https://github.com/smithlabcode/preseq) _**For users doing single-cell DNA sequencing:**The Preseq package includes new functionality designed for single-cell genome resequencing projects, in particular to predict the genomic coverage for single-cell DNA sequencing experiments. This functionality, which is called gc_extrap in the software, was presented at ISMB/ECCB 2013 and has recently been published online in Bioinformatics. The poster can be found[here](https://smithlabresearch.org/wp-content/uploads/ISMB2013.pdf), the paper can be found[here](http://bioinformatics.oxfordjournals.org/content/early/2014/08/08/bioinformatics.btu540.full.pdf+html), and basic usage is explained in the manual._ ### **Installing a pre-compiled binary** We provide pre-compiled binary releases for version 2.0 for your convenience. This is to save time and effort from compiling and building preseq yourself. Please choose the appropriate binary for your platform. [preseq_linux_v2.0.tar.bz2](https://smithlabresearch.org/downloads/preseq_linux_v2.0.tar.bz2) [preseq_osx_v2.0.tar.bz2](https://smithlabresearch.org/downloads/preseq_osx_v2.0.tar.bz2) In the following example, we use the Linux binary. Unpack the compressed archive $ tar -jxvf preseq_linux_v2.0.tar.bz2 you can now change into the directory $ cd preseq_linux_v2.0/ and can start using preseq. See the usage guide or manual below for more information. ### Building preseq from the source code Click to download the latest preseq [source code](https://github.com/smithlabcode/preseq/releases/download/v3.1.2/preseq-3.1.2.tar.gz) (version 3.1.2), or you can clone the latest version from our [Github repository](https://github.com/smithlabcode/preseq) . In the latter case recursive cloning is required to ensure the necessary submodules are included. ### System requirements 64-bit machine, GCC version >= 4.1 ### Installation To install preseq, download the compressed archive. Unpack it using $ tar -zxvf preseq-3.1.2.tar.gz Move into the preseq directory and configure: $ cd preseq-3.1.2 $ ./configure --prefix=/where/to/install If you would like to read in bam input , then HTSLib needs to be installed ( [http://www.htslib.org/download/](http://www.htslib.org/download/) ). Configure with the following flag: $ ./configure --enable-hts Compile and install the tools: $ make $ make install ### Quick usage guide There are five main programs in the preseq package: c_curve, lc_extrap, gc_extrap, bound_pop, and pop_size. The first two require as input a sorted bed or bam file, with duplicate reads included. bound_pop requires a sorted bed or mapped read (mr) file. The bam file can be sorted with the samtools ( [http://samtools.sourceforge.net/](http://samtools.sourceforge.net/) ) sort functions. The bed or mr file should be sorted by chromosome, start position, end position, and strand and can be done with the command line function sort $ sort -k 1,1 -k 2,2n -k 3,3n -k 6,6 input.bed > input.sort.bed c_curve computes the expected yield of distinct reads for experiments smaller than the input experiment in a .bed or .bam file through resampling. The full set of parameters can be outputed by simply typing the program name. If output.txt is the desired output file name and input.bed is the input .bed file, then simply type $ preseq c_curve -o output.txt input.sort.bed lc_extrap computes the expected future yield of distinct reads and bounds on the number of total distinct reads in the library and the associated confidence intervals. The -o parameter specifies the output file for the expected future yield. Our new module, pop_size, estimates the number of unique species in a sample, and receives input identical to lc_extrap. $ preseq lc_extrap -o yield.txt input.sort.bed If the input is in sorted bam format, then the option -B must be included. If the input is paired end, the option -P should be included. In this case only concordantly mapped reads are counted. c_curve and lc_extrap take count files, either as a text file histogram of duplicate counts or a text file column of duplicate counts, as input. The corresponding flags are -H and -V. gc_extrap computes the expected genomic coverage for deeper sequencing for single cell sequencing experiments. The input should be a mr or bed file. The tool to-mr is provided to convert sorted bam or sam files to mapped read format. $ preseq gc_extrap -o coverage_curve.txt input.sort.mr For fast estimates, lc_extrap or gc_extrap may be run in quick mode with the option -Q. This predicts the complexity without bootstrapping, so that confidence intervals will be missing. This significantly reduces the computation time. bound_pop estimates the total size of the population in the sample, known as the species richness, with a nonparametric moment-based estimator. Typical input will be a text file with a single column of duplicate counts. $ preseq bound_pop -o estim_species_richness.txt -V input.txt Preseq uses rational function approximations to Good and Toulmin\u2019s (1956) non-parametric empirical Bayes power series estimator. For more information, see the preseq [manual](https://smithlabresearch.org/manuals/preseqmanual.pdf) or [paper](http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.2375.html) , published in Nature Methods in the April 2013 issue. For more information on gc_extrap, see our [poster](https://smithlabresearch.org/wp-content/uploads/ISMB2013.pdf) presented at ISMB/ECCB 2013 in Berlin. Questions, comments, advice, or bugs can be sent to Andrew D. Smith at andrewds@usc.edu. Thank you for using preseq. ### R package An R package called preseqR makes the functionality of preseq available in the R statistical computing environment. It estimates the expected number of species as a function of the number of captures, which is called the species discovery curve or species accumulation curve in Ecology. Not only does preseqR covers the basic functionality of the preseq program, but it also functions to fit a zero-truncated negative binomial (ZTNB) distribution to the number of individuals captured for each observed species and extrapolate species accumulation curve. You can download [the R package](http://cran.r-project.org/web/packages/preseqR/index.html) from CRAN. ### The library complexity challenge You are always welcome to compare the performance of preseq with other software on predicting library complexity. [See this page for more information.](https://smithlabresearch.org/software/preseq/challenge/)']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"mumer4/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/mumer4/cmd/None\",\n    \"name\": \"mumer4\",\n    \"description\": [\n      \"System for rapidly aligning large DNA sequences to one another.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://mummer4.github.io/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Guillaume Mar\u00e7ais\",\n        \"email\": \"gmarcais@cs.cmu.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Aleksey Zimin\",\n        \"email\": \"alekseyz@ipst.umd.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c828ed10e2f3aa0b3b809f\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"sourceforge/crass/None/None\",\n    \"name\": \"crass\",\n    \"description\": [],\n    \"repository\": [\n      {\n        \"url\": \"https://sourceforge.net/projects/crass\",\n        \"kind\": \"sourceforge\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://edwards.sdsu.edu/crass/\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"sourceforge\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/crass:\n```\nProject metadata:\n{'description': None, 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['/p/crass/activity/feed', '/p/crass/activity/', 'https://sourceforge.net/auth/?return_to=%2Fprojects%2Fcrass%2F']}, {'text': 'Categories\\nBio-Informatics\\nCategories\\nBio-Informatics', 'hrefs': ['/directory/bio-informatics/']}, {'text': 'Follow crAss\\ncrAss Web Site\\nFollow crAss\\ncrAss Web Site', 'hrefs': ['http://edwards.sdsu.edu/crass/']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof crAss!\\nUser Reviews\\nBe the first to\\npost a review\\nof crAss!', 'hrefs': ['/projects/crass/reviews/new']}, {'text': 'Additional Project Details\\nProgramming Language\\nPerl\\nRelated Categories\\nPerl Bio-Informatics Software\\nRegistered\\n2011-11-25\\nAdditional Project Details\\nProgramming Language\\nPerl\\nRelated Categories\\nPerl Bio-Informatics Software\\nRegistered\\n2011-11-25', 'hrefs': ['/directory/perl/', '/directory/bio-informatics/perl/']}]}```"}, {"role": "user", "content": "Content from https://mummer4.github.io/:\n```\nContent:\n['# Description MUMmer is a system for rapidly aligning large DNA sequences to one another. It can align: * whole genomes to other genomes * large genome assemblies to one another * partial (draft) genomes sequences to one another * or (with release 4) a set of reads to a genome. MUMmer is very fast and easy to run. The current version, release 4.x, can find all 20-bp maximal exact matches between two bacterial genomes in just a few seconds on a typical desktop or laptop computer. MUMmer handles the 100s or 1000s of contigs from a draft genome with ease, and will align them to another set of contigs using the nucmer utility included with the system. The promer utility takes this a step further by generating alignments based upon the six-frame translations of both input sequences. Most of the improvements in release 4.x are to nucmer and MUMmer. Promer is unchanged from release 3.0. See the nucmer and promer readme files in the \"docs/\" subdirectory for more details. MUMmer is free and open source, so all we ask is that you cite our most recent paper in any publications that use it. Here are the relevant publications: #### Version 4.0+ MUMmer4 and nucmer4 are described in \" [MUMmer4: A fast and versatile genome alignment system](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005944) \" G. Mar\u00e7ais , A.L. Delcher, A.M. Phillippy, R. Coston, S.L. Salzberg, A. Zimin, _PLoS computational biology_ (2018), 14(1): e1005944. #### Version 3.0 Open source MUMmer 3.0 is described in \" [Versatile and open software for comparing large genomes](publications/MUMmer3.pdf) .\" S. Kurtz, A. Phillippy, A.L. Delcher, M. Smoot, M. Shumway, C. Antonescu, and S.L. Salzberg, _Genome Biology_ (2004), 5:R12. #### Version 2.1 MUMmer 2.1, NUCmer, and PROmer are described in \" [Fast Algorithms for Large-scale Genome Alignment and Comparision](publications/MUMmer2.pdf) .\" A.L. Delcher, A. Phillippy, J. Carlton, and S.L. Salzberg, _Nucleic Acids Research_ (2002), Vol. 30, No. 11 2478-2483. #### Version 1.0 MUMmer 1.0 is described in \" [Alignment of Whole Genomes](publications/MUMmer.pdf) .\" A.L. Delcher, S. Kasif, R.D. Fleischmann, J. Peterson, O. White, and S.L. Salzberg, _Nucleic Acids Research_ , 27:11 (1999), 2369-2376.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"orientagraph/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/orientagraph/cmd/1.1\",\n    \"name\": \"orientagraph\",\n    \"description\": [\n      \"OrientAGraph enables Maximum Likelihood Network Orientation (MNLO), as a standalone routine or as a search heuristic within TreeMix, a popular package for estimating admixture graphs from f-statistics (or related quantities).\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/sriramlab/OrientAGraph\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/sriramlab/OrientAGraph\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82ea910e2f3aa0b3ba6a2\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/orientagraph/cmd/None\",\n    \"name\": \"orientagraph\",\n    \"description\": [\n      \"Advancing admixture graph estimation via maximum likelihood network orientation.\\n\\nOrientAGraph enables Maximum Likelihood Network Orientation (MNLO) to be utilized as a search heuristic within TreeMix, a popular package for estimating admixture graphs. We found that MLNO improved (or else did not impact) the accuracy of the original TreeMix method in an experimental study.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/ekmolloy/OrientAGraph\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Erin K. Molloy\",\n        \"email\": \"ekmolloy@cs.ucla.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82ea910e2f3aa0b3ba6a2\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/sriramlab/OrientAGraph:\n```\nREADME content:\n['OrientAGraph ============ OrientAGraph implements **Maximum Likelihood Network Orientation (MNLO)** within [TreeMix](https://doi.org/10.1371/journal.pgen.1002967), a popular package for estimating admixture graphs from f-statistics (and related quantities). In our experimental study, we found that MLNO either improved or else did not impact the accuracy of the original TreeMix search heuristic. To learn more, check out [this paper](https://doi.org/10.1093/bioinformatics/btab267) with Arun Durvasula and Sriram Sankararaman. **Starting in version 1.2:** By default, OrientAGraph searches for the MLNO only after each of the first two admixture edges are added (equivalent to using the flag: `-mlno 1,2`). To get started, see the installation instructions below as well as [this example](example/arctic-data/README.md). Other Useages ------------- + To execute OrientAGraph as described in our original paper, you must include the `-allmigs` and `-mlno` flags to the end of your command. These options expand the maximum likelihood search but may be prohibitively expensive for large populations! + Based on our experience, it is important to use the `-root <outgroup>` flag. This option roots the starting tree at the user-specified outgroup before starting the network search. + To determine how to set the other options, we recommend reading the [TreeMix manual](https://bitbucket.org/nygcresearch/treemix/downloads/). + There are also some options specific to OrientAGraph. As an example, OrientAGraph can be used to find the MLNO of a user-provided graph (option: `-gf <vertex file> <edge file> -score mlno`). Acknowledgements ---------------- OrientAGraph is built from the [TreeMix code](https://bitbucket.org/nygcresearch/treemix/src/master/) by J.K. Pickrell and J.K. Pritchard, and like the TreeMix code, is provided under the [GNU General Public License v3.0](LICENSE). TreeMix is presented in [Pickrell and Pritchard (2012)](https://doi.org/10.1371/journal.pgen.1002967) and in [Pickrell et al. (2012)](https://doi.org/10.1038/ncomms2140). OrientAGraph implements algorithms / utilizes theoretical results from [Huber et al. (2019)](https://arxiv.org/abs/1906.07430) and [Francis and Steel (2015)](https://doi.org/10.1093/sysbio/syv037). OrientAGraph has only been tested on Mac (using Apple clang version 12.0.0) and Linux (using gcc versions 4.8.5 and 4.9.3), both with GSL version 2.6. Installation on MAC OS with [homebrew](https://brew.sh) ------------------------------------------------------- 1. Install GSL (we used version 2.7.1) ``` brew install gsl ``` 2. Export the path information for GSL. ``` GSL_VERSION=$(ls /opt/homebrew/Cellar/gsl/) export INCLUDE_PATH=\"/opt/homebrew/Cellar/gsl/${GSL_VERSION}/include\" export LIBRARY_PATH=\"/opt/homebrew/Cellar/gsl/${GSL_VERSION}/lib\" ``` Now check if these paths are working by typing ``` ls $INCLUDE_PATH ``` which should return `gsl` and typing ``` ls $LIBRARY_PATH ``` which should return `libgsl.a` among other files. 3. Install BOOST (we used version 1.83.0). ``` brew install boost ``` 4. Export the path information for BOOST. ``` BOOST_VERSION=$(ls /opt/homebrew/Cellar/boost/) export BOOST_PATH=\"/opt/homebrew/Cellar/boost/${BOOST_VERSION}\" ``` Now check to see if these paths are working by typing `ls $BOOST_PATH` which should return `include` and `lib`, among other files. 5. Now download and build OrientAGraph. ``` git clone https://github.com/ekmolloy/OrientAGraph.git cd OrientAGraph ./configure CPPFLAGS=-I${INCLUDE_PATH} LDFLAGS=\"-L${LIBRARY_PATH}\" --with-boost=\"${BOOST_PATH}\" make ``` 6. If everything has gone well, typing ``` source ~/.bash_profile orientagraph ``` will produce the help message: ``` OrientAGraph 1.2 OrientAGraph is built from TreeMix v1.13 Revision 231 by J.K. Pickrell and J.K. Pritchard and implements several new features, including Maximum Likelihood Network Orientation (MLNO), which can be used as a graph search heuristic. Contact: Erin Molloy (ekmolloy@umd.edu) COMMAND: ./src/orientagraph -h TreeMix Options: -h Display this help -i [file] Input file (e.g. containing allele frequencies) -o [stem] Output prefix (i.e. output will be [stem].treeout.gz, [stem].cov.gz, [stem].modelcov.gz, etc.) -k [int] Number of SNPs per block for estimation of covariance matrix (1) -global Do a round of global rearrangements after adding all populations -tf [newick file] Read tree from a file, rather than estimating it -m [int] Number of migration edges to add (default: 0) -root [string] Comma-delimited list of populations to put on one side of root -gf [vertices file] [edges file] Read graph from files (e.g. [stem].vertices.gz and [stem].edges.gz from a previous TreeMix run) -se Calculate standard errors of migration weights (computationally expensive) -micro Input is microsatellite data -bootstrap Perform a single bootstrap replicate -cor_mig [file] List of known migration events to include (also use -climb) -noss Turn off sample size correction -seed [int] Set the seed for random number generation -n_warn [int] Display first N warnings Options added for OrientAGraph: -freq2stat Estimate covariances or f2-statistics from allele frequencies and then exit; the resulting files can be given as input using the -givenmat option -givenmat [matrix file] Allows user to input matrix (e.g. [stem].cov.gz) with the -i flag, the file after this flag should contain the standard error (e.g. [stem].covse.gz) -refit Refit model parameters on starting tree (-tf) or graph (-gf) -score [string] Score input tree (-tf) or graph (-gf) and then exit: \\'asis\\' = score \\'as is\\' i.e. without refitting, \\'rfit\\' = score after refitting (default), \\'mlbt\\' = score each base tree (with refitting) and return best, \\'mlno\\' = score each network orientation (with refitting) and return best -mlno [string] Comma-delimited list of integers, indicating when to run maximum likelihood network orientation (MLNO) as part of heuristic search e.g. \\'1,2\\' means run MLNO only after adding the first two migration edges (default) \\'0\\' means do NOT run MLNO \\'\\' (no string) means run MLNO after adding each migration edge -allmigs [string] Comma-delimited list of integers, indicating when to run evaluate all legal ways of adding migration edge to base tree instead of using heuristic (similar to -mlno but default is -allmigs 0) -popaddorder [population list file] Order to add populations when building starting tree -checkpoint Write checkpoint files ``` You can also check the compile by typing ``` file src/orientagraph ``` which should produce something like ``` src/orientagraph: Mach-O 64-bit executable arm64 ``` 7. To use orientagraph from any directory on your system, you can add its path to your profile. Check the path with the `pwd` command, then add the following line ``` export PATH=<result of pwd>/src:$PATH\" ``` is for the bash profile. Similar commands exist for other profiles. Installation on Linux ---------------------- A similar installation can be done for linux but you will need to use apt-get (or some other tool) instead of homebrew. If you are using a Linux system, you can ask your system admin about how to access GSL and BOOST; however, you may not be able to compile static binaries then. Installation from source ------------------------ 1. Download OrientAGraph. ``` git clone https://github.com/ekmolloy/OrientAGraph.git cd OrientAGraph ``` 2. Build and install BOOST. ``` wget https://boostorg.jfrog.io/artifactory/main/release/1.82.0/source/boost_1_82_0.tar.gz tar -zxvf boost_1_82_0.tar.gz cd boost_1_82_0 mv install old-install mkdir install BOOST_PATH=\"$(pwd)/install\" ./bootstrap.sh --prefix=\"$BOOST_PATH\" ./b2 install cd .. ``` Check there are static libs ``` ls ${BOOST_PATH}/lib/*a ``` should return a bunch of files. 3. Build and install GSL. ``` wget https://ftp.gnu.org/gnu/gsl/gsl-2.7.1.tar.gz tar -zxvf gsl-2.7.1.tar.gz cd gsl-2.7.1 mv install old-install mkdir install export GSL_PATH=\"$(pwd)/install\" export INCLUDE_PATH=\"${GSL_PATH}/include\" export LIBRARY_PATH=\"${GSL_PATH}/lib\" ./configure --prefix=\"$GSL_PATH\" make make check make install cd .. ``` Check there are static libs ``` ls $LIBRARY_PATH/*a ``` should return `libgsl.a` and `libgslcblas.a`. 4. Build OrientAGraph ``` ./configure CPPFLAGS=-I${INCLUDE_PATH} LDFLAGS=\"-L${LIBRARY_PATH} -static\" --with-boost=\"${BOOST_PATH}\" make ``` If using Mac, remove `-static` because you cannot really make static binaries for Mac. 4. Check if the build is static with command ``` ldd ./src/orientagraph ``` should return `not a dynamic executable` If using Mac, try `otool -l ./src/orientagraph`.']```"}, {"role": "user", "content": "Content from https://github.com/sriramlab/OrientAGraph:\n```\nRepository metadata:\n{'name': 'OrientAGraph', 'label': ['OrientAGraph'], 'description': [], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['1.2', '1.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/sriramlab/OrientAGraph', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Erin  Molloy', 'type': 'person', 'email': 'ekmolloy@erins-mbp.myfiosgateway.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Erin Molloy', 'type': 'person', 'email': 'ekmolloy@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Erin  Molloy', 'type': 'person', 'email': 'ekmolloy@wireless-10-104-203-118.umd.edu', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Erin  Molloy', 'type': 'person', 'email': 'ekmolloy@wireless-10-104-203-45.umd.edu', 'maintainer': False}, 'id': 3}, {'term': {'name': 'ekmolloy', 'type': 'person', 'email': 'molloy.erin.k@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Erin Katherine Molloy', 'type': 'person', 'email': 'ekmolloy@veena.cs.ucla.edu', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/sriramlab/OrientAGraph/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/ekmolloy/OrientAGraph:\n```\nREADME content:\n['OrientAGraph ============ OrientAGraph implements **Maximum Likelihood Network Orientation (MNLO)** within [TreeMix](https://doi.org/10.1371/journal.pgen.1002967), a popular package for estimating admixture graphs from f-statistics (and related quantities). In our experimental study, we found that MLNO either improved or else did not impact the accuracy of the original TreeMix search heuristic. To learn more, check out [this paper](https://doi.org/10.1093/bioinformatics/btab267) with Arun Durvasula and Sriram Sankararaman. **Starting in version 1.2:** By default, OrientAGraph searches for the MLNO only after each of the first two admixture edges are added (equivalent to using the flag: `-mlno 1,2`). To get started, see the installation instructions below as well as [this example](example/arctic-data/README.md). Other Useages ------------- + To execute OrientAGraph as described in our original paper, you must include the `-allmigs` and `-mlno` flags to the end of your command. These options expand the maximum likelihood search but may be prohibitively expensive for large populations! + Based on our experience, it is important to use the `-root <outgroup>` flag. This option roots the starting tree at the user-specified outgroup before starting the network search. + To determine how to set the other options, we recommend reading the [TreeMix manual](https://bitbucket.org/nygcresearch/treemix/downloads/). + There are also some options specific to OrientAGraph. As an example, OrientAGraph can be used to find the MLNO of a user-provided graph (option: `-gf <vertex file> <edge file> -score mlno`). Acknowledgements ---------------- OrientAGraph is built from the [TreeMix code](https://bitbucket.org/nygcresearch/treemix/src/master/) by J.K. Pickrell and J.K. Pritchard, and like the TreeMix code, is provided under the [GNU General Public License v3.0](LICENSE). TreeMix is presented in [Pickrell and Pritchard (2012)](https://doi.org/10.1371/journal.pgen.1002967) and in [Pickrell et al. (2012)](https://doi.org/10.1038/ncomms2140). OrientAGraph implements algorithms / utilizes theoretical results from [Huber et al. (2019)](https://arxiv.org/abs/1906.07430) and [Francis and Steel (2015)](https://doi.org/10.1093/sysbio/syv037). OrientAGraph has only been tested on Mac (using Apple clang version 12.0.0) and Linux (using gcc versions 4.8.5 and 4.9.3), both with GSL version 2.6. Installation on MAC OS with [homebrew](https://brew.sh) ------------------------------------------------------- 1. Install GSL (we used version 2.7.1) ``` brew install gsl ``` 2. Export the path information for GSL. ``` GSL_VERSION=$(ls /opt/homebrew/Cellar/gsl/) export INCLUDE_PATH=\"/opt/homebrew/Cellar/gsl/${GSL_VERSION}/include\" export LIBRARY_PATH=\"/opt/homebrew/Cellar/gsl/${GSL_VERSION}/lib\" ``` Now check if these paths are working by typing ``` ls $INCLUDE_PATH ``` which should return `gsl` and typing ``` ls $LIBRARY_PATH ``` which should return `libgsl.a` among other files. 3. Install BOOST (we used version 1.83.0). ``` brew install boost ``` 4. Export the path information for BOOST. ``` BOOST_VERSION=$(ls /opt/homebrew/Cellar/boost/) export BOOST_PATH=\"/opt/homebrew/Cellar/boost/${BOOST_VERSION}\" ``` Now check to see if these paths are working by typing `ls $BOOST_PATH` which should return `include` and `lib`, among other files. 5. Now download and build OrientAGraph. ``` git clone https://github.com/ekmolloy/OrientAGraph.git cd OrientAGraph ./configure CPPFLAGS=-I${INCLUDE_PATH} LDFLAGS=\"-L${LIBRARY_PATH}\" --with-boost=\"${BOOST_PATH}\" make ``` 6. If everything has gone well, typing ``` source ~/.bash_profile orientagraph ``` will produce the help message: ``` OrientAGraph 1.2 OrientAGraph is built from TreeMix v1.13 Revision 231 by J.K. Pickrell and J.K. Pritchard and implements several new features, including Maximum Likelihood Network Orientation (MLNO), which can be used as a graph search heuristic. Contact: Erin Molloy (ekmolloy@umd.edu) COMMAND: ./src/orientagraph -h TreeMix Options: -h Display this help -i [file] Input file (e.g. containing allele frequencies) -o [stem] Output prefix (i.e. output will be [stem].treeout.gz, [stem].cov.gz, [stem].modelcov.gz, etc.) -k [int] Number of SNPs per block for estimation of covariance matrix (1) -global Do a round of global rearrangements after adding all populations -tf [newick file] Read tree from a file, rather than estimating it -m [int] Number of migration edges to add (default: 0) -root [string] Comma-delimited list of populations to put on one side of root -gf [vertices file] [edges file] Read graph from files (e.g. [stem].vertices.gz and [stem].edges.gz from a previous TreeMix run) -se Calculate standard errors of migration weights (computationally expensive) -micro Input is microsatellite data -bootstrap Perform a single bootstrap replicate -cor_mig [file] List of known migration events to include (also use -climb) -noss Turn off sample size correction -seed [int] Set the seed for random number generation -n_warn [int] Display first N warnings Options added for OrientAGraph: -freq2stat Estimate covariances or f2-statistics from allele frequencies and then exit; the resulting files can be given as input using the -givenmat option -givenmat [matrix file] Allows user to input matrix (e.g. [stem].cov.gz) with the -i flag, the file after this flag should contain the standard error (e.g. [stem].covse.gz) -refit Refit model parameters on starting tree (-tf) or graph (-gf) -score [string] Score input tree (-tf) or graph (-gf) and then exit: \\'asis\\' = score \\'as is\\' i.e. without refitting, \\'rfit\\' = score after refitting (default), \\'mlbt\\' = score each base tree (with refitting) and return best, \\'mlno\\' = score each network orientation (with refitting) and return best -mlno [string] Comma-delimited list of integers, indicating when to run maximum likelihood network orientation (MLNO) as part of heuristic search e.g. \\'1,2\\' means run MLNO only after adding the first two migration edges (default) \\'0\\' means do NOT run MLNO \\'\\' (no string) means run MLNO after adding each migration edge -allmigs [string] Comma-delimited list of integers, indicating when to run evaluate all legal ways of adding migration edge to base tree instead of using heuristic (similar to -mlno but default is -allmigs 0) -popaddorder [population list file] Order to add populations when building starting tree -checkpoint Write checkpoint files ``` You can also check the compile by typing ``` file src/orientagraph ``` which should produce something like ``` src/orientagraph: Mach-O 64-bit executable arm64 ``` 7. To use orientagraph from any directory on your system, you can add its path to your profile. Check the path with the `pwd` command, then add the following line ``` export PATH=<result of pwd>/src:$PATH\" ``` is for the bash profile. Similar commands exist for other profiles. Installation on Linux ---------------------- A similar installation can be done for linux but you will need to use apt-get (or some other tool) instead of homebrew. If you are using a Linux system, you can ask your system admin about how to access GSL and BOOST; however, you may not be able to compile static binaries then. Installation from source ------------------------ 1. Download OrientAGraph. ``` git clone https://github.com/ekmolloy/OrientAGraph.git cd OrientAGraph ``` 2. Build and install BOOST. ``` wget https://boostorg.jfrog.io/artifactory/main/release/1.82.0/source/boost_1_82_0.tar.gz tar -zxvf boost_1_82_0.tar.gz cd boost_1_82_0 mv install old-install mkdir install BOOST_PATH=\"$(pwd)/install\" ./bootstrap.sh --prefix=\"$BOOST_PATH\" ./b2 install cd .. ``` Check there are static libs ``` ls ${BOOST_PATH}/lib/*a ``` should return a bunch of files. 3. Build and install GSL. ``` wget https://ftp.gnu.org/gnu/gsl/gsl-2.7.1.tar.gz tar -zxvf gsl-2.7.1.tar.gz cd gsl-2.7.1 mv install old-install mkdir install export GSL_PATH=\"$(pwd)/install\" export INCLUDE_PATH=\"${GSL_PATH}/include\" export LIBRARY_PATH=\"${GSL_PATH}/lib\" ./configure --prefix=\"$GSL_PATH\" make make check make install cd .. ``` Check there are static libs ``` ls $LIBRARY_PATH/*a ``` should return `libgsl.a` and `libgslcblas.a`. 4. Build OrientAGraph ``` ./configure CPPFLAGS=-I${INCLUDE_PATH} LDFLAGS=\"-L${LIBRARY_PATH} -static\" --with-boost=\"${BOOST_PATH}\" make ``` If using Mac, remove `-static` because you cannot really make static binaries for Mac. 4. Check if the build is static with command ``` ldd ./src/orientagraph ``` should return `not a dynamic executable` If using Mac, try `otool -l ./src/orientagraph`.']```"}, {"role": "user", "content": "Content from https://github.com/ekmolloy/OrientAGraph:\n```\nRepository metadata:\n{'name': 'OrientAGraph', 'label': ['OrientAGraph'], 'description': [], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['1.2', '1.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/sriramlab/OrientAGraph', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Erin  Molloy', 'type': 'person', 'email': 'ekmolloy@erins-mbp.myfiosgateway.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Erin Molloy', 'type': 'person', 'email': 'ekmolloy@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Erin  Molloy', 'type': 'person', 'email': 'ekmolloy@wireless-10-104-203-118.umd.edu', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Erin  Molloy', 'type': 'person', 'email': 'ekmolloy@wireless-10-104-203-45.umd.edu', 'maintainer': False}, 'id': 3}, {'term': {'name': 'ekmolloy', 'type': 'person', 'email': 'molloy.erin.k@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Erin Katherine Molloy', 'type': 'person', 'email': 'ekmolloy@veena.cs.ucla.edu', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/sriramlab/OrientAGraph/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"biotransformer/*": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/biotransformer/app/3.1.0\",\n    \"name\": \"biotransformer\",\n    \"description\": [\n      \"BioTransformer is a freely available web server that supports accurate, rapid and comprehensive in silico metabolism prediction.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://biotransformer.ca/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"David S Wishart\",\n        \"email\": \"david.wishart@ualberta.ca\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Thomas O Metz\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8358910e2f3aa0b3bc389\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/biotransformer/cmd/3.0.20230403\",\n    \"name\": \"biotransformer\",\n    \"description\": [\n      \"BioTransformer is a software tool that predicts small molecule metabolism in mammals, their gut microbiota,\\nas well as the soil/aquatic microbiota. BioTransformer also assists scientists in metabolite identification,\\nbased on the metabolism prediction.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://bitbucket.org/wishartlab/biotransformer3.0jar\",\n        \"kind\": \"bitbucket\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://bitbucket.org/wishartlab/biotransformer3.0jar/src\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dc9eac7ee6cb57891314\"\n      },\n      {\n        \"$oid\": \"67e2dc0cac7ee6cb57891297\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bitbucket.org/wishartlab/biotransformer3.0jar/src:\n```\nRepository metadata:\n{'type': 'repository', 'full_name': 'wishartlab/biotransformer3.0jar', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar'}, 'html': {'href': 'https://bitbucket.org/wishartlab/biotransformer3.0jar'}, 'avatar': {'href': 'https://bytebucket.org/ravatar/%7Ba4f73fc7-1db2-48e5-9cee-519c67db472b%7D?ts=default'}, 'pullrequests': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/pullrequests'}, 'commits': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/commits'}, 'forks': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/forks'}, 'watchers': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/watchers'}, 'branches': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/refs/branches'}, 'tags': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/refs/tags'}, 'downloads': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/downloads'}, 'source': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/src'}, 'clone': [{'name': 'https', 'href': 'https://bitbucket.org/wishartlab/biotransformer3.0jar.git'}, {'name': 'ssh', 'href': 'git@bitbucket.org:wishartlab/biotransformer3.0jar.git'}], 'hooks': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/hooks'}}, 'name': 'BioTransformer3.0Jar', 'slug': 'biotransformer3.0jar', 'description': '', 'scm': 'git', 'website': None, 'owner': {'display_name': 'Wishartlab', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/%7B30aefd2c-1c20-4326-8a48-37bd6f5a9631%7D'}, 'avatar': {'href': 'https://bitbucket.org/account/wishartlab/avatar/'}, 'html': {'href': 'https://bitbucket.org/%7B30aefd2c-1c20-4326-8a48-37bd6f5a9631%7D/'}}, 'type': 'team', 'uuid': '{30aefd2c-1c20-4326-8a48-37bd6f5a9631}', 'username': 'wishartlab'}, 'workspace': {'type': 'workspace', 'uuid': '{30aefd2c-1c20-4326-8a48-37bd6f5a9631}', 'name': 'Wishartlab', 'slug': 'wishartlab', 'links': {'avatar': {'href': 'https://bitbucket.org/workspaces/wishartlab/avatar/?ts=1673901073'}, 'html': {'href': 'https://bitbucket.org/wishartlab/'}, 'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/wishartlab'}}}, 'is_private': False, 'project': {'type': 'project', 'key': 'PROJ', 'uuid': '{d00ef0a1-9d80-4110-8d75-deee19769c7e}', 'name': 'General', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/wishartlab/projects/PROJ'}, 'html': {'href': 'https://bitbucket.org/wishartlab/workspace/projects/PROJ'}, 'avatar': {'href': 'https://bitbucket.org/wishartlab/workspace/projects/PROJ/avatar/32?ts=1464787908'}}}, 'fork_policy': 'allow_forks', 'created_on': '2021-11-23T18:26:51.552948+00:00', 'updated_on': '2023-05-25T20:43:37.922916+00:00', 'size': 2139955905, 'language': '', 'uuid': '{a4f73fc7-1db2-48e5-9cee-519c67db472b}', 'mainbranch': {'name': 'master', 'type': 'branch'}, 'override_settings': {'default_merge_strategy': False, 'branching_model': False}, 'parent': None, 'enforced_signed_commits': None, 'has_issues': False, 'has_wiki': True}```"}, {"role": "user", "content": "Content from https://biotransformer.ca/:\n```\nContent:\n['Quantitative metabolomics services for biomarker discovery and validation. Specializing in ready to use metabolomics kits. Your source for quantitative metabolomics technologies and bioinformatics.']```"}, {"role": "user", "content": "Content from https://bitbucket.org/wishartlab/biotransformer3.0jar:\n```\nRepository metadata:\n{'type': 'repository', 'full_name': 'wishartlab/biotransformer3.0jar', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar'}, 'html': {'href': 'https://bitbucket.org/wishartlab/biotransformer3.0jar'}, 'avatar': {'href': 'https://bytebucket.org/ravatar/%7Ba4f73fc7-1db2-48e5-9cee-519c67db472b%7D?ts=default'}, 'pullrequests': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/pullrequests'}, 'commits': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/commits'}, 'forks': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/forks'}, 'watchers': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/watchers'}, 'branches': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/refs/branches'}, 'tags': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/refs/tags'}, 'downloads': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/downloads'}, 'source': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/src'}, 'clone': [{'name': 'https', 'href': 'https://bitbucket.org/wishartlab/biotransformer3.0jar.git'}, {'name': 'ssh', 'href': 'git@bitbucket.org:wishartlab/biotransformer3.0jar.git'}], 'hooks': {'href': 'https://api.bitbucket.org/2.0/repositories/wishartlab/biotransformer3.0jar/hooks'}}, 'name': 'BioTransformer3.0Jar', 'slug': 'biotransformer3.0jar', 'description': '', 'scm': 'git', 'website': None, 'owner': {'display_name': 'Wishartlab', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/%7B30aefd2c-1c20-4326-8a48-37bd6f5a9631%7D'}, 'avatar': {'href': 'https://bitbucket.org/account/wishartlab/avatar/'}, 'html': {'href': 'https://bitbucket.org/%7B30aefd2c-1c20-4326-8a48-37bd6f5a9631%7D/'}}, 'type': 'team', 'uuid': '{30aefd2c-1c20-4326-8a48-37bd6f5a9631}', 'username': 'wishartlab'}, 'workspace': {'type': 'workspace', 'uuid': '{30aefd2c-1c20-4326-8a48-37bd6f5a9631}', 'name': 'Wishartlab', 'slug': 'wishartlab', 'links': {'avatar': {'href': 'https://bitbucket.org/workspaces/wishartlab/avatar/?ts=1673901073'}, 'html': {'href': 'https://bitbucket.org/wishartlab/'}, 'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/wishartlab'}}}, 'is_private': False, 'project': {'type': 'project', 'key': 'PROJ', 'uuid': '{d00ef0a1-9d80-4110-8d75-deee19769c7e}', 'name': 'General', 'links': {'self': {'href': 'https://api.bitbucket.org/2.0/workspaces/wishartlab/projects/PROJ'}, 'html': {'href': 'https://bitbucket.org/wishartlab/workspace/projects/PROJ'}, 'avatar': {'href': 'https://bitbucket.org/wishartlab/workspace/projects/PROJ/avatar/32?ts=1464787908'}}}, 'fork_policy': 'allow_forks', 'created_on': '2021-11-23T18:26:51.552948+00:00', 'updated_on': '2023-05-25T20:43:37.922916+00:00', 'size': 2139955905, 'language': '', 'uuid': '{a4f73fc7-1db2-48e5-9cee-519c67db472b}', 'mainbranch': {'name': 'master', 'type': 'branch'}, 'override_settings': {'default_merge_strategy': False, 'branching_model': False}, 'parent': None, 'enforced_signed_commits': None, 'has_issues': False, 'has_wiki': True}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"odamnet/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/odamnet/cmd/1.1.0\",\n    \"name\": \"odamnet\",\n    \"description\": [\n      \"Study molecular relationships between chemicals and rare diseases.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/MOohTus/ODAMNet\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://pypi.org/project/ODAMNet/1.1.0/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/odamnet/cmd/None\",\n    \"name\": \"odamnet\",\n    \"description\": [\n      \"A Python package to study molecular relationship between environmental factors and rare diseases.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://pypi.org/project/ODAMNet/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Morgane T\u00e9r\u00e9zol\",\n        \"email\": \"morgane.terezol@univ-amu.fr\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Ana\u00efs Baudot\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Ozan Ozisik\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://pypi.org/project/ODAMNet/:\n```\nProject metadata:\n{'author': 'Morgane T\u00e9r\u00e9zol', 'author_email': '', 'classifiers': ['Environment :: Console', 'License :: OSI Approved :: MIT License', 'Operating System :: POSIX :: Linux', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.9'], 'description': \"# README\\n\\nODAMNet is a Python package to study molecular relationship between environmental factors (called chemicals here) and \\nrare diseases. \\n\\nThe [ODAMNet documentation][ODAMNet documentation] is available in ReadTheDocs.\\n\\nThis tool was created within the framework of the [EJRP-RD project][EJPRD].\\n\\n## Installation \\n\\n### From PyPI\\n\\nODAMNet is available in [Python package][pypi]. You can easily install it using `pip`.\\n\\n```console\\n$ python3 -m pip install odamnet\\n```\\n\\n### From Conda\\n\\nODAMNet is also available in [bioconda][bioconda] using ``conda``.\\n\\n```console\\n$ conda install odamnet\\n```\\n\\n### From Github\\n\\n1. Clone the repository from GitHub\\n\\n```console\\n$ git clone https://github.com/MOohTus/ODAMNet.git\\n```\\n\\n2. Then, install it\\n\\n```console\\n$ python3 -m pip install -e ODAMNet/\\n```\\n\\n*If it's not working, try to update pip using pip install pip --upgrade*\\n\\n## Usage\\n\\nThree different approaches are available: \\n\\n- Overlap analysis\\n- Active Modules Identification (AMI, using [DOMINO][DOMINO])\\n- Random Walk with Restart (RWR, using [multiXrank][multiXrank])\\n\\n```console\\n$ odamnet [overlap|domino|multixrank|networkCreation|networkDownloading] [ARGS]\\n```\\n\\n## Examples\\n\\nThree approaches are implemented to study relationships between genes targeted by chemicals (retrieved automatically \\nfrom the [Comparative Toxicogenomics Database][CTD] (CTD)) and rare diseases (retrieved automatically from \\n[WikiPathways][WikiPathways]).\\n\\n### Overlap analysis\\n\\nThe first approach computes the overlap between chemical target genes and rare disease pathways. It is looking for \\ndirect associations, i.e. chemical target genes that are part of rare disease pathways.\\n\\nGive your chemicals list into `--chemicalsFile` input. \\n\\n```console\\n$ odamnet overlap --chemicalsFile FILENAME\\n```\\n\\n### Active Module Identification (AMI)\\n\\nThe Active Module Identification is performed using DOMINO tool. \\n\\nDOMINO defines target genes as *active genes* to search for active modules using a biological network \\n(e.g. protein-protein interaction network, PPI). Then, an overlap analysis is performed between identified active \\nmodules and rare disease pathways. \\n\\nGive your chemicals list and your biological network into `--chemicalsFile` and `--networkFile` respectively. \\n\\n```console\\n$ odamnet domino --chemicalsFile FILENAME --networkFile FILENAME\\n```\\n\\n### Random Walk with Restart (RWR)\\n\\nThe Random Walk with Restart is performed using multiXrank Python package. This approach mesures the proximity of every node \\n(e.g. genes and diseases) to the target genes within a multilayer network. The multilayer network is composed of genes networks \\nand rare disease pathway network. Diseases and genes are linked using a bipartite.  \\n\\nGive your chemicals list into `--chemicalsFile` input. \\n\\nMultiXrank needs a configuration file (`--configPath`), networks directory (`--networksPath`),\\nthe target genes file (`--seedsFile`) and a name to write the result into network file (`--sifFileName`). \\n\\n```console\\n$ odamnet multixrank --chemicalsFile FILENAME --configPath PATH --networksPath PATH --seedsFile FILENAME --sifFileName FILENAME\\n```\\n\\n*You can have more details about the configuration file in the [documentation page][doc].*\\n\\n### Other functions\\n\\n#### Network and bipartite creation\\n\\nFor the RWR, you should need to create a rare disease pathways network to integrate disease information into the multilayer.\\nODAMNet creates a disconnected network (no connection between disease nodes) and its corresponding bipartite that connects \\ndiseases with genes that are involved in. \\n\\nGive a path to save generated disease network and disease-gene bipartite using `--networksPath` and `--bipartitePath`\\nrespectively.\\n\\n```console\\n$ odamnet networkCreation --networksPath PATH --bipartitePath PATH\\n```\\n*Rare disease pathways are retrieved automatically from WikiPathways.*\\n\\n#### Network downloading\\n\\nODAMNet allows you to download automatically biological networks from [NDEx][NDEx] using the network ID (`--netUUID`). \\nYou can choose the network name file with `--networkFile`.\\n\\n```console\\n$ odamnet networkDownloading --netUUID TEXT --networkFile FILENAME\\n```\\n\\n[ODAMNet documentation]: https://odamnet.readthedocs.io/\\n[pypi]: https://pypi.org/project/ODAMNet/\\n[bioconda]: https://bioconda.github.io/index.html\\n[EJPRD]: https://www.ejprarediseases.org/\\n[DOMINO]: http://domino.cs.tau.ac.il\\n[multiXrank]: https://multixrank-doc.readthedocs.io/en/latest/index.html\\n[WikiPathways]: https://www.wikipathways.org/\\n[CTD]: https://ctdbase.org/\\n[NDEx]: https://www.ndexbio.org/\\n[doc]: https://odamnet.readthedocs.io/en/latest/pages/formats/Input.html#configuration-file\\n\", 'description_content_type': 'text/markdown', 'download_url': '', 'downloads': {'last_day': -1, 'last_month': -1, 'last_week': -1}, 'home_page': '', 'keywords': 'rare diseases,overlap,WikiPathways,chemicals,pathways overlap,pathways diffusion,active module identification,random walk with restart,RWR,AMI,EJP-RD', 'license': 'MIT License  Copyright (c) 2022 Morgane T\u00e9r\u00e9zol  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ', 'maintainer': '', 'maintainer_email': '', 'name': 'ODAMNet', 'package_url': 'https://pypi.org/project/ODAMNet/', 'project_url': 'https://pypi.org/project/ODAMNet/', 'project_urls': {'Documentation': 'https://odamnet.readthedocs.io/en/latest/', 'Source': 'https://github.com/MOohTus/ODAMNet'}, 'release_url': 'https://pypi.org/project/ODAMNet/1.1.0/', 'requires_dist': ['requests (>=2.27.1)', 'SPARQLWrapper (>=1.8.5)', 'pandas (>=1.3.5)', 'scipy (>=1.7.3)', 'statsmodels (>=0.13.2)', 'alive-progress (>=2.4.1)', 'click-option-group (>=0.5.3)', 'click (>=8.0.1)', 'sphinx-tabs (>=3.4.1)', 'multixrank (==0.1)', 'ndex2 (==3.5.0)', 'networkx (==2.5)'], 'requires_python': '>=3.9', 'summary': 'Study molecular relationships between chemicals and rare diseases', 'version': '1.1.0', 'yanked': False, 'releases': ['1.0.0', '1.0.1', '1.0.2', '1.1.0']}```"}, {"role": "user", "content": "Content from https://pypi.org/project/ODAMNet/1.1.0/:\n```\nProject metadata:\n{'author': 'Morgane T\u00e9r\u00e9zol', 'author_email': '', 'classifiers': ['Environment :: Console', 'License :: OSI Approved :: MIT License', 'Operating System :: POSIX :: Linux', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.9'], 'description': \"# README\\n\\nODAMNet is a Python package to study molecular relationship between environmental factors (called chemicals here) and \\nrare diseases. \\n\\nThe [ODAMNet documentation][ODAMNet documentation] is available in ReadTheDocs.\\n\\nThis tool was created within the framework of the [EJRP-RD project][EJPRD].\\n\\n## Installation \\n\\n### From PyPI\\n\\nODAMNet is available in [Python package][pypi]. You can easily install it using `pip`.\\n\\n```console\\n$ python3 -m pip install odamnet\\n```\\n\\n### From Conda\\n\\nODAMNet is also available in [bioconda][bioconda] using ``conda``.\\n\\n```console\\n$ conda install odamnet\\n```\\n\\n### From Github\\n\\n1. Clone the repository from GitHub\\n\\n```console\\n$ git clone https://github.com/MOohTus/ODAMNet.git\\n```\\n\\n2. Then, install it\\n\\n```console\\n$ python3 -m pip install -e ODAMNet/\\n```\\n\\n*If it's not working, try to update pip using pip install pip --upgrade*\\n\\n## Usage\\n\\nThree different approaches are available: \\n\\n- Overlap analysis\\n- Active Modules Identification (AMI, using [DOMINO][DOMINO])\\n- Random Walk with Restart (RWR, using [multiXrank][multiXrank])\\n\\n```console\\n$ odamnet [overlap|domino|multixrank|networkCreation|networkDownloading] [ARGS]\\n```\\n\\n## Examples\\n\\nThree approaches are implemented to study relationships between genes targeted by chemicals (retrieved automatically \\nfrom the [Comparative Toxicogenomics Database][CTD] (CTD)) and rare diseases (retrieved automatically from \\n[WikiPathways][WikiPathways]).\\n\\n### Overlap analysis\\n\\nThe first approach computes the overlap between chemical target genes and rare disease pathways. It is looking for \\ndirect associations, i.e. chemical target genes that are part of rare disease pathways.\\n\\nGive your chemicals list into `--chemicalsFile` input. \\n\\n```console\\n$ odamnet overlap --chemicalsFile FILENAME\\n```\\n\\n### Active Module Identification (AMI)\\n\\nThe Active Module Identification is performed using DOMINO tool. \\n\\nDOMINO defines target genes as *active genes* to search for active modules using a biological network \\n(e.g. protein-protein interaction network, PPI). Then, an overlap analysis is performed between identified active \\nmodules and rare disease pathways. \\n\\nGive your chemicals list and your biological network into `--chemicalsFile` and `--networkFile` respectively. \\n\\n```console\\n$ odamnet domino --chemicalsFile FILENAME --networkFile FILENAME\\n```\\n\\n### Random Walk with Restart (RWR)\\n\\nThe Random Walk with Restart is performed using multiXrank Python package. This approach mesures the proximity of every node \\n(e.g. genes and diseases) to the target genes within a multilayer network. The multilayer network is composed of genes networks \\nand rare disease pathway network. Diseases and genes are linked using a bipartite.  \\n\\nGive your chemicals list into `--chemicalsFile` input. \\n\\nMultiXrank needs a configuration file (`--configPath`), networks directory (`--networksPath`),\\nthe target genes file (`--seedsFile`) and a name to write the result into network file (`--sifFileName`). \\n\\n```console\\n$ odamnet multixrank --chemicalsFile FILENAME --configPath PATH --networksPath PATH --seedsFile FILENAME --sifFileName FILENAME\\n```\\n\\n*You can have more details about the configuration file in the [documentation page][doc].*\\n\\n### Other functions\\n\\n#### Network and bipartite creation\\n\\nFor the RWR, you should need to create a rare disease pathways network to integrate disease information into the multilayer.\\nODAMNet creates a disconnected network (no connection between disease nodes) and its corresponding bipartite that connects \\ndiseases with genes that are involved in. \\n\\nGive a path to save generated disease network and disease-gene bipartite using `--networksPath` and `--bipartitePath`\\nrespectively.\\n\\n```console\\n$ odamnet networkCreation --networksPath PATH --bipartitePath PATH\\n```\\n*Rare disease pathways are retrieved automatically from WikiPathways.*\\n\\n#### Network downloading\\n\\nODAMNet allows you to download automatically biological networks from [NDEx][NDEx] using the network ID (`--netUUID`). \\nYou can choose the network name file with `--networkFile`.\\n\\n```console\\n$ odamnet networkDownloading --netUUID TEXT --networkFile FILENAME\\n```\\n\\n[ODAMNet documentation]: https://odamnet.readthedocs.io/\\n[pypi]: https://pypi.org/project/ODAMNet/\\n[bioconda]: https://bioconda.github.io/index.html\\n[EJPRD]: https://www.ejprarediseases.org/\\n[DOMINO]: http://domino.cs.tau.ac.il\\n[multiXrank]: https://multixrank-doc.readthedocs.io/en/latest/index.html\\n[WikiPathways]: https://www.wikipathways.org/\\n[CTD]: https://ctdbase.org/\\n[NDEx]: https://www.ndexbio.org/\\n[doc]: https://odamnet.readthedocs.io/en/latest/pages/formats/Input.html#configuration-file\\n\", 'description_content_type': 'text/markdown', 'download_url': '', 'downloads': {'last_day': -1, 'last_month': -1, 'last_week': -1}, 'home_page': '', 'keywords': 'rare diseases,overlap,WikiPathways,chemicals,pathways overlap,pathways diffusion,active module identification,random walk with restart,RWR,AMI,EJP-RD', 'license': 'MIT License  Copyright (c) 2022 Morgane T\u00e9r\u00e9zol  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ', 'maintainer': '', 'maintainer_email': '', 'name': 'ODAMNet', 'package_url': 'https://pypi.org/project/ODAMNet/', 'project_url': 'https://pypi.org/project/ODAMNet/', 'project_urls': {'Documentation': 'https://odamnet.readthedocs.io/en/latest/', 'Source': 'https://github.com/MOohTus/ODAMNet'}, 'release_url': 'https://pypi.org/project/ODAMNet/1.1.0/', 'requires_dist': ['requests (>=2.27.1)', 'SPARQLWrapper (>=1.8.5)', 'pandas (>=1.3.5)', 'scipy (>=1.7.3)', 'statsmodels (>=0.13.2)', 'alive-progress (>=2.4.1)', 'click-option-group (>=0.5.3)', 'click (>=8.0.1)', 'sphinx-tabs (>=3.4.1)', 'multixrank (==0.1)', 'ndex2 (==3.5.0)', 'networkx (==2.5)'], 'requires_python': '>=3.9', 'summary': 'Study molecular relationships between chemicals and rare diseases', 'version': '1.1.0', 'yanked': False, 'releases': ['1.0.0', '1.0.1', '1.0.2', '1.1.0']}```"}, {"role": "user", "content": "Content from https://github.com/MOohTus/ODAMNet:\n```\nREADME content:\n[\"# README ODAMNet is a Python package to study molecular relationship between environmental factors (called chemicals here) and rare diseases. The [ODAMNet documentation][ODAMNet documentation] is available in ReadTheDocs. This tool was created within the framework of the [EJRP-RD project][EJPRD]. If you use ODAMNet, please cite our publication: >T\u00e9r\u00e9zol M, Baudot A, Ozisik O. ODAMNet: A Python package to identify molecular relationships between chemicals and rare diseases using overlap, active module and random walk approaches. SoftwareX. 2024;26:101701. doi: [10.1016/j.softx.2024.101701][doi]. ## Installation ### From PyPI ODAMNet is available in [Python package][pypi]. You can easily install it using `pip`. ```console $ python3 -m pip install odamnet ``` ### From Conda ODAMNet is also available in [bioconda][bioconda] using ``conda``. ```console $ conda install odamnet ``` ### From Github 1. Clone the repository from GitHub ```console $ git clone https://github.com/MOohTus/ODAMNet.git ``` 2. Then, install it ```console $ python3 -m pip install -e ODAMNet/ ``` *If it's not working, try to update pip using pip install pip --upgrade* ## Usage Three different approaches are available: - Overlap analysis - Active Modules Identification (AMI, using [DOMINO][DOMINO]) - Random Walk with Restart (RWR, using [multiXrank][multiXrank]) ```console $ odamnet [overlap|domino|multixrank|networkCreation|networkDownloading] [ARGS] ``` ## Examples Three approaches are implemented to study relationships between genes targeted by chemicals (retrieved automatically from the [Comparative Toxicogenomics Database][CTD] (CTD)) and rare diseases (retrieved automatically from [WikiPathways][WikiPathways]). ### Overlap analysis The first approach computes the overlap between chemical target genes and rare disease pathways. It is looking for direct associations, i.e. chemical target genes that are part of rare disease pathways. Give your chemicals list into `--chemicalsFile` input. ```console $ odamnet overlap --chemicalsFile FILENAME ``` ### Active Module Identification (AMI) The Active Module Identification is performed using DOMINO tool. DOMINO defines target genes as *active genes* to search for active modules using a biological network (e.g. protein-protein interaction network, PPI). Then, an overlap analysis is performed between identified active modules and rare disease pathways. Give your chemicals list and your biological network into `--chemicalsFile` and `--networkFile` respectively. ```console $ odamnet domino --chemicalsFile FILENAME --networkFile FILENAME ``` ### Random Walk with Restart (RWR) The Random Walk with Restart is performed using multiXrank Python package. This approach mesures the proximity of every node (e.g. genes and diseases) to the target genes within a multilayer network. The multilayer network is composed of genes networks and rare disease pathway network. Diseases and genes are linked using a bipartite. Give your chemicals list into `--chemicalsFile` input. MultiXrank needs a configuration file (`--configPath`), networks directory (`--networksPath`), the target genes file (`--seedsFile`) and a name to write the result into network file (`--sifFileName`). ```console $ odamnet multixrank --chemicalsFile FILENAME --configPath PATH --networksPath PATH --seedsFile FILENAME --sifFileName FILENAME ``` *You can have more details about the configuration file in the [documentation page][doc].* ### Other functions #### Network and bipartite creation For the RWR, you should need to create a rare disease pathways network to integrate disease information into the multilayer. ODAMNet creates a disconnected network (no connection between disease nodes) and its corresponding bipartite that connects diseases with genes that are involved in. Give a path to save generated disease network and disease-gene bipartite using `--networksPath` and `--bipartitePath` respectively. ```console $ odamnet networkCreation --networksPath PATH --bipartitePath PATH ``` *Rare disease pathways are retrieved automatically from WikiPathways.* #### Network downloading ODAMNet allows you to download automatically biological networks from [NDEx][NDEx] using the network ID (`--netUUID`). You can choose the network name file with `--networkFile`. ```console $ odamnet networkDownloading --netUUID TEXT --networkFile FILENAME ``` [ODAMNet documentation]: https://odamnet.readthedocs.io/ [pypi]: https://pypi.org/project/ODAMNet/ [bioconda]: https://anaconda.org/bioconda/odamnet [EJPRD]: https://www.ejprarediseases.org/ [DOMINO]: http://domino.cs.tau.ac.il [multiXrank]: https://multixrank-doc.readthedocs.io/en/latest/index.html [WikiPathways]: https://www.wikipathways.org/ [CTD]: https://ctdbase.org/ [NDEx]: https://www.ndexbio.org/ [doc]: https://odamnet.readthedocs.io/en/latest/pages/formats/Input.html#configuration-file [doi]: https://www.softxjournal.com/article/S2352-7110(24)00072-4/fulltext\"]```"}, {"role": "user", "content": "Content from https://github.com/MOohTus/ODAMNet:\n```\nRepository metadata:\n{'name': 'ODAMNet', 'label': ['ODAMNet'], 'description': [], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.1.0'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/MOohTus/ODAMNet', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Morgane T\u00e9r\u00e9zol', 'type': 'person', 'email': '10067410+MOohTus@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'MOohTus', 'type': 'person', 'email': 'caco630@hotmail.fr', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/MOohTus/ODAMNet/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'requirements', 'url': 'https://github.com/MOohTus/ODAMNet/blob/main/requirements.txt'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"metacoag/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/metacoag/cmd/1.1.4\",\n    \"name\": \"metacoag\",\n    \"description\": [\n      \"MetaCoAG is a metagenomic contig binning tool that makes use of the connectivity information found in assembly graphs.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/metagentools/MetaCoAG\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/metagentools/MetaCoAG\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dc02ac7ee6cb5789128f\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/metacoag/cmd/None\",\n    \"name\": \"metacoag\",\n    \"description\": [\n      \"MetaCoAG is a metagenomic contig binning tool that makes use of the connectivity information found in assembly graphs, apart from the composition and coverage information. MetaCoAG makes use of single-copy marker genes along with a graph matching technique and a label propagation technique to bin contigs. MetaCoAG is tested on contigs obtained from next-generation sequencing (NGS) data. Currently MetaCoAG supports contigs assembled using metaSPAdes and MEGAHIT.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/Vini2/MetaCoAG\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Vijini Mallawaarachchi\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Yu Lin\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8346510e2f3aa0b3bbd84\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/Vini2/MetaCoAG:\n```\nREADME content:\n['<p align=\"center\"> <img src=\"https://raw.githubusercontent.com/metagentools/MetaCoAG/develop/docs/images/MetaCoAG_Logo_light.png#gh-light-mode-only\" width=\"500\" title=\"MetaCoAG logo\" alt=\"MetaCoAG logo\"> <img src=\"https://raw.githubusercontent.com/metagentools/MetaCoAG/develop/docs/images/MetaCoAG_Logo_dark.png#gh-dark-mode-only\" width=\"500\" title=\"MetaCoAG logo\" alt=\"MetaCoAG logo\"> </p> # MetaCoAG: Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs [![DOI](https://img.shields.io/badge/DOI-10.1007/978--3--031--04749--7__5-informational)](https://doi.org/10.1007/978-3-031-04749-7_5) [![DOI](https://img.shields.io/badge/DOI-10.1089/cmb.2022.0262-green)](https://doi.org/10.1089/cmb.2022.0262) [![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/metacoag/README.html) [![Conda](https://img.shields.io/conda/v/bioconda/metacoag)](https://anaconda.org/bioconda/metacoag) [![Conda](https://img.shields.io/conda/dn/bioconda/metacoag)](https://anaconda.org/bioconda/metacoag) [![PyPI version](https://badge.fury.io/py/metacoag.svg)](https://badge.fury.io/py/metacoag) [![Downloads](https://static.pepy.tech/badge/metacoag)](https://pepy.tech/project/metacoag) [![CI](https://github.com/metagentools/MetaCoAG/actions/workflows/testing_python.yml/badge.svg)](https://github.com/metagentools/MetaCoAG/actions/workflows/testing_python.yml) [![Coverage Status](https://coveralls.io/repos/github/metagentools/MetaCoAG/badge.svg?branch=develop)](https://coveralls.io/github/metagentools/MetaCoAG?branch=develop) ![GitHub](https://img.shields.io/github/license/Vini2/MetaCoAG) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![CodeQL](https://github.com/metagentools/MetaCoAG/actions/workflows/codeql.yml/badge.svg)](https://github.com/metagentools/MetaCoAG/actions/workflows/codeql.yml) [![Documentation Status](https://readthedocs.org/projects/metacoag/badge/?version=latest)](https://metacoag.readthedocs.io/en/latest/?badge=latest) MetaCoAG is a metagenomic contig binning tool that makes use of the connectivity information found in assembly graphs, apart from the composition and coverage information. MetaCoAG makes use of single-copy marker genes along with a graph matching technique and a label propagation technique to bin contigs. MetaCoAG is tested on contigs obtained from next-generation sequencing (NGS) data. Currently, MetaCoAG supports contigs assembled using metaSPAdes and MEGAHIT, and recently we have added support for Flye assemblies (has not been tested extensively). For detailed instructions on installation, usage and visualisation, please refer to the [documentation hosted at Read the Docs](https://metacoag.readthedocs.io/). **NEW:** MetaCoAG is now available on bioconda at [https://anaconda.org/bioconda/metacoag](https://anaconda.org/bioconda/metacoag) and on PyPI at [https://pypi.org/project/metacoag/](https://pypi.org/project/metacoag/). ## Dependencies MetaCoAG installation requires Python 3.7 or above. You will need the following python dependencies to run MetaCoAG and related support scripts. The latest tested versions of the dependencies are listed as well. * [python](https://www.python.org/) - version 3.11.0 * [python-igraph](https://igraph.org/python/) - version 0.10.4 * [biopython](https://biopython.org/) - version 1.80 * [networkx](https://networkx.github.io/) - version 3.0 * [scipy](https://www.scipy.org/) - version 1.10.0 * [numpy](https://numpy.org/) - version 1.24.2 * [tqdm](https://github.com/tqdm/tqdm) - version 4.64.1 * [click](https://click.palletsprojects.com/) - version 8.1.3 MetaCoAG uses the following tools to scan for single-copy marker genes. These tools have been tested on the following versions. * [FragGeneScan](https://sourceforge.net/projects/fraggenescan/) - version 1.31 * [HMMER](http://hmmer.org/) - version 3.3.2 ## Installing MetaCoAG using conda We recommend that you use [`conda`](https://docs.conda.io/en/latest/) to run MetaCoAG. You can install MetaCoAG from [bioconda](https://anaconda.org/bioconda/metacoag). ```shell # add channels conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge # create conda environment and install metacoag conda create -n metacoag -c bioconda metacoag # activate metacoag environment conda activate metacoag # check metacoag installation metacoag -h ``` ## Setting up MetaCoAG for development ### Downloading MetaCoAG You can clone the MetaCoAG repository to your machine. ``` git clone https://github.com/Vini2/MetaCoAG.git ``` Now go into the MetaCoAG folder using the command ``` cd MetaCoAG/ ``` ### Using `conda` Once you have installed `conda`, make sure you are in the MetaCoAG folder. Now run the following commands to create a `conda` environment and activate it to run MetaCoAG. ``` conda env create -f environment.yml conda activate metacoag ``` ### Using `pip` You can run the following command to install MetaCoAG using `pip`. Make sure you are in the MetaCoAG folder. ``` pip install . ``` ### Using `flit` You can run the following command to install MetaCoAG using `flit`. Make sure you are in the MetaCoAG folder. ``` flit install ``` **Note:** If you use pip or flit to setup MetaCoAG for development, you will have to install [FragGeneScan](https://sourceforge.net/projects/fraggenescan/) and [HMMER](http://hmmer.org/) manually and add them to your system path. ### Test the setup After setting up, run the following command to ensure that metacoag is working. ``` metacoag --help ``` ## Example Usage ``` metacoag --assembler spades --graph /path/to/graph_file.gfa --contigs /path/to/contigs.fasta --paths /path/to/paths_file.paths --abundance /path/to/abundance.tsv --output /path/to/output_folder ``` ``` metacoag --assembler megahit --graph /path/to/graph_file.gfa --contigs /path/to/contigs.fasta --abundance /path/to/abundance.tsv --output /path/to/output_folder ``` ``` metacoag --assembler flye --graph /path/to/assembly_graph.gfa --contigs /path/to/assembly.fasta --paths /path/to/assembly_info.txt --abundance /path/to/abundance.tsv --output /path/to/output_folder ``` ## Citation MetaCoAG has been accepted at [RECOMB 2022](https://recomb2022.net/accepted-papers/) and is published in Lecture Notes in Computer Science at [https://doi.org/10.1007/978-3-031-04749-7_5](https://doi.org/10.1007/978-3-031-04749-7_5). > Mallawaarachchi, V., Lin, Y. (2022). MetaCoAG: Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs. In: Pe\\'er, I. (eds) Research in Computational Molecular Biology. RECOMB 2022. Lecture Notes in Computer Science(), vol 13278. Springer, Cham. DOI: [https://doi.org/10.1007/978-3-031-04749-7_5](https://doi.org/10.1007/978-3-031-04749-7_5) A journal extension of MetaCoAG is published in the Journal of Computational Biology at [https://www.liebertpub.com/doi/10.1089/cmb.2022.0262](https://www.liebertpub.com/doi/10.1089/cmb.2022.0262). > Vijini Mallawaarachchi and Yu Lin. Accurate Binning of Metagenomic Contigs Using Composition, Coverage, and Assembly Graphs. Journal of Computational Biology 2022 29:12, 1357-1376. DOI: [https://doi.org/10.1089/cmb.2022.0262](https://doi.org/10.1089/cmb.2022.0262) If you use MetaCoAG in your work, please cite the following publications. ```bibtex @InProceedings{10.1007/978-3-031-04749-7_5, author=\"Mallawaarachchi, Vijini and Lin, Yu\", editor=\"Pe\\'er, Itsik\", title=\"MetaCoAG: Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs\", booktitle=\"Research in Computational Molecular Biology\", year=\"2022\", publisher=\"Springer International Publishing\", address=\"Cham\", pages=\"70--85\", abstract=\"Metagenomics has allowed us to obtain various genetic material from different species and gain valuable insights into microbial communities. Binning plays an important role in the early stages of metagenomic analysis pipelines. A typical pipeline in metagenomics binning is to assemble short reads into longer contigs and then bin into groups representing different species in the metagenomic sample. While existing binning tools bin metagenomic contigs, they do not make use of the assembly graphs that produce such assemblies. Here we propose MetaCoAG, a tool that utilizes assembly graphs with the composition and coverage information to bin metagenomic contigs. MetaCoAG uses single-copy marker genes to estimate the number of initial bins, assigns contigs into bins iteratively and adjusts the number of bins dynamically throughout the binning process. Experimental results on simulated and real datasets demonstrate that MetaCoAG significantly outperforms state-of-the-art binning tools, producing similar or more high-quality bins than the second-best tool. To the best of our knowledge, MetaCoAG is the first stand-alone contig-binning tool to make direct use of the assembly graph information.\", isbn=\"978-3-031-04749-7\" } @Article{doi:10.1089/cmb.2022.0262, author = {Mallawaarachchi, Vijini and Lin, Yu}, title = {Accurate Binning of Metagenomic Contigs Using Composition, Coverage, and Assembly Graphs}, journal = {Journal of Computational Biology}, volume = {29}, number = {12}, pages = {1357--1376}, year = {2022}, doi = {10.1089/cmb.2022.0262}, note ={PMID: 36367700}, URL = {https://doi.org/10.1089/cmb.2022.0262}, eprint = {https://doi.org/10.1089/cmb.2022.0262}, abstract = { Metagenomics enables the recovery of various genetic materials from different species, thus providing valuable insights into microbial communities. Metagenomic binning group sequences belong to different organisms, which is an important step in the early stages of metagenomic analysis pipelines. The classic pipeline followed in metagenomic binning is to assemble short reads into longer contigs and then bin these resulting contigs into groups representing different taxonomic groups in the metagenomic sample. Most of the currently available binning tools are designed to bin metagenomic contigs, but they do not make use of the assembly graphs that produce such assemblies. In this study, we propose MetaCoAG, a metagenomic binning tool that uses assembly graphs with the composition and coverage information of contigs. MetaCoAG estimates the number of initial bins using single-copy marker genes, assigns contigs into bins iteratively, and adjusts the number of bins dynamically throughout the binning process. We show that MetaCoAG significantly outperforms state-of-the-art binning tools by producing similar or more high-quality bins than the second-best binning tool on both simulated and real datasets. To the best of our knowledge, MetaCoAG is the first stand-alone contig-binning tool that directly makes use of the assembly graph information along with other features of the contigs. } } ``` ## Funding MetaCoAG is funded by an [Essential Open Source Software for Science Grant](https://chanzuckerberg.com/eoss/proposals/cogent3-python-apis-for-iq-tree-and-graphbin-via-a-plug-in-architecture/) from the Chan Zuckerberg Initiative. <p align=\"left\"> <img src=\"https://chanzuckerberg.com/wp-content/themes/czi/img/logo.svg\" width=\"300\"> </p>']```"}, {"role": "user", "content": "Content from https://github.com/Vini2/MetaCoAG:\n```\nRepository metadata:\n{'name': 'MetaCoAG', 'label': ['MetaCoAG'], 'description': [{'term': '\ud83d\udea6\ud83e\uddec Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://metacoag.readthedocs.io/en/stable/', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.2.2', 'v1.2.1', 'v1.2.0', 'v1.1.4', 'v1.1.3', 'v1.1.2', 'v1.1.1', 'v1.1', 'v1.0', 'v0.2', 'v0.1'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/metagentools/MetaCoAG', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/binning', 'term': 'binning', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/metagenomics', 'term': 'metagenomics', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/contigs', 'term': 'contigs', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/assembly-graphs', 'term': 'assembly-graphs', 'vocabulary': ''}, 'id': 3}], 'operations': [], 'authors': [{'term': {'name': 'Vijini Mallawaarachchi', 'type': 'person', 'email': 'viji.mallawaarachchi@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Gavin Huttley', 'type': 'person', 'email': 'Gavin.Huttley@anu.edu.au', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'code_of_conduct', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/CODE_OF_CONDUCT.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'requirements', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/requirements.txt'}, 'id': 2}, {'term': {'type': 'citation', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/citation.md'}, 'id': 3}, {'term': {'type': 'docs', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/index.md'}, 'id': 4}, {'term': {'type': 'installation', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/install.md'}, 'id': 5}, {'term': {'type': 'docs', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/preprocess.md'}, 'id': 6}, {'term': {'type': 'requirements', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/requirements.txt'}, 'id': 7}, {'term': {'type': 'usage', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/usage.md'}, 'id': 8}, {'term': {'type': 'docs', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/workflow.md'}, 'id': 9}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [{'term': {'title': 'MetaCoAG: Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs', 'year': '2022', 'doi': '', 'url': ''}, 'id': 0}, {'term': {'title': 'Accurate Binning of Metagenomic Contigs Using Composition, Coverage, and Assembly Graphs', 'year': '2022', 'doi': '10.1089/cmb.2022.0262', 'url': 'https://doi.org/10.1089/cmb.2022.0262'}, 'id': 1}], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/metagentools/MetaCoAG:\n```\nREADME content:\n['<p align=\"center\"> <img src=\"https://raw.githubusercontent.com/metagentools/MetaCoAG/develop/docs/images/MetaCoAG_Logo_light.png#gh-light-mode-only\" width=\"500\" title=\"MetaCoAG logo\" alt=\"MetaCoAG logo\"> <img src=\"https://raw.githubusercontent.com/metagentools/MetaCoAG/develop/docs/images/MetaCoAG_Logo_dark.png#gh-dark-mode-only\" width=\"500\" title=\"MetaCoAG logo\" alt=\"MetaCoAG logo\"> </p> # MetaCoAG: Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs [![DOI](https://img.shields.io/badge/DOI-10.1007/978--3--031--04749--7__5-informational)](https://doi.org/10.1007/978-3-031-04749-7_5) [![DOI](https://img.shields.io/badge/DOI-10.1089/cmb.2022.0262-green)](https://doi.org/10.1089/cmb.2022.0262) [![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/metacoag/README.html) [![Conda](https://img.shields.io/conda/v/bioconda/metacoag)](https://anaconda.org/bioconda/metacoag) [![Conda](https://img.shields.io/conda/dn/bioconda/metacoag)](https://anaconda.org/bioconda/metacoag) [![PyPI version](https://badge.fury.io/py/metacoag.svg)](https://badge.fury.io/py/metacoag) [![Downloads](https://static.pepy.tech/badge/metacoag)](https://pepy.tech/project/metacoag) [![CI](https://github.com/metagentools/MetaCoAG/actions/workflows/testing_python.yml/badge.svg)](https://github.com/metagentools/MetaCoAG/actions/workflows/testing_python.yml) [![Coverage Status](https://coveralls.io/repos/github/metagentools/MetaCoAG/badge.svg?branch=develop)](https://coveralls.io/github/metagentools/MetaCoAG?branch=develop) ![GitHub](https://img.shields.io/github/license/Vini2/MetaCoAG) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![CodeQL](https://github.com/metagentools/MetaCoAG/actions/workflows/codeql.yml/badge.svg)](https://github.com/metagentools/MetaCoAG/actions/workflows/codeql.yml) [![Documentation Status](https://readthedocs.org/projects/metacoag/badge/?version=latest)](https://metacoag.readthedocs.io/en/latest/?badge=latest) MetaCoAG is a metagenomic contig binning tool that makes use of the connectivity information found in assembly graphs, apart from the composition and coverage information. MetaCoAG makes use of single-copy marker genes along with a graph matching technique and a label propagation technique to bin contigs. MetaCoAG is tested on contigs obtained from next-generation sequencing (NGS) data. Currently, MetaCoAG supports contigs assembled using metaSPAdes and MEGAHIT, and recently we have added support for Flye assemblies (has not been tested extensively). For detailed instructions on installation, usage and visualisation, please refer to the [documentation hosted at Read the Docs](https://metacoag.readthedocs.io/). **NEW:** MetaCoAG is now available on bioconda at [https://anaconda.org/bioconda/metacoag](https://anaconda.org/bioconda/metacoag) and on PyPI at [https://pypi.org/project/metacoag/](https://pypi.org/project/metacoag/). ## Dependencies MetaCoAG installation requires Python 3.7 or above. You will need the following python dependencies to run MetaCoAG and related support scripts. The latest tested versions of the dependencies are listed as well. * [python](https://www.python.org/) - version 3.11.0 * [python-igraph](https://igraph.org/python/) - version 0.10.4 * [biopython](https://biopython.org/) - version 1.80 * [networkx](https://networkx.github.io/) - version 3.0 * [scipy](https://www.scipy.org/) - version 1.10.0 * [numpy](https://numpy.org/) - version 1.24.2 * [tqdm](https://github.com/tqdm/tqdm) - version 4.64.1 * [click](https://click.palletsprojects.com/) - version 8.1.3 MetaCoAG uses the following tools to scan for single-copy marker genes. These tools have been tested on the following versions. * [FragGeneScan](https://sourceforge.net/projects/fraggenescan/) - version 1.31 * [HMMER](http://hmmer.org/) - version 3.3.2 ## Installing MetaCoAG using conda We recommend that you use [`conda`](https://docs.conda.io/en/latest/) to run MetaCoAG. You can install MetaCoAG from [bioconda](https://anaconda.org/bioconda/metacoag). ```shell # add channels conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge # create conda environment and install metacoag conda create -n metacoag -c bioconda metacoag # activate metacoag environment conda activate metacoag # check metacoag installation metacoag -h ``` ## Setting up MetaCoAG for development ### Downloading MetaCoAG You can clone the MetaCoAG repository to your machine. ``` git clone https://github.com/Vini2/MetaCoAG.git ``` Now go into the MetaCoAG folder using the command ``` cd MetaCoAG/ ``` ### Using `conda` Once you have installed `conda`, make sure you are in the MetaCoAG folder. Now run the following commands to create a `conda` environment and activate it to run MetaCoAG. ``` conda env create -f environment.yml conda activate metacoag ``` ### Using `pip` You can run the following command to install MetaCoAG using `pip`. Make sure you are in the MetaCoAG folder. ``` pip install . ``` ### Using `flit` You can run the following command to install MetaCoAG using `flit`. Make sure you are in the MetaCoAG folder. ``` flit install ``` **Note:** If you use pip or flit to setup MetaCoAG for development, you will have to install [FragGeneScan](https://sourceforge.net/projects/fraggenescan/) and [HMMER](http://hmmer.org/) manually and add them to your system path. ### Test the setup After setting up, run the following command to ensure that metacoag is working. ``` metacoag --help ``` ## Example Usage ``` metacoag --assembler spades --graph /path/to/graph_file.gfa --contigs /path/to/contigs.fasta --paths /path/to/paths_file.paths --abundance /path/to/abundance.tsv --output /path/to/output_folder ``` ``` metacoag --assembler megahit --graph /path/to/graph_file.gfa --contigs /path/to/contigs.fasta --abundance /path/to/abundance.tsv --output /path/to/output_folder ``` ``` metacoag --assembler flye --graph /path/to/assembly_graph.gfa --contigs /path/to/assembly.fasta --paths /path/to/assembly_info.txt --abundance /path/to/abundance.tsv --output /path/to/output_folder ``` ## Citation MetaCoAG has been accepted at [RECOMB 2022](https://recomb2022.net/accepted-papers/) and is published in Lecture Notes in Computer Science at [https://doi.org/10.1007/978-3-031-04749-7_5](https://doi.org/10.1007/978-3-031-04749-7_5). > Mallawaarachchi, V., Lin, Y. (2022). MetaCoAG: Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs. In: Pe\\'er, I. (eds) Research in Computational Molecular Biology. RECOMB 2022. Lecture Notes in Computer Science(), vol 13278. Springer, Cham. DOI: [https://doi.org/10.1007/978-3-031-04749-7_5](https://doi.org/10.1007/978-3-031-04749-7_5) A journal extension of MetaCoAG is published in the Journal of Computational Biology at [https://www.liebertpub.com/doi/10.1089/cmb.2022.0262](https://www.liebertpub.com/doi/10.1089/cmb.2022.0262). > Vijini Mallawaarachchi and Yu Lin. Accurate Binning of Metagenomic Contigs Using Composition, Coverage, and Assembly Graphs. Journal of Computational Biology 2022 29:12, 1357-1376. DOI: [https://doi.org/10.1089/cmb.2022.0262](https://doi.org/10.1089/cmb.2022.0262) If you use MetaCoAG in your work, please cite the following publications. ```bibtex @InProceedings{10.1007/978-3-031-04749-7_5, author=\"Mallawaarachchi, Vijini and Lin, Yu\", editor=\"Pe\\'er, Itsik\", title=\"MetaCoAG: Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs\", booktitle=\"Research in Computational Molecular Biology\", year=\"2022\", publisher=\"Springer International Publishing\", address=\"Cham\", pages=\"70--85\", abstract=\"Metagenomics has allowed us to obtain various genetic material from different species and gain valuable insights into microbial communities. Binning plays an important role in the early stages of metagenomic analysis pipelines. A typical pipeline in metagenomics binning is to assemble short reads into longer contigs and then bin into groups representing different species in the metagenomic sample. While existing binning tools bin metagenomic contigs, they do not make use of the assembly graphs that produce such assemblies. Here we propose MetaCoAG, a tool that utilizes assembly graphs with the composition and coverage information to bin metagenomic contigs. MetaCoAG uses single-copy marker genes to estimate the number of initial bins, assigns contigs into bins iteratively and adjusts the number of bins dynamically throughout the binning process. Experimental results on simulated and real datasets demonstrate that MetaCoAG significantly outperforms state-of-the-art binning tools, producing similar or more high-quality bins than the second-best tool. To the best of our knowledge, MetaCoAG is the first stand-alone contig-binning tool to make direct use of the assembly graph information.\", isbn=\"978-3-031-04749-7\" } @Article{doi:10.1089/cmb.2022.0262, author = {Mallawaarachchi, Vijini and Lin, Yu}, title = {Accurate Binning of Metagenomic Contigs Using Composition, Coverage, and Assembly Graphs}, journal = {Journal of Computational Biology}, volume = {29}, number = {12}, pages = {1357--1376}, year = {2022}, doi = {10.1089/cmb.2022.0262}, note ={PMID: 36367700}, URL = {https://doi.org/10.1089/cmb.2022.0262}, eprint = {https://doi.org/10.1089/cmb.2022.0262}, abstract = { Metagenomics enables the recovery of various genetic materials from different species, thus providing valuable insights into microbial communities. Metagenomic binning group sequences belong to different organisms, which is an important step in the early stages of metagenomic analysis pipelines. The classic pipeline followed in metagenomic binning is to assemble short reads into longer contigs and then bin these resulting contigs into groups representing different taxonomic groups in the metagenomic sample. Most of the currently available binning tools are designed to bin metagenomic contigs, but they do not make use of the assembly graphs that produce such assemblies. In this study, we propose MetaCoAG, a metagenomic binning tool that uses assembly graphs with the composition and coverage information of contigs. MetaCoAG estimates the number of initial bins using single-copy marker genes, assigns contigs into bins iteratively, and adjusts the number of bins dynamically throughout the binning process. We show that MetaCoAG significantly outperforms state-of-the-art binning tools by producing similar or more high-quality bins than the second-best binning tool on both simulated and real datasets. To the best of our knowledge, MetaCoAG is the first stand-alone contig-binning tool that directly makes use of the assembly graph information along with other features of the contigs. } } ``` ## Funding MetaCoAG is funded by an [Essential Open Source Software for Science Grant](https://chanzuckerberg.com/eoss/proposals/cogent3-python-apis-for-iq-tree-and-graphbin-via-a-plug-in-architecture/) from the Chan Zuckerberg Initiative. <p align=\"left\"> <img src=\"https://chanzuckerberg.com/wp-content/themes/czi/img/logo.svg\" width=\"300\"> </p>']```"}, {"role": "user", "content": "Content from https://github.com/metagentools/MetaCoAG:\n```\nRepository metadata:\n{'name': 'MetaCoAG', 'label': ['MetaCoAG'], 'description': [{'term': '\ud83d\udea6\ud83e\uddec Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://metacoag.readthedocs.io/en/stable/', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.2.2', 'v1.2.1', 'v1.2.0', 'v1.1.4', 'v1.1.3', 'v1.1.2', 'v1.1.1', 'v1.1', 'v1.0', 'v0.2', 'v0.1'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/metagentools/MetaCoAG', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/binning', 'term': 'binning', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/metagenomics', 'term': 'metagenomics', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/contigs', 'term': 'contigs', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/assembly-graphs', 'term': 'assembly-graphs', 'vocabulary': ''}, 'id': 3}], 'operations': [], 'authors': [{'term': {'name': 'Vijini Mallawaarachchi', 'type': 'person', 'email': 'viji.mallawaarachchi@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Gavin Huttley', 'type': 'person', 'email': 'Gavin.Huttley@anu.edu.au', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'code_of_conduct', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/CODE_OF_CONDUCT.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'requirements', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/requirements.txt'}, 'id': 2}, {'term': {'type': 'citation', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/citation.md'}, 'id': 3}, {'term': {'type': 'docs', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/index.md'}, 'id': 4}, {'term': {'type': 'installation', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/install.md'}, 'id': 5}, {'term': {'type': 'docs', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/preprocess.md'}, 'id': 6}, {'term': {'type': 'requirements', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/requirements.txt'}, 'id': 7}, {'term': {'type': 'usage', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/usage.md'}, 'id': 8}, {'term': {'type': 'docs', 'url': 'https://github.com/metagentools/MetaCoAG/blob/main/docs/workflow.md'}, 'id': 9}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [{'term': {'title': 'MetaCoAG: Binning Metagenomic Contigs via Composition, Coverage and Assembly Graphs', 'year': '2022', 'doi': '', 'url': ''}, 'id': 0}, {'term': {'title': 'Accurate Binning of Metagenomic Contigs Using Composition, Coverage, and Assembly Graphs', 'year': '2022', 'doi': '10.1089/cmb.2022.0262', 'url': 'https://doi.org/10.1089/cmb.2022.0262'}, 'id': 1}], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"pegs/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/pegs/cmd/0.6.6\",\n    \"name\": \"pegs\",\n    \"description\": [\n      \"Peak-set Enrichment of Gene-Sets (PEGS).\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/fls-bioinformatics-core/pegs\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/fls-bioinformatics-core/pegs\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"BSD-3-Clause\",\n        \"url\": \"https://spdx.org/licenses/BSD-3-Clause.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/pegs/cmd/None\",\n    \"name\": \"pegs\",\n    \"description\": [\n      \"PEGS (Peak-set Enrichment of Gene-Sets) is a Python bioinformatics utility for calculating enrichments of gene cluster enrichments from peak data at different genomic distances.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://pegs.readthedocs.io/en/latest/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Mudassar Iqbal\",\n        \"email\": \"mudassar.iqbal@manchester.ac.uk\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"A. Louise Hunter\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Shen-hsi Yang\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Andrew D. Sharrocks\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Peter Briggs\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8344f10e2f3aa0b3bbd2d\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://pegs.readthedocs.io/en/latest/:\n```\nContent:\n['# PEGS: Peak-set Enrichment of Gene-Sets PEGS ( **P** eak-set **E** nrichment of **G** ene- **S** ets) is a Python bioinformatics utility for calculating enrichments of gene cluster enrichments from peak data at different genomic distances.']```"}, {"role": "user", "content": "Content from https://github.com/fls-bioinformatics-core/pegs:\n```\nREADME content:\n['************************************** PEGS: Peak-set Enrichment of Gene-Sets ************************************** ``PEGS`` (**P**\\\\ eak-set **E**\\\\ nrichment of **G**\\\\ ene-**S**\\\\ ets) is a Python bioinformatics utility for calculating enrichments of gene clusters at different genomic distances from peaks. .. image:: https://readthedocs.org/projects/pegs/badge/?version=latest :target: https://pegs.readthedocs.io/ .. image:: https://badge.fury.io/py/pegs.svg :target: https://pypi.python.org/pypi/pegs/ .. image:: https://github.com/fls-bioinformatics-core/pegs/workflows/Python%20package/badge.svg :target: https://github.com/fls-bioinformatics-core/pegs/actions?query=workflow%3A%22Python+package%22 * Free software: 3-Clause BSD License * Documentation: https://pegs.readthedocs.io/en/latest/ * Code: https://github.com/fls-bioinformatics-core/pegs ----------- Quick Start ----------- It is recommended to install ``PEGS`` via ``pip`` in a virtualenv, for example:: % virtualenv venv.pegs % source venv.pegs/bin/activate % pip install pegs This will provide the ``pegs`` and ``mk_pegs_intervals`` programs. Alternatively you can install ``PEGS`` into a Conda environment, for example:: % conda create -c bioconda -c conda-forge -n pegs pegs The simplest use of ``PEGS`` is: :: pegs GENE_INTERVALS --peaks PEAKS1 [PEAKS2 ...] --genes CLUSTERS1 [CLUSTERS2 ...] where ``GENE_INTERVALS`` is a set of reference transcription start sites (TSSs) for all genes, ``PEAKS1...N`` are one or more BED files with peak-sets, and ``CLUSTERS1...N`` are one or more files defining gene clusters. This will output a PNG heatmap and an XLXS file with the p-values and gene counts from the enrichment calculations. Full documentation can be found at: * https://pegs.readthedocs.io/en/latest/ --------- Licensing --------- PEGS is licensed under the 3-Clause BSD License. --------------------------------------- Example: Mouse glucocorticoidal dataset --------------------------------------- *Thanks to Louise Hunter for providing this example.* An example application of ``PEGS`` is the analysis of up- and down-regulated glucocorticoidal targets from an RNA-seq study of liver samples from mice treated acutely with dexamethasone or vehicle (Caratti *et al.* 2018), combined with corresponding GR ChIP-seq and chromatin accessibility data (DNase I hypersensitive (DHS) regions) (Grontved *et al.* 2013 and Sobel *et al.* 2017 respectively), and mouse liver TAD boundary data (Kim *et al.* 2018). The files for these datasets are available here: * RNA-seq data: `1_zt6up.txt <https://raw.githubusercontent.com/fls-bioinformatics-core/pegs/master/examples/glucocorticoid_example/data/clusters/1_zt6up.txt>`_ and `2_zt6down.txt <https://raw.githubusercontent.com/fls-bioinformatics-core/pegs/master/examples/glucocorticoid_example/data/clusters/2_zt6down.txt>`_ * ChIP-seq and chromatin accessibility data: `Grontved_GR_ChIP.bed <https://raw.githubusercontent.com/fls-bioinformatics-core/pegs/master/examples/glucocorticoid_example/data/peaks/Grontved_GR_ChIP.bed>`_ and `ZT6_DNase.bed <https://raw.githubusercontent.com/fls-bioinformatics-core/pegs/master/examples/glucocorticoid_example/data/peaks/ZT6_DNase.bed>`_ * TAD boundary data: `mESC-TADs_mm10.txt <https://raw.githubusercontent.com/fls-bioinformatics-core/pegs/master/examples/glucocorticoid_example/data/mESC-TADs_mm10.txt>`_ The ``PEGS`` analysis was performed on these data using the command:: pegs mm10 \\\\ --peaks data/peaks/*.bed --genes data/clusters/*.txt \\\\ -t data/mESC-TADs_mm10.txt \\\\ -d 1000 5000 10000 50000 100000 200000 500000 1000000 5000000 The resulting output files (XLSX and heatmap) can be found here: * `glucocorticoid_heatmap.png <https://raw.githubusercontent.com/fls-bioinformatics-core/pegs/master/examples/glucocorticoid_example/results/glucocorticoid_heatmap.png>`_ * `glucocorticoid_results.xlsx <https://raw.githubusercontent.com/fls-bioinformatics-core/pegs/master/examples/glucocorticoid_example/results/glucocorticoid_results.xlsx>`_ .. image:: https://raw.githubusercontent.com/fls-bioinformatics-core/pegs/master/examples/glucocorticoid_example/results/glucocorticoid_heatmap.png The analyses indicate a strong association of dexamethasone up-regulated genes with dexamethasone-induced GR peaks at distances up to 500kbp from these peaks, but no evidence of down-regulated genes - indicating distinct mechanisms of gene activation and repression by glucocorticoids. At the same time, there is promoter proximal enrichment for both up-and **References** * Caratti, G. *et al.* (2018) REVERBa couples the clock to hepatic glucocorticoid action. *J Clin Invest* **128(10)**:4454-4471 * Grontved, L. *et al.* (2013) C/EBP maintains chromatin accessibility in liver and facilitates glucocorticoid receptor recruitment to steroid response elements. *EMBO J* **32(11)**, 1568-83 * Kim, Y.H. *et al.* (2018) Rev-erb\u03b1 dynamically modulates chromatin looping to control circadian gene transcription. *Science* **359(6381)**:1274-1277 * Sobel, J.A. *et al.* (2017) Transcriptional regulatory logic of the diurnal cycle in the mouse liver. *PLoS Biol* **15(4)**: e2001069 ---------------- Acknowledgements ---------------- Thanks to Louise Hunter for providing the mouse glucocorticoidal example, and to Kyle Pollina for allowing us to use the ``pegs`` name on the Python Package Index.']```"}, {"role": "user", "content": "Content from https://github.com/fls-bioinformatics-core/pegs:\n```\nRepository metadata:\n{'name': 'pegs', 'label': ['pegs', 'PEGS: An efficient tool for gene set enrichment within defined sets of genomic intervals'], 'description': [{'term': 'PEGS: Peak-set Enrichment of Gene-Sets', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['0.6.6', '0.6.5', '0.6.4', '0.6.3', '0.6.2', '0.6.1', '0.6.0', '0.5.1'], 'license': [{'term': {'name': 'BSD 3-Clause \"New\" or \"Revised\" License', 'url': 'http://choosealicense.com/licenses/bsd-3-clause/'}, 'id': 0}, {'term': {'name': 'BSD-3-Clause', 'url': ''}, 'id': 1}], 'repository': [{'term': 'https://github.com/fls-bioinformatics-core/pegs', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Peter Briggs', 'email': '', 'maintainer': False, 'type': 'person'}, 'id': 0}, {'term': {'name': 'A. Louise Hunter', 'email': '', 'maintainer': False, 'type': 'person'}, 'id': 1}, {'term': {'name': 'Shen-hsi Yang', 'email': '', 'maintainer': False, 'type': 'person'}, 'id': 2}, {'term': {'name': 'Andrew D. Sharrocks', 'email': '', 'maintainer': False, 'type': 'person'}, 'id': 3}, {'term': {'name': 'Mudassar Iqbal', 'email': '', 'maintainer': False, 'type': 'person'}, 'id': 4}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'requirements', 'url': 'https://github.com/fls-bioinformatics-core/pegs/blob/main/requirements.txt'}, 'id': 0}, {'term': {'type': 'requirements', 'url': 'https://github.com/fls-bioinformatics-core/pegs/blob/main/docs/requirements.txt'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [{'term': {'title': 'PEGS: An efficient tool for gene set enrichment within defined sets of genomic intervals [version 2; peer review: 2 approved]', 'year': 2021, 'doi': 'https://doi.org/10.12688/f1000research.53926.2', 'url': ''}, 'id': 0}], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': '', 'citation': '# This CITATION.cff file was generated with cffinit.\\n# Visit https://bit.ly/cffinit to generate yours today!\\n\\ncff-version: 1.2.0\\ntitle: >-\\n  PEGS: An efficient tool for gene set enrichment within\\n  defined sets of genomic intervals\\nmessage: >-\\n  If you use this software, please cite it using the\\n  metadata from this file.\\ntype: software\\nauthors:\\n  - given-names: Peter\\n    family-names: Briggs\\n  - given-names: A. Louise\\n    family-names: Hunter\\n    orcid: \\'https://orcid.org/0000-0002-3874-4852\\'\\n  - given-names: Shen-hsi\\n    family-names: Yang\\n  - given-names: Andrew D.\\n    family-names: Sharrocks\\n  - given-names: Mudassar\\n    family-names: Iqbal\\n    orcid: \\'https://orcid.org/0000-0002-5006-4331\\'\\nidentifiers:\\n  - type: doi\\n    value: 10.5281/zenodo.1234\\nrepository-code: \\'https://github.com/fls-bioinformatics-core/pegs\\'\\nabstract: >-\\n  PEGS (Peak-set Enrichment of Gene-Sets) is a Python\\n  bioinformatics utility for calculating enrichments of gene\\n  clusters at different genomic distances from peaks.\\nkeywords:\\n  - bioinformatics\\nlicense: BSD-3-Clause\\npreferred-citation:\\n  type: article\\n  authors:\\n    - given-names: Peter\\n      family-names: Briggs\\n    - given-names: A. Louise\\n      family-names: Hunter\\n      orcid: \\'https://orcid.org/0000-0002-3874-4852\\'\\n    - given-names: Shen-hsi\\n      family-names: Yang\\n    - given-names: Andrew D.\\n      family-names: Sharrocks\\n    - given-names: Mudassar\\n      family-names: Iqbal\\n      orcid: \\'https://orcid.org/0000-0002-5006-4331\\'\\n  doi: \"https://doi.org/10.12688/f1000research.53926.2\"\\n  journal: \"F1000Research\"\\n  title:  >-\\n    PEGS: An efficient tool for gene set enrichment within\\n    defined sets of genomic intervals [version 2; peer review:\\n    2 approved]\\n  issue: 579\\n  volume: 10\\n  year: 2021\\n'}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"cmfinder/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/cmfinder/cmd/0.4.1.9\",\n    \"name\": \"cmfinder\",\n    \"description\": [\n      \"CMfinder - A Covariance Model Based RNA Motif Finding Algorithm.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://sourceforge.net/projects/weinberg-cmfinder/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/cmfinder/cmd/0.3\",\n    \"name\": \"cmfinder\",\n    \"description\": [],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Frnateam%2Fgraphclust_cmfinder%2FcmFinder%2F0.3\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/weinberg-cmfinder/:\n```\nProject metadata:\n{'description': 'The CMfinder package is a suite of programs with two main purposes:\\n(1) the cmfinder program infers an alignment and consensus secondary structure of an RNA, using unaligned sequences as input.\\n(2) other tools score alignments to indicate evidence of RNA secondary structure within the alignment.  RNAphylo assigns a probabilistic score to an existing alignment, using an explicit phylogenetic model. hmmpair assigns a score based on evidence of covariation that is supported by sequence conservation. The ScoreMotif.pl script combines these two scores into one.\\nCMfinder 0.4.1 was written by Zasha Weinberg, based on the CMfinder 0.3 package by Zizhen Yao, Infernal 1.1 by Eric Nawrocki and Sean Eddy, and others.  The main differences of CMfinder 0.4.1 are handling truncated metagenomic sequences, integration of infernal 1.1 and the hmmpair program.\\nSee the Wiki for additional information, and citations.', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['/p/weinberg-cmfinder/activity/feed', '/p/weinberg-cmfinder/activity/', 'https://sourceforge.net/auth/?return_to=%2Fprojects%2Fweinberg-cmfinder%2F']}, {'text': 'License\\nGNU General Public License version 3.0 (GPLv3)\\nLicense\\nGNU General Public License version 3.0 (GPLv3)', 'hrefs': []}, {'text': 'Follow Weinberg-CMfinder\\nWeinberg-CMfinder Web Site\\nFollow Weinberg-CMfinder\\nWeinberg-CMfinder Web Site', 'hrefs': ['https://weinberg-cmfinder.sourceforge.io']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof Weinberg-CMfinder!\\nUser Reviews\\nBe the first to\\npost a review\\nof Weinberg-CMfinder!', 'hrefs': ['/projects/weinberg-cmfinder/reviews/new']}, {'text': 'Additional Project Details\\nRegistered\\n2017-02-23\\nAdditional Project Details\\nRegistered\\n2017-02-23', 'hrefs': []}]}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"drawcell/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconductor/drawcell/lib/0.99.0\",\n    \"name\": \"drawcell\",\n    \"description\": [\n      \"The package drawCell allows the user the fast generation of cell images with highlighted subcellular components. It works by providing as input the taxonomy id of an organism, and one or multiple SL (Subcellular IDs). It retrieves this data from the SwissBioPics API. An internet connection is required to use drawCell.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://git.bioconductor.org/packages/drawCell\",\n        \"kind\": \"bioconductor\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/drawCell\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Alvaro Sanchez-Villalba\",\n        \"email\": \"sanchezvillalba.alvaro@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Greg Yannes\",\n        \"email\": \"greg.yannes@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"bioconductor\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/drawcell/lib/1.0.1\",\n    \"name\": \"drawcell\",\n    \"description\": [\n      \"Plots a filled bacteria cell with a given length, width, position, angle, and color specified.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://www.mathworks.com/matlabcentral/fileexchange/70264-drawcell\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Andrew Woodward\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"https://www.mathworks.com/matlabcentral/profile/8870078-andrew-woodward\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://git.bioconductor.org/packages/drawCell:\n```\nRepository metadata:\n{'url': 'https://git.bioconductor.org/packages/drawCell'}```"}, {"role": "user", "content": "Content from https://www.mathworks.com/matlabcentral/fileexchange/70264-drawcell:\n```\nContent:\n[\"drawCell - plots a filled bacteria cell with a given length, width, position, angle, and color Inputs: POSITION - coordinates of the cell centroid [x y] LENGTH - length of the cell (um) WIDTH - width of the cell (um) ANGLE - orientation of the cell in radians COLOR - color of the cell (rgb triplet) varargin - used to input the current axis hangle if given Output: handle - the handle for the cell's scatter More information about the function with some figure examples can be found here: [https://www.andrew-woodward.ca/blog/plotting-bacteria-cells-length-width-orientation-onto-a-figure-matlab](https://www.andrew-woodward.ca/blog/plotting-bacteria-cells-length-width-orientation-onto-a-figure-matlab)\"]```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/drawCell:\n```\nContent:\n['# Removed Packages A list of packages removed from Bioconductor along with their last-available landing pages. ## Packages deprecated in Bioconductor 3.21 (to be removed in 3.22) Software Packages: Experiment Data Packages: Annotation Data Packages: Workflow Packages: ## Packages removed with Bioconductor 3.21 release Software Packages: Note: [zlibbioc](/packages/3.19/bioc/html/zlibbioc.html) is deprecated but will be given an extra 6 months (removed in 3.22) to allow multiple dependencies to adapt. Experiment Data Packages: ## Packages removed with Bioconductor 3.20 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.19 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.18 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.17 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.16 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Packages: ## Packages removed with Bioconductor 3.15 release Software Packages: Experiment Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.14 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.13 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.12 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.11 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.10 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.9 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.8 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.7 release Software Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.6 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.5 release Software Packages: Experimental Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.4 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.3 release No packages were removed with this release. 17 packages were marked as deprecated, to be removed in the next release. ## Packages removed with Bioconductor 3.2 release No packages were removed with this release. ## Packages removed with Bioconductor 3.1 release ## Packages removed with Bioconductor 3.0 release ## Packages removed with Bioconductor 2.14 release ## Packages removed with Bioconductor 2.13 release ## Packages removed with Bioconductor 2.12 release ## Packages removed with Bioconductor 2.11 release ## Packages removed with Bioconductor 2.10 release ## Packages removed with Bioconductor 2.9 release ## Packages removed with Bioconductor 2.8 release ## Packages removed with Bioconductor 2.7 release ## Packages removed with Bioconductor 2.6 release ## Packages removed with Bioconductor 2.5 release ## Packages removed with Bioconductor 2.4 release ## Packages removed with Bioconductor 2.3 release ## Packages removed with Bioconductor 2.2 release ## Packages removed with Bioconductor 2.1 release ## Packages removed with Bioconductor 2.0 release ## Packages removed with Bioconductor 1.9 release ## Packages removed with Bioconductor 1.8 release ## Packages removed with Bioconductor 1.7 release']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"ucsc-twobittofa/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/ucsc-twobittofa/cmd/455\",\n    \"name\": \"ucsc-twobittofa\",\n    \"description\": [\n      \"Convert all or part of .2bit file to fasta.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://hgdownload.cse.ucsc.edu/admin/exe/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"varies; see http://genome.ucsc.edu/license\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/ucsc-twobittofa/cmd/357\",\n    \"name\": \"ucsc-twobittofa\",\n    \"description\": [\n      \"Convert all or part of .2bit file to FASTA.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fucsc_twobittofa%2Fucsc-twobittofa%2F357\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"emmtyper/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/emmtyper/cmd/0.2.0\",\n    \"name\": \"emmtyper\",\n    \"description\": [\n      \"Streptococcus pyogenes in silico EMM typer.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/MDUPHL/emmtyper\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/MDUPHL/emmtyper\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"toolshed/emmtyper/cmd/0.1.0\",\n    \"name\": \"emmtyper\",\n    \"description\": [],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/MDU-PHL/emmtyper\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"toolshed\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/MDU-PHL/emmtyper:\n```\nREADME content:\n['# emmtyper - Emm Automatic Isolate Labeller [![CI](https://github.com/MDU-PHL/emmtyper/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/MDU-PHL/emmtyper/actions/workflows/unit-tests.yml) [![Coverage Status](https://coveralls.io/repos/github/MDU-PHL/emmtyper/badge.svg?branch=master)](https://coveralls.io/github/MDU-PHL/emmtyper?branch=master) ![PyPI - Implementation](https://img.shields.io/pypi/implementation/emmtyper) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/emmtyper) ![PyPI](https://img.shields.io/pypi/v/emmtyper) ![PyPI - License](https://img.shields.io/pypi/l/emmtyper) ![PyPI - Wheel](https://img.shields.io/pypi/wheel/emmtyper) ![Conda](https://img.shields.io/conda/pn/bioconda/emmtyper?label=bioconda) ![PyPI - Downloads](https://img.shields.io/pypi/dm/emmtyper) ![PyPI - Status](https://img.shields.io/pypi/status/emmtyper) ![GitHub issues](https://img.shields.io/github/issues-raw/MDU-PHL/emmtyper) ## Table of Content - [emmtyper - Emm Automatic Isolate Labeller](#emmtyper---emm-automatic-isolate-labeller) - [Table of Content](#table-of-content) - [Background](#background) - [Inner workings](#inner-workings) - [Requirements](#requirements) - [Installation](#installation) - [Brew](#brew) - [Conda](#conda) - [Usage](#usage) - [Example Commands](#example-commands) - [Result Format](#result-format) - [Short format](#short-format) - [Verbose format](#verbose-format) - [Visual format](#visual-format) - [Tags](#tags) - [Example outputs](#example-outputs) - [BLAST or PCR?](#blast-or-pcr) - [Validation data](#validation-data) - [Authors](#authors) - [Maintainer](#maintainer) - [Issues](#issues) - [References](#references) ## Background `emmtyper` is a command line tool for emm-typing of _Streptococcus pyogenes_ using a _de novo_ or complete assembly. By default, we use the U.S. Centers for Disease Control and Prevention trimmed emm subtype database, which can be found [here](https://www2a.cdc.gov/ncidod/biotech/strepblast.asp). The database is curated by Dr. Velusamy Srinivasan. We take this opportunity to thank Dr. Srinivasan for his work. The tool has two basic modes: * `blast`: In this mode the contigs are blasted against the trimmed FASTA database curated by the CDC. * `pcr`: In this mode, first an _in silico_ PCR is done on the contigs using the `isPCR` tool (Kuhn et al. 2013). The resulting fragments are then blasted against the trimmed FASTA database curated by the CDC. Two sets of primers are provided for the user to choose from: 1. The canonical CDC primers used for **conventional** PCR (Whatmore and Kehoe 1994); 2. the primers described by Frost et al. 2020. This last set uses the forward primers of Whatmore and Kehoe (1994), but provide a re-designed reverse primer. There is also the option of the user providing their own primer set in the format required by the `isPCR` tool. ### Inner workings The difficulty in performing M-typing is that there is a single gene of interest (`emm`), but two other homologue genes (`enn` and `mrp`), often referred to as `emm-like`. The homologue genes may or may not occur in the isolate of interest. When performing `emm-typing` from an assembly, we can distinguish betweeen one or more clusters of matches on the contigs. The best match for each of the clusters identified is then parsed from the BLAST results. Where possible, we try to distinguish between matches to the `emm` gene, and matches to one of the `emm-like` genes. Possible arrangments: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_emm_ ---->>>>>>>---- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_mrp_&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_emm_&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_enn_ ---->>>>>>----->>>>>>------>>>>>>----- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_emm_&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_enn_ ----->>>>>>------>>>>>>----- ## Requirements - `blastn` \u2265 2.6 (tested on 2.9) - `isPcr` - `python` \u2265 3.6 ## Installation ### Brew ```bash brew install python blast ispcr pip3 install emmtyper emmtyper --help ``` ### Conda ```bash conda install -c conda-forge -c bioconda -c defaults emmtyper ``` ## Usage `emmtyper` has two workflows: directly BLASTing the contigs against the DB, or using `isPcr` to generate an _in silico_ PCR product that is then BLASTed against the DB. The BLAST results go through `emmtyper`\\'s business logic to distinguish between `emm` and `emm-like` alleles and derive the isoolate M-type. The basic usage of emmtyper is in the form of: ```bash emmtyper [options] contig1 contig2 ... contigN ``` All the available options can be printed to the console with `emmtyper --help`. Options passed on to `blast` are tagged with `[BLAST]`, and those for `isPcr` are tagged with `[isPcr]`. ```bash Usage: emmtyper [OPTIONS] [FASTA]... Welcome to emmtyper. Usage: emmtyper *.fasta Options: --version Show the version and exit. -w, --workflow [blast|pcr] Choose workflow [default: blast] -db, --blast_db TEXT Path to EMM BLAST DB [default: /path/to/emmtyper/db/emm.fna] -k, --keep Keep BLAST and isPcr output files. [default: False] -d, --cluster-distance INTEGER Distance between cluster of matches to consider as different clusters. [default: 500] -o, --output TEXT Output stream. Path to file for output to a file. [default: stdout] -f, --output-format [short|verbose|visual] Output format. --dust [yes|no|level window linker] [BLAST] Filter query sequence with DUST. [default: no] --percent-identity INTEGER [BLAST] Minimal percent identity of sequence. [default: 95] --culling-limit INTEGER [BLAST] Total hits to return in a position. [default: 5] --mismatch INTEGER [BLAST] Threshold for number of mismatch to allow in BLAST hit. [default: 4] --align-diff INTEGER [BLAST] Threshold for difference between alignment length and subject length in BLAST hit. [default: 5] --gap INTEGER [BLAST] Threshold gap to allow in BLAST hit. [default: 2] --blast-path TEXT [BLAST] Specify full path to blastn executable. --pcr-primers [cdc|frost] [isPcr] Primer set to use (either canonical CDC or Frost et al. 2020). [default: cdc] --primer-db TEXT [isPcr] PCR primer. Text file with 3 columns: Name, Forward Primer, Reverse Primer. This options overrides --pcr- primers. --min-perfect INTEGER [isPcr] Minimum size of perfect match at 3\\\\\\' primer end. [default: 15] --min-good INTEGER [isPcr] Minimum size where there must be 2 matches for each mismatch. [default: 15] --max-size INTEGER [isPcr] Maximum size of PCR product. [default: 2000] --ispcr-path TEXT [isPcr] Specify full path to isPcr executable. --help Show this message and exit. ``` Most of these options are self explanatory. The two expections are: 1. `--workflow`: choose between a `blast` only workflow, or a _in silico_ PCR followed by `blast` workflow. See below for more information. 2. `--clust_distance` defines the minimum distance between clusters of matched sequences on the contigs to generate separate `emm-type` calls for each clusters. Clusters of matches that are within the minimum `clust-distance` are treated as a single location match. 3. `--output_type` demonstrated below. ### Example Commands ```bash # basic call using the blast workflow for a single contig file emmtyper isolate1.fa # basic call using the pcr workflow for all the .fa files in a folder emmtyper -w pcr *.fa # basic call changing some of the options for blast emmtyper --keep --culling_limit 10 --align_diff 10 *.fa # call using the pcr workflow changing some of the isPcr options and # using the visual output format - this will run the with the CDC canonical # primers emmtyper -w pcr --output-format visual --max-size 2000 --mismatch 5 *.fa # same as above, but now with the Frost et al. 2020 primers. emmtyper -w pcr --output-format visual --max-size 2000 --mismatch 5 --pcr-primers frost *.fa ``` ## Result Format ### Short format emmtyper has three different result formats: `short`, `verbose`, and `visual`. emmtyper by default produces the `short` version. This consists of five values in tab-separated format printed to stdout. The values are: - Isolate name - Number of clusters: should be between 1 and 3, larger values could indicate contamination - Predicted `emm-type` - Possible `emm-like` alleles (semi-colon separated list) - EMM cluster: Functional grouping of EMM types into 48 clusters ### Verbose format The verbose result returns: - Isolate name - Number of BLAST hits - Number of clusters: should be between 1 and 3, larger values could indicate contamination - Predicted `emm-type` - Position(s) `emm-like` alleles in the assembly - Possible `emm-like` alleles (semi-colon separated list) - `emm-like` position(s) in assembly - EMM cluster: Functional grouping of EMM types into 48 clusters The positions in the assembly are presented in the following format `<contig_number>:<position_in_contig>`. ### Visual format The visual result returns an ASCII map of the `emm` and, if found any `emm-alleles`, in the genome. Alleles on a single contig are separated by \"-\", with each \"-\" representing 500bp. Alleles found on different contigs are separated with tab. ### Tags The alleles can be tagged with a suffix character to indicate different possibilities: | Tag | Description | Additional Information | | --- | ------------------ | --------------------------------------------------------- | | \\\\* | Suspect `emm-like` | Allele flagged in the CDC database as possibly `emm-like` | | ~ | Imperfect score | Match score below 100% | ### Example outputs Example for all result format: Short format: ``` Isolate1 1 EMM65.0 E6 Isolate2 3 EMM4.0 EMM236.3*;EMM156.0* E1 Isolate3 2 EMM52.1 EMM134.2* D4 ``` Verbose format: ``` Isolate1 6 1 EMM65.0 5:82168 E6 Isolate2 8 3 EMM4.0 2:104111 EMM236.3*;EMM156.0* 2:102762;2:105504 E1 Isolate3 5 2 EMM52.1 14:10502 EMM134.2* 5:913 D4 ``` Visual format: ``` Isolate1 EMM65.0 Isolate2 EMM156.0*--EMM4.0--EMM236.3* Isolate3 EMM52.1 EMM134.2* ``` ## BLAST or PCR? If you are not sure which pipeline to choose from, we recommend using `blast` first. The `blast` workflow is fast and works well with assemblies. You can then use the `pcr` mode if you wish to perform some troubleshooting. For example, the `pcr` workflow might be useful when troubleshooting isolates for which emmtyper has reported more than 3 clusteres and/or too many alleles. An important thing to note is that not all `emm-like` alleles can be identified by using by PCR typing. The `pcr` workflow can be used to test which hits would be returned if carrying out conventional M-typing using PCR. However, the workflow is not foolproof, as _in silico_ PCR will fail when one or both primers do not align in the same contig (i.e., the allele is broken across two or more contigs) or there are mutations in the primer sites. In the former case, this might be an indication of poor sequence coverage or contamination. ## Validation data We compared `emmtyper` against `Sanger` sequencing data and PHE\\'s tool [`emm-typing-tool`](https://github.com/phe-bioinformatics/emm-typing-tool). You can check out the validation comparison go to out binder: [![badge](https://img.shields.io/badge/launch-binder-579ACA.svg?logo=data:image/png;base64,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)](https://mybinder.org/v2/gh/MDU-PHL/emmtyper/946ddb74e7ce92654567630bd5787430d5945451) ## Authors - Andre Tan - Torsten Seemann - Kristy Horan - Jake Lacey - Himal Shrestha - Raquel Cooper - Anders Gon&ccedil;alves da Silva ## Acknowledgements The codebase for `emmtyper` was primarly written by Andre Tan as part of his Master\\'s Degree in Bioinformatics. Torsten Seemann, Deborah Williamson, and Anders Gon&ccedil;alves da Silva provided supervision and assistance. Hannah Frost contributed with EMM clustering by suggesting we incorporate it in to the code, and providing the necessary information to do so and test it. Validation was conducted within MDU-Bioinformatics `emmtyper`. ## Maintainer The code is actively maintained by MDU Bioinformatics Team. Contact the principal maintainer at MDU-Bioinformatics@unimelb.edu.au. ## Issues Please post bug reports, questions, suggestions in the [Issues](https://github.com/MDU-PHL/emmtyper/issues) section. ## References Frost, H. R., Davies, M. R., Velusamy, S., Delforge, V., Erhart, A., Darboe, S., Steer, A., Walker, M. J., Beall, B., Botteaux, A., & Smeesters, P. R. (2020). Updated emm-typing protocol for Streptococcus pyogenes. Clinical Microbiology and Infection: The Official Publication of the European Society of Clinical Microbiology and Infectious Diseases, 26(7), 946.e5\u2013e946.e8. Kuhn, R. M., Haussler, D., & Kent, W. J. (2013). The UCSC genome browser and associated tools. Briefings in Bioinformatics, 14(2), 144\u2013161. Whatmore, A. M., & Kehoe, M. A. (1994). Horizontal gene transfer in the evolution of group A streptococcal emm-like genes: gene mosaics and variation in Vir regulons. Molecular Microbiology, 11(2), 363\u2013374.']```"}, {"role": "user", "content": "Content from https://github.com/MDU-PHL/emmtyper:\n```\nRepository metadata:\n{'name': 'emmtyper', 'label': ['emmtyper'], 'description': [{'term': 'emm Automatic Isolate Labeller', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/MDU-PHL/emmtyper', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/streptococcus-pyogenes', 'term': 'streptococcus-pyogenes', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/emm-typing', 'term': 'emm-typing', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/genotyping', 'term': 'genotyping', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/genome-annotation', 'term': 'genome-annotation', 'vocabulary': ''}, 'id': 4}], 'operations': [], 'authors': [{'term': {'name': 'Jake Lacey', 'type': 'person', 'email': '38679862+JA-Lacey@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Himal Shrestha', 'type': 'person', 'email': 'himal.shrestha@unimelb.edu.au', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Josh Zhang', 'type': 'person', 'email': 'josh.zhang@unimelb.edu.au', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Josh Zhang', 'type': 'person', 'email': 'zjs0317@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Anders Goncalves da Silva', 'type': 'person', 'email': 'andersgs@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'pedrorvc', 'type': 'person', 'email': 'pedrorvc@gmail.com', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Torsten Seemann', 'type': 'person', 'email': 'tseemann@users.noreply.github.com', 'maintainer': False}, 'id': 6}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/MDU-PHL/emmtyper/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/MDU-PHL/emmtyper/blob/main/paper.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"spades/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/spades/cmd/3.15.5\",\n    \"name\": \"spades\",\n    \"description\": [\n      \"SPAdes (St. Petersburg genome assembler) is a genome assembly algorithm which was designed for\\nsingle cell and multi-cells bacterial data sets. However, it might not be suitable for large\\ngenomes projects.\\n\\nSPAdes works with Ion Torrent, PacBio, Oxford Nanopore, and Illumina paired-end, mate-pairs and\\nsingle reads.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/ablab/spades\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"http://cab.spbu.ru/software/spades/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-2.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-2.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a171d3b8acd1a2f9b4ee\"\n      },\n      {\n        \"$oid\": \"67e2dc48ac7ee6cb578912d1\"\n      },\n      {\n        \"$oid\": \"67c82cd010e2f3aa0b3b990b\"\n      },\n      {\n        \"$oid\": \"67c7a12bd3b8acd1a2f9b4d0\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/spades/cmd/3.11.1\",\n    \"name\": \"spades\",\n    \"description\": [\n      \"Genome assembler for regular and single-cell projects.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fnml%2Fspades%2Fspades%2F3.11.1\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/ablab/spades:\n```\nREADME content:\n['[![UnitTests](https://github.com/ablab/spades/actions/workflows/basic.yml/badge.svg)](https://github.com/ablab/IsoQuant/actions/workflows/Unit_tests.yml) [![License](https://img.shields.io/badge/licence-GPLv2-blue)](https://www.gnu.org/licenses/old-licenses/gpl-2.0) [![GitHub release (latest)](https://img.shields.io/github/v/release/ablab/spades)](https://github.com/ablab/spades/releases/) [![GitHub Downloads](https://img.shields.io/github/downloads/ablab/spades/total.svg?style=social&logo=github&label=Download)](https://github.com/ablab/spades/releases) [![BioConda Downloads](https://anaconda.org/bioconda/spades/badges/downloads.svg)](https://anaconda.org/bioconda/spades) [![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg)](code_of_conduct.md) # About SPAdes SPAdes is a versatile toolkit designed for assembly and analysis of sequencing data. SPAdes is primarily developed for Illumina sequencing data, but can be used for IonTorrent as well. Most of SPAdes pipelines support hybrid mode, i.e. allow using long reads (PacBio and Oxford Nanopore) as a supplementary data. SPAdes package contains assembly pipelines for isolated and single-cell bacterial, as well as metagenomic and transcriptomic data. Additional modes allow to discover bacterial plasmids and RNA viruses, as well as perform HMM-guided assembly. Besides, SPAdes package includes supplementary tools for efficient k-mer counting and k-mer-based read filtering, assembly graph construction and simplification, sequence-to-graph alignment and metagenomic binning refinement. - [Complete SPAdes user manual](https://ablab.github.io/spades/) - [SPAdes download page](https://github.com/ablab/spades/releases/latest/) - [Latest SPAdes publication](https://currentprotocols.onlinelibrary.wiley.com/doi/abs/10.1002/cpbi.102) Current version: see `VERSION` file. # Quick start - Complete user manual can be found [here](https://ablab.github.io/spades/). Information below is provided merely for your convenience and cannot be considered as the user guide. - SPAdes assembler supports: - Assembly of second-generation sequencing data (Illumina or IonTorrent); - PacBio and Nanopore reads that are used as supplementary data only. - SPAdes allows to assemble genomes, metagenomes, transcriptomes, viral genomes etc. - Download SPAdes binaries for Linux or MacOS [here](https://github.com/ablab/spades/releases/latest/). You can also compile SPAdes from [source](https://github.com/ablab/spades/releases/latest/) (requires g++ 9.0+, cmake 3.16+, zlib and libbz2). SPAdes requires only Python 3.8+ to be installed. - Test your SPAdes installation by running ``` bin/spades.py --test ``` - A single paired-end library (separate files, gzipped): ``` bin/spades.py -1 left.fastq.gz -2 right.fastq.gz -o output_folder ``` - IonTorrent data: ``` bin/spades.py --iontorrent -s it_reads.fastq -o output_folder ``` - A paired-end library coupled with long PacBio reads: ``` bin/spades.py -1 left.fastq.gz -2 right.fastq.gz --pacbio pb.fastq -o output_folder ``` - Available assembly modes: `--isolate`, `--sc`, `--plasmid`, `--meta`, `--metaplasmid`, `--metaviral`, `--rna`, `--rnaviral`, `--bio`, `--corona`, `--sewage`. - Standalone tools in SPAdes package: k-mer counting, k-mer cardinality estimation, k-mer-based read filtering, assembly graph construction, assembly graph simplification, alignment of long reads to an assembly graph, refinement of metagenomic binning. # Citation If you use SPAdes in your research, please cite [our latest paper](https://currentprotocols.onlinelibrary.wiley.com/doi/abs/10.1002/cpbi.102). In case you perform hybrid assembly using PacBio or Nanopore reads, you may also cite [Antipov et al., 2015](http://bioinformatics.oxfordjournals.org/content/early/2015/11/20/bioinformatics.btv688.short). If you use multiple paired-end and/or mate-pair libraries you may additionally cite papers describing SPAdes repeat resolution algorithms [Prjibelski et al., 2014](http://bioinformatics.oxfordjournals.org/content/30/12/i293.short) and [Vasilinetc et al., 2015](http://bioinformatics.oxfordjournals.org/content/31/20/3262.abstract). If you use other pipelines, please cite the following papers: - metaSPAdes: [Nurk et al., 2017](https://genome.cshlp.org/content/27/5/824.short). - plasmidSPAdes: [Antipov et al., 2016](https://academic.oup.com/bioinformatics/article/32/22/3380/2525610). - metaplasmidSPAdes / plasmidVerify: [Antipov et al., 2019](https://genome.cshlp.org/content/29/6/961.short) - metaviralSPAdes / viralVerify: [Antipov et al., 2020](https://academic.oup.com/bioinformatics/article-abstract/36/14/4126/5837667) - rnaSPAdes: [Bushmanova et al., 2019](https://academic.oup.com/gigascience/article/8/9/giz100/5559527). - biosyntheticSPAdes: [Meleshko et al., 2019](https://genome.cshlp.org/content/early/2019/06/03/gr.243477.118?top=1). - coronaSPAdes: [Meleshko et al., 2022](https://academic.oup.com/bioinformatics/article/38/1/1/6354349). You may also include older papers [Nurk, Bankevich et al., 2013](http://link.springer.com/chapter/10.1007%2F978-3-642-37195-0_13) or [Bankevich, Nurk et al., 2012](http://online.liebertpub.com/doi/abs/10.1089/cmb.2012.0021), especially if you assemble single-cell data. # Feedback and bug reports Please, leave your comments and bug reports at [our GitHub repository tracker](https://github.com/ablab/spades/issues). If you have any troubles running SPAdes, please attach `params.txt` and `spades.log` from the output folder.']```"}, {"role": "user", "content": "Content from https://github.com/ablab/spades:\n```\nRepository metadata:\n{'name': 'spades', 'label': ['spades'], 'description': [{'term': 'SPAdes Genome Assembler', 'id': 0}], 'links': [], 'webpage': [{'term': 'http://ablab.github.io/spades/', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['continuous', 'v4.1.0', 'v4.0.0', 'cloudspades-0.1', 'rnacloudSPAdes', 'splitter-preprint', 'v3.15.5', 'binspreader-recombseq', 'v3.15.4', 'v3.15.3', 'v3.15.2', 'v3.15.1', 'v3.15.0', 'v3.14.1', 'v3.14.0', 'spaligner-paper', 'cami2-submission', 'cloudspades-paper', 'v0.5-recomb', 'v3.13.1', 'metaplasmid-paper', 'v3.13.0', 'v3.12.0', 'v3.11.1', 'v3.11.0', 'v3.10.1', 'v3.10.0', 'v3.9.0', 'v3.8.2', 'v3.8.1', 'v3.8.0'], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/ablab/spades', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/genome-assembly', 'term': 'genome-assembly', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/sequence-assembler', 'term': 'sequence-assembler', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/sequencing', 'term': 'sequencing', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/transcriptome-assembly', 'term': 'transcriptome-assembly', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/illumina', 'term': 'illumina', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/metagenome-assembly', 'term': 'metagenome-assembly', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/next-generation-sequencing', 'term': 'next-generation-sequencing', 'vocabulary': ''}, 'id': 6}], 'operations': [], 'authors': [{'term': {'name': 'andrewprzh', 'type': 'person', 'email': 'andrewprzh@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Simon-Brandt', 'type': 'person', 'email': 'simon.brandt@uni-greifswald.de', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Anton Korobeynikov', 'type': 'person', 'email': 'anton@korobeynikov.info', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Itolstoganov', 'type': 'person', 'email': 'itolstoganov@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Douglas Scofield', 'type': 'person', 'email': 'douglasgscofield@gmail.com', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Bruno Bzeznik', 'type': 'person', 'email': 'Bruno.Bzeznik@imag.fr', 'maintainer': False}, 'id': 5}, {'term': {'name': '1dayac', 'type': 'person', 'email': '1dayac@gmail.com', 'maintainer': False}, 'id': 6}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/ablab/spades/blob/main/GPLv2.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/ablab/spades/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'changelog', 'url': 'https://github.com/ablab/spades/blob/main/changelog.md'}, 'id': 2}, {'term': {'type': 'code_of_conduct', 'url': 'https://github.com/ablab/spades/blob/main/code_of_conduct.md'}, 'id': 3}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/binspreader.md'}, 'id': 4}, {'term': {'type': 'citation', 'url': 'https://github.com/ablab/spades/blob/main/docs/citation.md'}, 'id': 5}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/datatypes.md'}, 'id': 6}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/feedback.md'}, 'id': 7}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/getting-started.md'}, 'id': 8}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/hmm.md'}, 'id': 9}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/hybrid.md'}, 'id': 10}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/index.md'}, 'id': 11}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/input.md'}, 'id': 12}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/installation.md'}, 'id': 13}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/output.md'}, 'id': 14}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/pathracer.md'}, 'id': 15}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/rna.md'}, 'id': 16}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/running.md'}, 'id': 17}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/spaligner.md'}, 'id': 18}, {'term': {'type': 'docs', 'url': 'https://github.com/ablab/spades/blob/main/docs/standalone.md'}, 'id': 19}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"minimap/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/minimap/cmd/0.2_r124\",\n    \"name\": \"minimap\",\n    \"description\": [\n      \"Experimental tool to find approximate mapping positions between long sequences.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/lh3/minimap\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/lh3/minimap\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/minimap2/cmd/2.5\",\n    \"name\": \"minimap2\",\n    \"description\": [\n      \"A fast pairwise aligner for genomic and spliced nucleotide sequences.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fminimap2%2Fminimap2%2F2.5\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/lh3/minimap:\n```\nREADME content:\n['## Introduction Minimap is an *experimental* tool to efficiently find multiple approximate mapping positions between two sets of long sequences, such as between reads and reference genomes, between genomes and between long noisy reads. By default, it is tuned to have high sensitivity to 2kb matches around 20% divergence but with low specificity. Minimap does not generate alignments as of now and because of this, it is usually tens of times faster than mainstream *aligners*. With four CPU cores, minimap can map 1.6Gbp PacBio reads to human in 2.5 minutes, 1Gbp PacBio E. coli reads to pre-indexed 9.6Gbp bacterial genomes in 3 minutes, to pre-indexed >100Gbp nt database in ~1 hour (of which ~20 minutes are spent on loading index from the network filesystem; peak RAM: 10GB), map 2800 bacteria to themselves in 1 hour, and map 1Gbp E. coli reads against themselves in a couple of minutes. Minimap does not replace mainstream aligners, but it can be useful when you want to quickly identify long approximate matches at moderate divergence among a huge collection of sequences. For this task, it is much faster than most existing tools. ## Usage * Map two sets of long sequences: ```sh minimap target.fa.gz query.fa.gz > out.mini ``` The output is TAB-delimited with each line consisting of query name, length, 0-based start, end, strand, target name, length, start, end, the number of matching bases, the number of co-linear minimizers in the match and the fraction of matching bases. * All-vs-all PacBio read self-mapping for [miniasm][miniasm]: ```sh minimap -Sw5 -L100 -m0 reads.fa reads.fa | gzip -1 > reads.paf.gz ``` * Prebuild index and then map: ```sh minimap -d target.mmi target.fa.gz minimap -l target.mmi query.fa.gz > out.mini ``` Minimap indexing is very fast (1 minute for human genome; 50 minutes for >100Gbp nt database retrieved on 2015-09-30), but for huge repeatedly used databases, prebuilding index is still preferred. * Map sequences against themselve without diagnal matches: ```sh minimap -S sequences.fa sequences.fa > self-match.mini ``` The output may still contain overlapping matches in repetitive regions. ## Algorithm Overview 1. Indexing. Collect all [(*w*,*k*)-minimizers][mini] in a batch (**-I**=4 billion bp) of target sequences and store them in a hash table. Mark top **-f**=0.1% of most frequent minimizers as repeats. Minimap uses [invertible hash function][invhash] to avoid taking ploy-A as minimizers. 2. For each query, collect all (*w*,*k*)-minimizers and look up the hash table for matches (*q<sub>i</sub>*,*t<sub>i</sub>*,*s<sub>i</sub>*), where *q<sub>i</sub>* is the query position, *t<sub>i</sub>* the target position and *s<sub>i</sub>* indicates whether the minimizer match is on the same strand. 3. For matches on the same strand, sort by {*q<sub>i</sub>*-*t<sub>i</sub>*} and then cluster matches within a **-r**=500bp window. Minimap merges two windows if **-m**=50% of minimizer matches overlap. For matches on different strands, sort {*q<sub>i</sub>*+*t<sub>i</sub>*} and apply a similar clustering procedure. This is inspired by the [Hough transformation][hough]. 4. For each cluster, sort (*q<sub>i</sub>*,*t<sub>i</sub>*) by *q<sub>i</sub>* and solve a [longest increasing sequence problem][lis] for *t<sub>i</sub>*. This finds the longest co-linear matching chain. Break the chain whenever there is a gap longer than **-g**=10000. 5. Output the start and end of the chain if it contains **-c**=4 or more minimizer matches and the matching length is no less than **-L**=40. 6. Go to 1 and rewind to the first record of query if there are more target sequences; otherwise stop. To increase sensitivity, we may decrease **-w** to index more minimizers; we may also decrease **-k**, though this may greatly impact performance for mammalian genomes. Also note that by default, if the total length of target sequences is less than 4Gbp (1G=1 billion; controlled by **-I**), minimap creates one index and stream all the query sequences in one go. The multiple hits of a query sequence is adjacent to each other in the output. If the total length is greater than 4Gbp, minimap needs to read query sequences multiple times. The multiple hits of a query may not be adjacent. [mini]: http://bioinformatics.oxfordjournals.org/content/20/18/3363.abstract [lis]: https://en.wikipedia.org/wiki/Longest_increasing_subsequence [hough]: https://en.wikipedia.org/wiki/Hough_transform [invhash]: https://gist.github.com/lh3/974ced188be2f90422cc [miniasm]: https://github.com/lh3/miniasm']```"}, {"role": "user", "content": "Content from https://github.com/lh3/minimap:\n```\nRepository metadata:\n{'name': 'minimap', 'label': ['minimap'], 'description': [{'term': 'This repo is DEPRECATED. Please use minimap2, the successor of minimap.', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://github.com/lh3/minimap2', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.2', 'v0.1'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/lh3/minimap', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Heng Li', 'type': 'person', 'email': 'lh3@me.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Christopher Dunn', 'type': 'person', 'email': 'cdunn@pacificbiosciences.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Sascha Steinbiss', 'type': 'person', 'email': 'sascha@steinbiss.name', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/lh3/minimap/blob/main/LICENSE.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/lh3/minimap/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"ectyper/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/ectyper/cmd/1.0.0\",\n    \"name\": \"ectyper\",\n    \"description\": [\n      \"ECtyper is a python program for serotyping E. coli genomes.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/phac-nml/ecoli_serotyping\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/phac-nml/ecoli_serotyping\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Apache-2.0\",\n        \"url\": \"https://spdx.org/licenses/Apache-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/ectyper/cmd/None\",\n    \"name\": \"ectyper\",\n    \"description\": [\n      \"Ectyper is a standalone serotyping module for Escherichia coli. It supports fasta and fastq file formats. (Galaxy Version 1.0.0).\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://usegalaxy.eu/root?tool_id=ectyper\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Kyrylo Bessonov\",\n        \"email\": \"kyrylo.bessonov@canada.ca\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"Chad Laing\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": null,\n        \"name\": \"James Robertson\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8351210e2f3aa0b3bc0f9\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://usegalaxy.eu/root?tool_id=ectyper:\n```\nContent:\n['### Javascript Required for Galaxy The Galaxy analysis interface requires a browser with Javascript enabled. Please enable Javascript and refresh this page.']```"}, {"role": "user", "content": "Content from https://github.com/phac-nml/ecoli_serotyping:\n```\nREADME content:\n['[![European Galaxy server](https://img.shields.io/badge/usegalaxy-.eu-brightgreen?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAASCAYAAABB7B6eAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAACXBIWXMAAAsTAAALEwEAmpwYAAACC2lUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNS40LjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczp0aWZmPSJodHRwOi8vbnMuYWRvYmUuY29tL3RpZmYvMS4wLyI+CiAgICAgICAgIDx0aWZmOlJlc29sdXRpb25Vbml0PjI8L3RpZmY6UmVzb2x1dGlvblVuaXQ+CiAgICAgICAgIDx0aWZmOkNvbXByZXNzaW9uPjE8L3RpZmY6Q29tcHJlc3Npb24+CiAgICAgICAgIDx0aWZmOk9yaWVudGF0aW9uPjE8L3RpZmY6T3JpZW50YXRpb24+CiAgICAgICAgIDx0aWZmOlBob3RvbWV0cmljSW50ZXJwcmV0YXRpb24+MjwvdGlmZjpQaG90b21ldHJpY0ludGVycHJldGF0aW9uPgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KD0UqkwAAAn9JREFUOBGlVEuLE0EQruqZiftwDz4QYT1IYM8eFkHFw/4HYX+GB3/B4l/YP+CP8OBNTwpCwFMQXAQPKtnsg5nJZpKdni6/6kzHvAYDFtRUT71f3UwAEbkLch9ogQxcBwRKMfAnM1/CBwgrbxkgPAYqlBOy1jfovlaPsEiWPROZmqmZKKzOYCJb/AbdYLso9/9B6GppBRqCrjSYYaquZq20EUKAzVpjo1FzWRDVrNay6C/HDxT92wXrAVCH3ASqq5VqEtv1WZ13Mdwf8LFyyKECNbgHHAObWhScf4Wnj9CbQpPzWYU3UFoX3qkhlG8AY2BTQt5/EA7qaEPQsgGLWied0A8VKrHAsCC1eJ6EFoUd1v6GoPOaRAtDPViUr/wPzkIFV9AaAZGtYB568VyJfijV+ZBzlVZJ3W7XHB2RESGe4opXIGzRTdjcAupOK09RA6kzr1NTrTj7V1ugM4VgPGWEw+e39CxO6JUw5XhhKihmaDacU2GiR0Ohcc4cZ+Kq3AjlEnEeRSazLs6/9b/kh4eTC+hngE3QQD7Yyclxsrf3cpxsPXn+cFdenF9aqlBXMXaDiEyfyfawBz2RqC/O9WF1ysacOpytlUSoqNrtfbS642+4D4CS9V3xb4u8P/ACI4O810efRu6KsC0QnjHJGaq4IOGUjWTo/YDZDB3xSIxcGyNlWcTucb4T3in/3IaueNrZyX0lGOrWndstOr+w21UlVFokILjJLFhPukbVY8OmwNQ3nZgNJNmKDccusSb4UIe+gtkI+9/bSLJDjqn763f5CQ5TLApmICkqwR0QnUPKZFIUnoozWcQuRbC0Km02knj0tPYx63furGs3x/iPnz83zJDVNtdP3QAAAABJRU5ErkJggg==)](https://usegalaxy.eu/root?tool_id=ectyper) ![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/phac-nml/ecoli_serotyping) ![Conda](https://img.shields.io/conda/dn/bioconda/ectyper) [![PyPI version](https://badge.fury.io/py/ectyper.svg)](https://pypi.org/project/ectyper/) ![GitHub issues](https://img.shields.io/github/issues/phac-nml/ecoli_serotyping) ![Docker Pulls](https://img.shields.io/docker/pulls/kbessonov/ectyper) ![GitHub commit activity](https://img.shields.io/github/commit-activity/t/phac-nml/ecoli_serotyping) # ECTyper (an easy typer) `ECTyper` is a standalone versatile serotyping module for _Escherichia coli_. It supports both _fasta_ (assembled) and _fastq_ (raw reads) file formats. The tool provides convenient species identification coupled to quality control module giving a complete, transparent and reference laboratories suitable report on *E.coli* serotyping, Shiga toxin typing and pathotyping. # Introduction *Escherichia coli* is a priority foodborne pathogen of public health concern and popular model organism. Phenotypic characterization such as serotyping, toxin typing and pathotyping provide critical information for surveillance and outbreak detection activities and research including source attribution, outbreak cluster assignment, pathogenicity potential, risk assessement and others. `ECTyper` uses whole-genome sequencing (WGS) for E.coli characterization including species identification, *in silico* serotyping covering O and H antigens, Shiga toxin typing and DEC pathotyping. It is a versatile, scallable, easy to use tool allowing to obtain key information on E.coli accepting both raw and assembled inputs. As WGS becomes standard within public health and research laboratories, it is important to harness the high throughput and resolution potential of this technology providing accurate and rapid at scale typing of E.coli both in public health, clinical and research contexts. ## Citation If you find `ECTyper` useful, please cite the following paper: > Bessonov, Kyrylo, Chad Laing, James Robertson, Irene Yong, Kim Ziebell, Victor PJ Gannon, Anil Nichani, Gitanjali Arya, John HE Nash, and Sara Christianson. **\"ECTyper: in silico Escherichia coli serotype and species prediction from raw and assembled whole-genome sequence data.\"** Microbial genomics 7, no. 12 (2021): 000728. [https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000728](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000728) ## Contact For any questions, issues or comments please make a Github issue or reach out to [Kyrylo Bessonov](kyrylo.bessonov@phac-aspc.gc.ca). # Installation Multiple installation options are available depending on the user context and needs. The most convinient installation is as a `conda` package as it will install all required dependencies. ### Docker and Singularity images availability Docker and Singularity images are also available from [https://biocontainers.pro/tools/ectyper](https://biocontainers.pro/tools/ectyper) that could be useful for NextFlow or hassle-free deployment ### Databases ECTyper uses multiple databases - the species identification database is available from [Zenodo](https://doi.org/10.5281/zenodo.10211568) repository - the O and H antigen allele sequences are stored in [ectyper_alleles_db.json](ectyper/Data/ectyper_alleles_db.json) - the toxin and pathotype signature marker sequences are stored in [ectyper_patho_stx_toxin_typing_database.json](ectyper/Data/ectyper_patho_stx_toxin_typing_database.json) ## Option 1: As a conda package Optionally if you do not have a conda environment, get and install `miniconda` or `anaconda`: ``` wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh bash miniconda.sh -b -p $HOME/miniconda echo \". $HOME/miniconda/etc/profile.d/conda.sh\" >> ~/.bashrc source ~/.bashrc ``` Install the latest `ectyper` conda package from a `bioconda` channel ``` conda install -c bioconda ectyper ``` ## Option 2: Install using pip Install using `pip3` utility including python but missing on [non-python dependencies](#dependencies) ``` pip3 install ectyper ``` ## Option 3: From source code Second option is to install from the source allowing to excercise maximum control over installation process. Install dependencies. On Ubuntu distro run ``` apt install samtools bowtie2 mash bcftools ncbi-blast+ seqtk ``` Install python dependencies via `pip`: ``` pip3 install pandas \"biopython<=1.85\" ``` Clone the repository or checkout a particular release (e.g `v1.0.0`, `v2.0.0` etc.): ``` git clone https://github.com/phac-nml/ecoli_serotyping.git git checkout v2.0.0 #optionally checkout a specific release version ``` Finally, install ectyper from source ``` python3 setup.py install # option 1 pip3 install . # option 2 ``` ## Compatibility ### Dependencies: - python >= 3.5 - bcftools >= 1.8 - blast == 2.7.1 - seqtk >= 1.2 - samtools >= 1.8 - bowtie2 >= 2.3.4.1 - mash >= 2.0 ### Python packages: - biopython >= 1.70,<=1.85 - pandas >= 0.23.1 - requests >= 2.0 # Getting started ## Basic Usage 1. Put the fasta/fastq files for serotyping analyses in one folder (concatenate paired raw reads files if you would like them to be considered a single entity) 1. `ectyper -i [file path] -o [output_dir]` 1. View the results on the console or in `cat [output folder]/output.csv` ## Example Input Scenarios * `ectyper -i ecoliA.fasta` for a single file (the output folder will be named using `ectyper_<date>_<time>` pattern as `-o` is not specified) * `ectyper -i ecoliA.fasta -o output_dir` for a single file with results stored in `output_dir` folder * `ectyper -i ecoliA.fasta ecoliB.fastq ecoli_folder/` for multiple files and the directory `ecoli_folder/` separated by space. Again the output folder will be named using `ectyper_<date>_<time>` pattern) * `ectyper -i ecoliA.fasta ecoliB.fastq,ecoliC.fna` for multiple files separated by comma `,` symbol * `ectyper -i ecoli_folder` scan for input files in a folder and subdirectories (all files in the folder will be checked by the tool) * `ectyper -i ecoli_folder/*.fasta` scan for FASTA input files in a folder and subdirectories with `fasta` extension ## Advanced Usage ``` usage: ectyper [-h] [-V] -i INPUT [-c CORES] [-opid PERCENTIDENTITYOTYPE] [-hpid PERCENTIDENTITYHTYPE] [-opcov PERCENTCOVERAGEOTYPE] [-hpcov PERCENTCOVERAGEHTYPE] [--verify] [-o OUTPUT] [-r REFSEQ] [-s] [--debug] [--dbpath DBPATH] ectyper v2.0.0 database v1.0 Prediction of Escherichia coli serotype from raw reads or assembled genome sequences. The default settings are recommended. options: -h, --help show this help message and exit -V, --version show program\\'s version number and exit -i INPUT, --input INPUT Location of E. coli genome file(s). Can be a single file, a comma- separated list of files, or a directory --longreads Enable for raw long reads FASTQ inputs (ONT, PacBio, other platforms) [default False] -c CORES, --cores CORES The number of cores to run ectyper with -opid PERCENTIDENTITYOTYPE, --percentIdentityOtype PERCENTIDENTITYOTYPE Percent identity required for an O antigen allele match [default 95] -hpid PERCENTIDENTITYHTYPE, --percentIdentityHtype PERCENTIDENTITYHTYPE Percent identity required for an H antigen allele match [default 95] -opcov PERCENTCOVERAGEOTYPE, --percentCoverageOtype PERCENTCOVERAGEOTYPE Minumum percent coverage required for an O antigen allele match [default 90] -hpcov PERCENTCOVERAGEHTYPE, --percentCoverageHtype PERCENTCOVERAGEHTYPE Minumum percent coverage required for an H antigen allele match [default 50] --verify Enable E. coli species verification -o OUTPUT, --output OUTPUT Directory location of output files -r REFSEQ, --refseq REFSEQ Location of pre-computed MASH RefSeq sketch. If provided, genomes identified as non-E. coli will have their species identified using MASH. For best results the pre-sketched RefSeq archive https://gembox.cbcb.umd.edu/mash/refseq.genomes.k21s1000.msh is recommended -s, --sequence Prints the allele sequences if enabled as the final columns of the output --debug Print more detailed log including debug messages --dbpath DBPATH Path to a custom database of O and H antigen alleles in JSON format. Check Data/ectyper_database.json for more information ``` ## Configuration and fine-tunning parameters `ECTyper` requires minimum options to run (`-i` input directory or file) but allows for extensive configuration to accomodate wide variaty of typing scenarios | Parameter| Explanation | Usage scenario | |----------|:----------------------------------------------------------------:|:----------------------------------------------------------------------------------: | `--maxdirdepth`| Maximum number of directory levels to use in a directory for input file search| Given a directory path use this number of levels (default 0 levels) to go down the specified directory path in search for files. Useful is a directory contains multiple sub-directories | | `-c` | The number of cores to run on | Use multiple cores to run multiple samples and subtools on. Especially useful if a directory contains multiple files or working with raw reads in FASTQ format | | `-opid` | Minimum `%identity` threshold just for O antigen match| Poor coverage of O antigen genes or for exploratory work (recommended value is 95) | | `-opcov` | Minimum `%covereage` threshold for a valid match against reference O antigen alleles | Poor coverage of O antigen genes and a user wants to get O antigen call regardless (recommend value is 90)| | `-hpid` | Minimum `%identity` threshold just for H antigen match| Poor coverage of O antigen genes or for exploratory work (recommend value is 95) | | `-hpcov` | Minimum `%covereage` threshold for a valid match against reference H antigen alleles | Poor coverage of O antigen genes and a user wants to get O antigen call regardless (recommend value is 50)| |`--verify`| Verify species of the input and run QC module providing information on the reliability of the result and any typing issues | User not sure if sample is E.coli and wants to obtain if serotype prediction is of sufficient quality for reporting purposes and predicted species is *E.coli*. Note that pathotyping and Shiga toxin module will not run if `--verify` is specified and species is other than *E.coli*| | `-r` | Specify custom MASH sketch of reference genomes that will be used for species inference | User has a new assembled genome that is not available in NCBI RefSeq database. Make sure to add metadata to `assembly_summary_refseq.txt` and provide custom accession number that start with `GCF_` prefix| |`--dbpath`| Provide custom appended database of O and H antigen reference alleles in JSON format following structure and field names as default database `ectyper_alleles_db.json` | User wants to add new alleles to the alleles database to improve typing performance | | `--pathotype` | Perform DEC pathotype and Shiga toxin subtyping prediction on a sample | Predict DEC pathotype using the key diagnostic pathotype markers. Results are only valid for E.coli samples. Also performs Shiga toxin subtyping if `stx1` or `stx2` genes are present | | `-pathpid` | Minimum `%identity` threshold for pathotype and Shiga toxin subtyping results filtering | All gene hits from pathotype database also containing `stx` genes will use this minimum cutoff to filter results. The default value is minimum 90% identity | | `-pathcov` | Minimum `%coverage` threshold for pathotype and Shiga toxin subtyping results filtering | All gene hits from pathotype database also containing `stx` genes will use this minimum cutoff to filter results. The default value is minimum 50% coverage | # Data Input Both raw and assembled reads are accepted in FASTA and FASTQ formats from any sequencing platform. The tool was designed for single sample inputs, but was shown to work on multi-taxa metagenomic raw reads FASTQ inputs. # Data Output The output of the tool is stored in text files with the main report stored in `output.tsv` tab-delimited text file. The BLASTN hits of the O and H antigen database are stored in `blastn_output_alleles.txt` tab-delimited file. The log messages are stored in `ectyper.log` text file ``` {out folder name} \u251c\u2500\u2500 blastn_output_alleles.txt \u251c\u2500\u2500 ectyper.log \u2514\u2500\u2500 output.tsv ``` ## Main submodules descriptions ### Species identification module ECTyper performs species identification by selecting the closest reference genome to the query input (i.e.having the smallest MASH distance) from the custom made MASH sketch currently represented by the 119,980 reference genomes with assigned taxonomy information. The enhanced ECTyper [species ID sketch](#databases) is both based on [The Genome Taxonomy Database (GTDB)](https://gtdb.ecogenomic.org/) release 214 (covering all known bacteria and archaea domains) and manually curated and selected *Escherichia* and *Shigella* genera reference genomes from the [EnteroBase](https://enterobase.warwick.ac.uk/species/index/ecoli) and [GenBank/RefSeq NCBI](https://www.ncbi.nlm.nih.gov/datasets/genome/) public databases. The `comment` field of the MASH sketch uses taxonomic path from domain to species level to meet the [MikroKondo MASH sketch guidelines](https://github.com/phac-nml/mikrokondo?tab=readme-ov-file#step-4-further-resources-to-download). For example, taxonomic path for *E.coli* is formatted as such `\"d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales;f__Enterobacteriaceae;g__Escherichia;s__Escherichia coli\"`. The sketch was generated using 1000 hashes per sketch (`-s 1000`) and k-mer size of 21 (`-k 21`) that are the default `mash sketch` parameters. The *Escherichia* and *Shigella* genera genomes sourced from EnteroBase accessed on 2023-09-07 were assembled using the [`shovill v1.1.0`](https://github.com/tseemann/shovill) assembler run on raw reads downloaded from the [NCBI SRA](https://www.ncbi.nlm.nih.gov/sra) via the corresponding SRA accession number. The *Escherichia* and *Shigella* RefSeq NCBI genomes were downloaded from [NCBI FTP](https://ftp.ncbi.nlm.nih.gov/) by accessing metadata from the [`assembly_summary_refseq.txt`](https://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/assembly_summary_refseq.txt) file, filtering the `organism_name` field using all known *Escherichia* and *Shigella* genera species. The reported species for the EnteroBase and RefSeq sourced genomes were verified by running the [`ShigEiFinder v1.3.5`](https://github.com/LanLab/ShigEiFinder) and checking the `SEROTYPE` field. Samples with values `SB13` (*Shigella boydii type 13*) and `Unknown` were rejected. For each *Escherichia* and *Shigella* species selected genomes a distance matrix was calculated by running [`mash triangle`](https://manpages.debian.org/testing/mash/mash-triangle.1.en.html) followed by the agglomerative hierarchical clustering using average linkage method via the [`linkage()` function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) form SciPy package. Next flat clusters were formed on the previously calculated hierarchical clustering object using the SciPy [`fcluster()` function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html) with `distance` as the clusters forming criterion. The flat clusters with 2 or more members (i.e. genomes) were selected and clusters with a single member (i.e. singletons) were discarded as potential outliers or noise. Finally the cluster centroids (i.e. representative cluster genomes) were defined by the partition around medoids algorithm (PAM) via the [`fasterpam()`](https://python-kmedoids.readthedocs.io/en/latest/) function from the `kmedoids` package. These cluster centroids formed the final list of *Escherichia* and *Shigella* genomes selected for the original GTDB sketch enhancement. Currently the MASH ECTyper species sketch contains 34,736 *Escherichia* and 4,627 *Shigella* genomes. The ECTyper species identification module performance was tested and validated against the highly curated 493 genomes with species and cgMLST clustering data covering *Shigella flexneri*, *Shigella dysenteriae*, *Shigella boydii*, *Shigella sonnei* and *E.coli* species described by the [Iman Yassine et al. 2022](https://www.nature.com/articles/s41467-022-28121-1) ### Serotyping module Independent of the input type (FASTQ or FASTA) [the reference database of O and H antigen alleles](./ectyper/Data/ectyper_alleles_db.json) is being scanned. The best matching alleles hits for O antigen (`wzx`, `wzy`, `wzm`, `wzt`) and H antigen (mostly represented by `fliC` in addition to `flkA`, `fllA` and `flmA`) genes are ranked by maximizing BOTH `%identity` and `%coverage` values via the allele `gene score=(%identity*%coverage)/10000` using the [BLASTN](https://blast.ncbi.nlm.nih.gov/Blast.cgi) default run parameters. For O and H antigen the highest scoring antigen is reported. Since some O antigens display very high level of sequence similarity represented by the 16 high similarity O groups delineated by the [Atsushi Iguchi et al. 2015](https://academic.oup.com/dnaresearch/article/22/1/101/442161?login=false), the mixed O antigen calls are possible separated by the `\\\\` symbol such as the group 9 that will be reported as `O17/O44/O73/O77/O106` mixed O serotype call. To account for any sequencing and other errors, if predicted O antigen is a member of the high similarity group, all O antigens in the group will be reported resulting in a mixed O antigen final call. The H antigens are better separated and are always reported as a single', 'antigen call. For more serotyping details and benchmarking results please see the ECTyper v1.0.0 publication in the [Citation](#citation) section. ### Pathotyping module Most of the E.coli samples are non-pathogenic, but some present public health risk and could lead from mild to sever health conditions. ECTyper currently supports typing of the 7 diarrheagenic *Escherichia coli* (DEC) pathotypes: DAEC, EAEC, EHEC, EIEC, EPEC, ETEC and STEC. Please note that the EHEC pathotype is being reported as `EHEC-STEC` as it is a STEC subtype that could cause bloody dysentery and hemolytic uremic syndrome (HUS). This module uses highly curated database of the diagnostic pathotype markers listed in the [pathotype and toxin typing database](./ectyper/Data/ectyper_patho_stx_toxin_typing_database.json) in JSON format (see [Databases](#databases) section). The database was assembled by curating the existing literature and tools. Each database entry contains diagnostic marker accession number, nucleotide sequence and its length, gene symbol, source and other information in the following format: ``` { \"accession\": \"QGG66503.1\", \"description\": \"intimin\", \"dnasequence\": \"ATGATTACTCATGGTTTTT...\", \"gene\": \"eae\", \"protsequence\": \"MITHGFYARTRHKHKLK...\", \"source\": \"https://github.com/B-UMMI/patho_typing/\", \"subtype\": \"-\", \"id\": \"28\", \"length_dnasequence\": \"2820\" } ``` Importantly the pathotype database under the `pathotypes` key contains the set of pathotyping classification gene-based rules that are used to predict a given E.coli pathotype from BLASTN output using the following format below. ``` \"EPEC\": { \"name\": \"Enteropathogenic Escherichia coli\", \"rules\": { \"16\": { \"genes\": [ \"eae\", \"!stx1\", \"!stx2\" ], \"publication\": \"PMID:11403406\" } } } ``` The `!` in front of the gene symbol (e.g., `!stx2` ) indicates the negation that is a given gene must NOT BE PRESENT in order to satisfy a given rule. Each pathotype classification rule might have one or more genes listed under the `genes` key. Each rule also has a corresponding publication reference under the `publication` key as an additional evidence and rule validity support. Each rule is tested for presence or absence of genes listed under the `genes` rule key. If all presence or absence conditions (i.e. the genes marked by the `!`) are met a given rule is considered to be valid and pathotype assigned. Thus a given sample might have several rules that would concurrently apply that could lead to mixed pathotype prediction separated via the `/` symbol such as `ETEC/STEC`. ### Shiga toxin typing module The Shiga toxin subtyping module supports typing of the *`stx1`* and *`stx2`* gene subtypes that is relevant both for epidemiological and risk assessment purposes (e.g., disease severity). This module also heavily relies on the [pathotype and toxin typing database](./ectyper/Data/ectyper_patho_stx_toxin_typing_database.json) (see [Databases](#databases) section). Currently the database supports * 4 **`stx1`** subtypes: *`stx1a`*, *`stx1c`*, *`stx1d`* and `stx1e` * 15 **`stx2`** subtypes: *`stx2a`*, *`stx2b`*, *`stx2c`*, *`stx2d`*, *`stx2e`*, *`stx2f`*, *`stx2g`* ,*`stx2h`*, *`stx2i`*, *`stx2j`*, *`stx2k`*,*`stx2l`*, *`stx2m`*, *`stx2n`*, *`stx2o`*. The input sequences are queried against the *`stx1`* and *`stx2`* markers via BLASTN and top hits are being reported separated by the `;` symbol. The module supports the multi-copy `stx` gene presence by taking into account the genomic `stx` location attributes for each `stx` subtype (i.e. gene coordinates, contig location, overlap with other `stx` hits). The multi-copy `stx` gene reporting is not exhaustive (not all hits are being reported). That is if multiple `stx` hits are found in the input, the highest quality hit(s) per each non-overlapping `stx` gene range is being reported (i.e. single or multiple top hits are possible with the highest identical `bitscore` value as some hits could not be resolved due to sequence truncation). For example, if several `stx` allele hits have identical `bitscore` in a given `stx` gene range, all such hits are being reported. Note that the `StxSubtypes` field lists only UNIQUE `stx` subtypes for the entire input sample such as `stx2e;stx2k` even if their genomic locations overlap or are identical due to truncated incomplete `stx` allele signatures. The `StxContigNames` and `StxCoordinates` lists all contig names and corresponding genomic coordinates for each listed `stx` type in the `StxSubtypes` field according to the alphabetical order. This allows to easily spot `stx` subtypes with the same genomic coordinates. Finally, these fields allow to better understand `stx` alleles context/function and spot truncated alleles while providing genomic location context. ### Quality Control (QC) module To provide an easier interpretation of the results and typing metrics, following QC codes were developed. These codes allow to quickly filter \"reportable\" and \"non-reportable\" samples. The QC module is tightly linked to ECTyper allele database, specifically, `MinPident` and `MinPcov` fields. For each reference allele minimum `%identity` and `%coverage` values were determined as a function of potential \"cross-talk\" between antigens (i.e. multiple potential antigen calls at a given setting). The QC module covers the following serotyping scenarios. More scenarios might be added in future versions depending on user needs. | QC flag | Explanation | |------------------|:-----------------------------------------------------------------| |PASS (REPORTABLE) |Both O and H antigen alleles meet min `%identity` or `%coverage` thresholds (ensuring no antigen cross-talk) and single antigen predicted for O and H| |FAIL (-:- TYPING) |Sample is E.coli and O and H antigens are not typed. Serotype: -:- | |WARNING MIXED O-TYPE|A mixed O antigen call is predicted requiring wet-lab confirmation | |WARNING (WRONG SPECIES)| A sample is non-E.coli (e.g. *E.albertii*, *Shigella*, etc.) based on RefSeq assemblies| |WARNING (-:H TYPING)| A sample is E.coli and O antigen is not predicted (e.g. -:H18)| |WARNING (O:- TYPING)| A sample is E.coli and H antigen is not predicted (e.g. O17:-)| |WARNING (O NON-REPORT)|O antigen alleles do not meet min %identity or %coverage thresholds| |WARNING (H NON-REPORT)|H antigen alleles do not meet min %id or %cov thresholds| |WARNING (O and H NON-REPORT)| Both O and H antigen alleles do not meet min %identity or %coverage thresholds| ## Report format (`output.tsv`) `ECTyper` capitalizes on a concise minimum output coupled to easy results interpretation and reporting. `ECTyper v1.0` serotyping results are available in a tab-delimited `output.tsv` file consisting of the 16 columns listed below: 1. **Name**: Sample name (usually a unique identifier) 2. **Species**: the species column provides valuable species identification information in case of inadvertent sample contamination or mislabelling events 3. **O-type**: O antigen 4. **H-type**: H antigen 5. **Serotype**: Predicted O and H antigen(s) 6. **QC**: The Quality Control value summarizing the overall quality of prediction 7. **Evidence**: How many alleles in total used to both call O and H antigens 8. **GeneScores**: ECTyper O and H antigen gene scores in 0 to 1 range 9. **AllelesKeys**: Best matching `ECTyper` database allele keys used to call the serotype 10. **GeneIdentities(%)**: `%identity` values of the query alleles 11. **GeneCoverages(%)**: `%coverage` values of the query alleles 12. **GeneContigNames**: the contig names where the query alleles were found 13. **GeneRanges**: genomic coordinates of the query alleles 14. **GeneLengths**: allele lengths of the query alleles 15. **Database**: database release version and date 16. **Warnings**: any additional warnings linked to the quality control status or any other error message(s). <br> Selected columns from the `ECTyper` typical serotyping report |Name|Species|Serotype|Evidence|QC|GeneScores|AlleleKeys|GeneIdentities(%) | GeneCoverages(%) | GeneContigNames| GeneRanges | GeneLengths | Database | Warnings| |------|:------|:-------|:--|:---------|:------------|:-----|:-----|:----|:----|:----|:----|:---|:--| |15-520|Escherichia coli|O174:H21|Based on 3 allele(s)|PASS (REPORTABLE)|wzx:1; wzy:1; fliC:1;|O104-5-wzx-origin;O104-13-wzy;H7-6-fliC-origin;|100;100;100;| 100;100;100;|contig00049;contig00001;contig00019;| 22302-23492;178-1290;6507-8264;| 1191;1113;1758;| v1.0 (2020-05-07) | - | EC20151709|Escherichia coli|O157:H43|Based on 3 allele(s)|PASS (REPORTABLE)|wzx:1;wzy:0.999;fliC:1|O157-5-wzx-origin;O157-9-wzy-origin;H43-1-fliC-origin;|100;99.916;99.934; | 100;100;100; | contig00002;contig00002;contig00003; | 62558-63949;64651-65835;59962-61467; | 1392;1185;1506; |v1.0 (2020-05-07) | - | Selected columns from the `ECTyper` typical pathotyping and shiga toxin subtyping report | Name | Species | SpeciesMashRatio | SpeciesMashTopID | Serotype | Pathotype | PathotypeGenes | PathotypeGeneNames | PathotypeAccessions | PathotypeIdentities(%) | PathotypeCoverages(%) | PathotypeRuleIDs | PathotypeGeneCounts | StxSubtypes | StxAccessions | StxIdentities(%) | StxCoverages(%) | StxLengths | StxContigNames | StxCoordinates | |------------|------------------|------------------|------------------|----------|-----------|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------|---------------------------|-----------------------|---------------------------|-----------------------------|-------------|-------------------|------------------|-----------------|------------|-------------------------|-----------------------| | SRR7947260 | Escherichia coli | 992/1000 | GCF_005046225.1 | O109:H16 | ETEC/STEC | ehxA,hlyE,sta1,stx2 | ehxA: enterohemolysin A,hlyE: hemolysin E chromosomal,sta1: heat-stable enterotoxin ST,stx2: Shiga toxin 2 subtype a complete sequence (subunits A and B) | EF204927.1;AVXX01000010.1;AJ555214;AB030484 | 99.619;97.807;95.89;100.0 | 70;100;100;100 | 20:ETEC:sta1;27:STEC:stx2 | ETEC:1 (sta1);STEC:1 (stx2) | stx2a;stx2g | AB030484;AY286000 | 100.0;99.758 | 100;100 | 1241;1242 | contig00064;contig00074 | 3723-4963;14725-15966 | ## FAQ **Does ECTyper can be run on multiple samples in a directory?** ECTyper proves flexible ways to specify inputs located in different locations. One can provide multiple paths to several directories separated by space. In addition, one can specify file type to look for in a given directory(ies). Note that paths that contain a star `*` symbol would only look for files in specified directory and would not look in subdirectories. For example, - Process all files in `folder1` and `folder2` directories and file `sample.fasta` located in `folder3` `ectyper -i folder1/ folder2/ folder3/sample.fasta -o ectyper_results` - Process all fasta files in `folder1` and all fastq files in `folder2`. All sub-directories in those 2 folders will be ignored. To process those sub-folders either specify path to them or provide paths to directories without the `*` wildcard symbol. `ectyper -i folder1/*.fasta folder2/*.fastq` **Why ECTyper sometimes provides serotype results separated by forward slash / for O-antigen** Some O-antigens display very high degree of homology and are very hard to discern even using wet-lab agglutination assays. Even using both `wzx` and `wzy` genes it is not possible to reliably resolve those O-antigens. The 16 high similarity groups were identified by [Joensen, Katrine G., et al.](https://journals.asm.org/doi/full/10.1128/jcm.00008-15). Thus, if a given O-antigen is a member of any of those high similarity groups, all potential O-antigens are reported separated by `/` such as group 9 reported as `O17/O44/O73/O77/O106`. # Availability |Resource|Description|Type| |--------|:----------|:---| |[PyPI](https://pypi.org/project/ectyper/)| PyPI package that could be installed via `pip` utility|Terminal| |[Conda](https://anaconda.org/bioconda/ectyper) | Conda package available from BioConda channel|Terminal| |[Docker](https://hub.docker.com/r/kbessonov/ectyper)| Images containing completely initialized ECTyper with all dependencies |Terminal| |[Singluarity](https://biocontainers.pro/tools/ectyper) | Images containing completely initialized ECTyper with all dependencies |Terminal| |[GitHub](https://github.com/phac-nml/ecoli_serotyping) | Install source code as any Python package|Terminal| |[Galaxy ToolShed](https://toolshed.g2.bx.psu.edu/view/nml/ectyper/)| Galaxy wrapper available for installation on a private/public instance|Web-based| |[Galaxy Europe](https://usegalaxy.eu/root?tool_id=ectyper)| Galaxy public server to execute your analysis from anywhere|Web-based| |[IRIDA plugin](https://github.com/phac-nml/irida-plugin-ectyper)| IRIDA instances could easily install additional pipeline|Web-based| # Legal and Compliance Information Copyright Government of Canada 2024 Written by: National Microbiology Laboratory, Public Health Agency of Canada Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this work except in compliance with the License. You may obtain a copy of the License at: [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0) Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.']```"}, {"role": "user", "content": "Content from https://github.com/phac-nml/ecoli_serotyping:\n```\nRepository metadata:\n{'name': 'ecoli_serotyping', 'label': ['ecoli_serotyping'], 'description': [{'term': ' In silico prediction of E. coli serotype, pathotype and shiga toxin subtyping', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://usegalaxy.eu/?tool_id=ectyper', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['2.0.0', '1.0.0', '0.9.1', '0.9.0', 'v0.8.1', 'v0.8.0', 'v0.5.4', 'v0.5.3', 'v0.5.1', 'v0.5.0', 'v0.4.1', 'v0.3.6', 'v0.1', 'v0.1-alpha'], 'license': [{'term': {'name': 'Apache License 2.0', 'url': 'http://choosealicense.com/licenses/apache-2.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/phac-nml/ecoli_serotyping', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Kirill Bessonov', 'type': 'person', 'email': 'kbessonov@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Josh Chorlton', 'type': 'person', 'email': 'josh@bugseq.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Chad Laing', 'type': 'person', 'email': 'chadlaing@inoutbox.com', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'changelog', 'url': 'https://github.com/phac-nml/ecoli_serotyping/blob/main/CHANGELOG.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/phac-nml/ecoli_serotyping/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bioconductor-mygene/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bioconductor-mygene/lib/1.38.0\",\n    \"name\": \"bioconductor-mygene\",\n    \"description\": [\n      \"MyGene.Info_ provides simple-to-use REST web services to query/retrieve gene annotation data. It's designed with simplicity and performance emphasized. *mygene*, is an easy-to-use R wrapper to access MyGene.Info_ services.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/3.18/bioc/html/mygene.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Artistic-2.0\",\n        \"url\": \"https://spdx.org/licenses/Artistic-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2dc3cac7ee6cb578912bf\"\n      }\n    ],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/mygene/lib/3.2.2\",\n    \"name\": \"mygene\",\n    \"description\": [\n      \"Python Client for MyGene.Info services.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/suLab/mygene.py\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [],\n    \"license\": [\n      {\n        \"name\": \"BSD License\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/suLab/mygene.py:\n```\nREADME content:\n['.. image:: https://badge.fury.io/py/mygene.svg :target: https://pypi.python.org/pypi/mygene .. image:: https://img.shields.io/pypi/pyversions/mygene.svg :target: https://pypi.python.org/pypi/mygene .. image:: https://img.shields.io/pypi/format/mygene.svg :target: https://pypi.python.org/pypi/mygene .. image:: https://img.shields.io/pypi/status/mygene.svg :target: https://pypi.python.org/pypi/mygene Intro ===== MyGene.Info_ provides simple-to-use REST web services to query/retrieve gene annotation data. It\\'s designed with simplicity and performance emphasized. *mygene*, is an easy-to-use Python wrapper to access MyGene.Info_ services. .. _MyGene.Info: http://mygene.info .. _requests: http://docs.python-requests.org/en/latest/ Requirements ============ python >=2.6 (including python3) requests_ (install using \"pip install requests\") Optional dependencies ====================== `pandas <http://pandas.pydata.org>`_ (install using \"pip install pandas\") is required for returning a list of gene objects as `DataFrame <http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe>`_. Installation ============= Option 1 pip install mygene Option 2 download/extract the source code and run:: python setup.py install Option 3 install the latest code directly from the repository:: pip install -e git+https://github.com/biothings/mygene.py#egg=mygene Version history =============== `CHANGES.txt <https://raw.githubusercontent.com/SuLab/mygene.py/master/CHANGES.txt>`_ Tutorial ========= * `ID mapping using mygene module in Python <http://nbviewer.ipython.org/6771106>`_ Documentation ============= http://mygene-py.readthedocs.org/ Usage ===== .. code-block:: python In [1]: import mygene In [2]: mg = mygene.MyGeneInfo() In [3]: mg.getgene(1017) Out[3]: {\\'_id\\': \\'1017\\', \\'entrezgene\\': 1017, \\'name\\': \\'cyclin-dependent kinase 2\\', \\'symbol\\': \\'CDK2\\', \\'taxid\\': 9606} In [4]: mg.getgene(1017, \\'name,symbol,refseq\\') Out[4]: {\\'_id\\': \\'1017\\', \\'name\\': \\'cyclin-dependent kinase 2\\', \\'refseq\\': {\\'genomic\\': [\\'AC_000144.1\\', \\'NC_000012.11\\', \\'NG_028086.1\\', \\'NT_029419.12\\', \\'NW_001838059.1\\'], \\'protein\\': [\\'NP_001789.2\\', \\'NP_439892.2\\'], \\'rna\\': [\\'NM_001798.3\\', \\'NM_052827.2\\']}, \\'symbol\\': \\'CDK2\\'} In [5]: mg.getgene(1017, \\'name,symbol,refseq.rna\\') Out[5]: {\\'_id\\': \\'1017\\', \\'name\\': \\'cyclin-dependent kinase 2\\', \\'refseq\\': {\\'rna\\': [\\'NM_001798\\', \\'NM_052827\\']}, \\'symbol\\': \\'CDK2\\'} In [6]: mg.getgenes([1017,1018,\\'ENSG00000148795\\']) Out[6]: [{\\'_id\\': \\'1017\\', \\'entrezgene\\': 1017, \\'name\\': \\'cyclin-dependent kinase 2\\', \\'query\\': \\'1017\\', \\'symbol\\': \\'CDK2\\', \\'taxid\\': 9606}, {\\'_id\\': \\'1018\\', \\'entrezgene\\': 1018, \\'name\\': \\'cyclin-dependent kinase 3\\', \\'query\\': \\'1018\\', \\'symbol\\': \\'CDK3\\', \\'taxid\\': 9606}, {\\'_id\\': \\'1586\\', \\'entrezgene\\': 1586, \\'name\\': \\'cytochrome P450, family 17, subfamily A, polypeptide 1\\', \\'query\\': \\'ENSG00000148795\\', \\'symbol\\': \\'CYP17A1\\', \\'taxid\\': 9606}] In [7]: mg.getgenes([1017,1018,\\'ENSG00000148795\\'], as_dataframe=True) Out[7]: _id entrezgene \\\\ query 1017 1017 1017 1018 1018 1018 ENSG00000148795 1586 1586 name symbol \\\\ query 1017 cyclin-dependent kinase 2 CDK2 1018 cyclin-dependent kinase 3 CDK3 ENSG00000148795 cytochrome P450, family 17, subfamily A, polyp... CYP17A1 taxid query 1017 9606 1018 9606 ENSG00000148795 9606 [3 rows x 5 columns] In [8]: mg.query(\\'cdk2\\', size=5) Out[8]: {\\'hits\\': [{\\'_id\\': \\'1017\\', \\'_score\\': 373.24667, \\'entrezgene\\': 1017, \\'name\\': \\'cyclin-dependent kinase 2\\', \\'symbol\\': \\'CDK2\\', \\'taxid\\': 9606}, {\\'_id\\': \\'12566\\', \\'_score\\': 353.90176, \\'entrezgene\\': 12566, \\'name\\': \\'cyclin-dependent kinase 2\\', \\'symbol\\': \\'Cdk2\\', \\'taxid\\': 10090}, {\\'_id\\': \\'362817\\', \\'_score\\': 264.88477, \\'entrezgene\\': 362817, \\'name\\': \\'cyclin dependent kinase 2\\', \\'symbol\\': \\'Cdk2\\', \\'taxid\\': 10116}, {\\'_id\\': \\'52004\\', \\'_score\\': 21.221401, \\'entrezgene\\': 52004, \\'name\\': \\'CDK2-associated protein 2\\', \\'symbol\\': \\'Cdk2ap2\\', \\'taxid\\': 10090}, {\\'_id\\': \\'143384\\', \\'_score\\': 18.617256, \\'entrezgene\\': 143384, \\'name\\': \\'CDK2-associated, cullin domain 1\\', \\'symbol\\': \\'CACUL1\\', \\'taxid\\': 9606}], \\'max_score\\': 373.24667, \\'took\\': 10, \\'total\\': 28} In [9]: mg.query(\\'reporter:1000_at\\') Out[9]: {\\'hits\\': [{\\'_id\\': \\'5595\\', \\'_score\\': 11.163337, \\'entrezgene\\': 5595, \\'name\\': \\'mitogen-activated protein kinase 3\\', \\'symbol\\': \\'MAPK3\\', \\'taxid\\': 9606}], \\'max_score\\': 11.163337, \\'took\\': 6, \\'total\\': 1} In [10]: mg.query(\\'symbol:cdk2\\', species=\\'human\\') Out[10]: {\\'hits\\': [{\\'_id\\': \\'1017\\', \\'_score\\': 84.17707, \\'entrezgene\\': 1017, \\'name\\': \\'cyclin-dependent kinase 2\\', \\'symbol\\': \\'CDK2\\', \\'taxid\\': 9606}], \\'max_score\\': 84.17707, \\'took\\': 27, \\'total\\': 1} In [11]: mg.querymany([1017, \\'695\\'], scopes=\\'entrezgene\\', species=\\'human\\') Finished. Out[11]: [{\\'_id\\': \\'1017\\', \\'entrezgene\\': 1017, \\'name\\': \\'cyclin-dependent kinase 2\\', \\'query\\': \\'1017\\', \\'symbol\\': \\'CDK2\\', \\'taxid\\': 9606}, {\\'_id\\': \\'695\\', \\'entrezgene\\': 695, \\'name\\': \\'Bruton agammaglobulinemia tyrosine kinase\\', \\'query\\': \\'695\\', \\'symbol\\': \\'BTK\\', \\'taxid\\': 9606}] In [12]: mg.querymany([1017, \\'695\\'], scopes=\\'entrezgene\\', species=9606) Finished. Out[12]: [{\\'_id\\': \\'1017\\', \\'entrezgene\\': 1017, \\'name\\': \\'cyclin-dependent kinase 2\\', \\'query\\': \\'1017\\', \\'symbol\\': \\'CDK2\\', \\'taxid\\': 9606}, {\\'_id\\': \\'695\\', \\'entrezgene\\': 695, \\'name\\': \\'Bruton agammaglobulinemia tyrosine kinase\\', \\'query\\': \\'695\\', \\'symbol\\': \\'BTK\\', \\'taxid\\': 9606}] In [13]: mg.querymany([1017, \\'695\\'], scopes=\\'entrezgene\\', species=9606, as_dataframe=True) Finished. Out[13]: _id entrezgene name symbol \\\\ query 1017 1017 1017 cyclin-dependent kinase 2 CDK2 695 695 695 Bruton agammaglobulinemia tyrosine kinase BTK taxid query 1017 9606 695 9606 [2 rows x 5 columns] In [14]: mg.querymany([1017, \\'695\\', \\'NA_TEST\\'], scopes=\\'entrezgene\\', species=\\'human\\') Finished. Out[14]: [{\\'_id\\': \\'1017\\', \\'entrezgene\\': 1017, \\'name\\': \\'cyclin-dependent kinase 2\\', \\'query\\': \\'1017\\', \\'symbol\\': \\'CDK2\\', \\'taxid\\': 9606}, {\\'_id\\': \\'695\\', \\'entrezgene\\': 695, \\'name\\': \\'Bruton agammaglobulinemia tyrosine kinase\\', \\'query\\': \\'695\\', \\'symbol\\': \\'BTK\\', \\'taxid\\': 9606}, {\\'notfound\\': True, \\'query\\': \\'NA_TEST\\'}] # query all human kinases using fetch_all parameter: In [15]: kinases = mg.query(\\'name:kinase\\', species=\\'human\\', fetch_all=True) In [16]: kinases Out [16]\" <generator object _fetch_all at 0x7fec027d2eb0> # kinases is a Python generator, now you can loop through it to get all 1073 hits: In [16]: for gene in kinases: ....: print gene[\\'_id\\'], gene[\\'symbol\\'] Out [16]: <output omitted here> Contact ======== Drop us any feedback at: help@mygene.info or on twitter `@mygeneinfo <https://twitter.com/mygeneinfo>`_.']```"}, {"role": "user", "content": "Content from https://github.com/suLab/mygene.py:\n```\nRepository metadata:\n{'name': 'mygene.py', 'label': ['mygene.py'], 'description': [{'term': ' mygene is an easy-to-use Python wrapper to access MyGene.Info services.', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/SuLab/mygene.py', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Chunlei Wu', 'type': 'person', 'email': 'anewgene@yahoo.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Chunlei Wu', 'type': 'person', 'email': 'newgene@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Cyrus Afrasiabi', 'type': 'person', 'email': 'cyrus.afrasiabi@gmail.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'cyrus0824', 'type': 'person', 'email': 'cyrus0824@users.noreply.github.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'cyrus0824', 'type': 'person', 'email': 'cyrus0824@localhost', 'maintainer': False}, 'id': 4}, {'term': {'name': 'cyrs0824', 'type': 'person', 'email': 'cyrs0824@localhost', 'maintainer': False}, 'id': 5}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/suLab/mygene.py/blob/main/CHANGES.txt'}, 'id': 0}, {'term': {'type': 'license', 'url': 'https://github.com/suLab/mygene.py/blob/main/LICENSE.txt'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/3.18/bioc/html/mygene.html:\n```\nContent:\n['This package is for version 3.18 of Bioconductor; for the stable, up-to-date release version, see [mygene](/packages/mygene/) . ## Access MyGene.Info_ services **Bioconductor version:** 3.18 MyGene.Info_ provides simple-to-use REST web services to query/retrieve gene annotation data. It\\'s designed with simplicity and performance emphasized. *mygene*, is an easy-to-use R wrapper to access MyGene.Info_ services. **Author:** Adam Mark, Ryan Thompson, Cyrus Afrasiabi, Chunlei Wu **Maintainer:** Adam Mark, Cyrus Afrasiabi, Chunlei Wu <cwu at scripps.edu> **Citation (from within R, entercitation(\"mygene\")):** ### Installation To install this package, start R (version \"4.3\") and enter: if (! require ( \"BiocManager\" , quietly = TRUE )) install. packages ( \"BiocManager\" ) BiocManager :: install ( \"mygene\" ) For older versions of R, please refer to the appropriate [Bioconductor release](/about/release-announcements/) . ### Documentation To view documentation for the version of this package installed in your system, start R and enter: browseVignettes (\"mygene\") ### Details biocViews [Annotation](../../BiocViews.html#___Annotation) , [Software](../../BiocViews.html#___Software) Version 1.38.0 In Bioconductor since BioC 3.0 (R-3.1) (9.5 years) License Artistic-2.0 Depends R (>= 3.2.1), [GenomicFeatures](../../bioc/html/GenomicFeatures.html) Imports httr (>= 0.3), jsonlite (>= 0.9.7), [S4Vectors](../../bioc/html/S4Vectors.html) , Hmisc, sqldf, plyr System Requirements URL <> See More ### Package Archives Follow [Installation](/install/#install-bioconductor-packages) instructions to use this package in your R session.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"pyrpipe/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/pyrpipe/cmd/0.0.5\",\n    \"name\": \"pyrpipe\",\n    \"description\": [\n      \"Pyrpipe is a lightweight python package for RNA-Seq workflows.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/urmi-21/pyrpipe\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/urmi-21/pyrpipe\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": \"https://spdx.org/licenses/MIT.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/pyrpipe/undefined/None\",\n    \"name\": \"pyrpipe\",\n    \"description\": [\n      \"A python package for RNA-Seq workflows.\\n\\npyrpipe: python rna-seq pipeliner.\\n\\nExample to identify orphan transcripts in maize by pyrpipe.\\n\\nA python package for RNA-Seq workflows \u2014 pyrpipe 0.0.1 documentation.\\n\\npyrpipe (Pronounced as \\\"pyre-pipe\\\") is a python package to easily develop bioinformatic or any other computational pipelines in pure python. pyrpipe provides an easy-to-use framework for importing any UNIX command in python. pyrpipe comes with specialized classes and functions to easily code RNA-Seq processing workflows.\\n\\npyrpipe (Pronounced as \\\"pyre-pipe\\\") is a python package to easily develop RNA-Seq analyses workflow by integrating popular RNA-Seq analysis programs in an object oriented manner. pyrpipe can be used on local computers or on HPC environments to manage analysis of RNA-Seq data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://pyrpipe.rtfd.io/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Urminder Singh\",\n        \"email\": \"usingh@iastate.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Eve Syrkin Wurtele\",\n        \"email\": \"mash@iastate.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8328d10e2f3aa0b3bb227\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/urmi-21/pyrpipe:\n```\nREADME content:\n['[![Build Status](https://travis-ci.org/urmi-21/pyrpipe.svg?branch=master)](https://travis-ci.org/urmi-21/pyrpipe) [![Coverage Status](https://coveralls.io/repos/github/urmi-21/pyrpipe/badge.svg?branch=master)](https://coveralls.io/github/urmi-21/pyrpipe?branch=master) [![Documentation Status](https://readthedocs.org/projects/pyrpipe/badge/?version=latest)](https://pyrpipe.readthedocs.io/en/latest/?badge=latest) ![PyPI](https://img.shields.io/pypi/v/pyrpipe?style=flat-square) [![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/pyrpipe/README.html) ![PyPI - License](https://img.shields.io/pypi/l/pyrpipe) [![publication](https://img.shields.io/badge/publication-NARGAB-blue)](https://academic.oup.com/nargab/article/3/2/lqab049/6290623) # pyrpipe: python rna-seq pipeliner ## Introduction pyrpipe (Pronounced as \"pyre-pipe\") is a python package to easily develop bioinformatic or any other computational pipelines in pure python. pyrpipe provides an easy-to-use framework for importing any UNIX command in python. pyrpipe comes with specialized classes and functions to easily code RNA-Seq processing workflows. Pipelines in pyrpipe can be created and extended by integrating third-party tools, executable scripts, or python libraries in an object oriented manner. Read the paper [here](https://academic.oup.com/nargab/article/3/2/lqab049/6290623) Read the docs [here](https://pyrpipe.readthedocs.io/en/latest/?badge=latest) **NOTE:** Due to change in API designs, pyrpipe version 0.0.5 and above is not compatible with lower versions. All the tutorials and documentation have been updated to reflect v0.0.5. ## What it does Allows fast and easy development of bioinformatics pipelines in python by providing * a high level api to popular RNA-Seq processing tools -- downloading, trimming, alignment, quantificantion and assembly * optimizes program parameters based on the data * a general framework to execute any linux command from python * comprehensive logging features to log all the commands, output and their return status * report generating features for easy sharing, reproducing, benchmarking and debugging ## Key Features (version 0.0.5) * Import any UNIX executable command/tool in python * Dry-run feature to check dependencies and commands before execution * Flexible and robust handling of options and arguments (both Linux and Java style options) * Auto load command options from .yaml files * Easily override threads and memory options using global values * Extensive logging for all the commands * Automatically verify Integrity of output targets * Resume feature to restart pipelines/jobs from where interrupted * Create reports, MultiQC reports for bioinformatic pipelines * Easily integrated into workflow managers like Snakemake and NextFlow (to schedule jobs, scale jobs, identify paralellel steps in pipelines) ## What it CAN NOT do by itself * Schedule jobs * Scale jobs on HPC/cloud * Identify parallel steps in pipelines ## Prerequisites * python 3.6 or higher * OS: Linux, Mac ## API to RNA-Seq tools include: | Tool | Purpose | |--------------------------------------------------------------------------------------|---------------------| | [SRA Tools](https://github.com/ncbi/sra-tools) (v. 2.10.9 ) | SRA access | | [Trimgalore](https://github.com/FelixKrueger/TrimGalore) (v. 0.6.0) | Trimming | | [BBDuk](https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/) (v. 38.76) | Trimming | | [Hisat2](https://ccb.jhu.edu/software/hisat2/index.shtml) (v. 2.2.1) | Alignment | | [STAR](https://github.com/alexdobin/STAR) (v. 2.7.7a) | Alignment | | [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml) (v. 2.3.5.1) | Alignment | | [Kallisto](https://pachterlab.github.io/kallisto/) (v. 0.46.2) | Quantification | | [Salmon](https://combine-lab.github.io/salmon/) (v. 0.14.1) | Quantification | | [Stringtie](https://github.com/gpertea/stringtie) (v. 2.1.4) | Transcript Assembly | | [Cufflinks](http://cole-trapnell-lab.github.io/cufflinks/) (v. 2.2.1) | Transcript Assembly | | [Samtools](https://github.com/samtools/samtools) (v. 1.9) | Tools | ## Examples #### Get started with the basic [tutorial](https://pyrpipe.readthedocs.io/en/latest/?badge=latest). Read the documentation [here](https://pyrpipe.readthedocs.io/en/latest/?badge=latest). Several examples are provided [here](https://github.com/urmi-21/pyrpipe/tree/master/case_studies) ### Download, trim and align RNA-Seq data Following python code downloads data from SRA, uses Trim Galore to trim the fastq files and STAR to align reads. More detailed examples are provided [here](https://github.com/urmi-21/pyrpipe/tree/master/case_studies) ``` from pyrpipe.sra import SRA from pyrpipe.qc import Trimgalore from pyrpipe.mapping import Star trimgalore = Trimgalore(threads=8) star = Star(index=\\'data/index\\',threads=4) for srr in [\\'SRR976159\\',\\'SRR978411\\',\\'SRR971778\\']: SRA(srr).trim(trimgalore).align(star) ``` ### Import a Unix command This simple example imports and runs the Unix `grep` command. See [this](https://github.com/urmi-21/pyrpipe/blob/imp/case_studies/Integrating%20third-party%20tools.ipynb) for more examples. ``` >>> from pyrpipe.runnable import Runnable >>> grep=Runnable(command=\\'grep\\') >>> grep.run(\\'query1\\',\\'file1.txt\\',verbose=True) >>> grep.run(\\'query2\\',\\'file2.txt\\',verbose=True) ``` ## Installation Please follow these instructions: ### To create a new Conda environment (recommended): **NOTE: You need to install the third-party tools to work with pyrpipe. We recomend installing these through [bioconda](https://bioconda.github.io/) where possible. An example of setting up the environment using conda is provided below. It is best to [share your conda environment files](https://stackoverflow.com/questions/41274007/anaconda-export-environment-file) with pyrpipe scripts to ensure reproducibility.** 1. Download and install [Conda](https://docs.conda.io/en/latest/miniconda.html) 2. `conda create -n pyrpipe python=3.8` 3. `conda activate pyrpipe` 4. `conda install -c bioconda pyrpipe star=2.7.7a sra-tools=2.10.9 stringtie=2.1.4 trim-galore=0.6.6` The above command will install pyrpipe and the required tools inside a conda environment. Alternatively, use the conda environment.yaml file provided in this repository and build the conda environment by running ``` conda env create -f pyrpipe_environment.yaml ``` ### Install latest stable version #### Through conda ``` conda install -c bioconda pyrpipe ``` #### Through PIP ``` pip install pyrpipe --upgrade ``` If above command fails due to dependency issues, try: 1. Download the [requirements.txt](https://github.com/urmi-21/pyrpipe/blob/master/requirements.txt) 2. `pip install -r requirements.txt` 3. `pip install pyrpipe` To run tests: 1. Download the [test set](https://github.com/urmi-21/pyrpipe/tree/master/tests) ([direct link](https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/urmi-21/pyrpipe/tree/master/tests)) 2. `pip install pytest` 3. To build test_environment. Please [READ THIS](https://github.com/urmi-21/pyrpipe/blob/master/tests/README.md) 4. From pyrpipe root directory, run `pytest tests/test_*` ### Install dev version ``` git clone https://github.com/urmi-21/pyrpipe.git pip install -r pyrpipe/requirements.txt pip install -e path_to/pyrpipe #Running tests; From pyrpipe root perform #To build test_environment (This will download tools): cd tests ; . ./build_test_env.sh #in same terminal py.test tests/test_* ``` ## Setting NCBI SRA-Tools If you face problems with downloading data from SRA, try configuring the SRA-Tools. Use ```vdb-config -i``` to configure SRA Toolkit. Make sure that: * Under the **TOOLS** tab, prefetch downloads to is set to public user-repository * Under the **CACHE** tab, location of public user-repository is not empty Use the following pyrpipe_diagnostic command to test if SRA-Tools are setup properly ``` pyrpipe_diagnostic test ``` ## Contributing Please see [CONTRIBUTING.md](https://github.com/urmi-21/pyrpipe/blob/master/CONTRIBUTING.md) ## Funding This work is funded in part by the National Science Foundation award IOS 1546858, \"Orphan Genes: An Untapped Genetic Reservoir of Novel Traits\".']```"}, {"role": "user", "content": "Content from https://github.com/urmi-21/pyrpipe:\n```\nRepository metadata:\n{'name': 'pyrpipe', 'label': ['pyrpipe'], 'description': [{'term': 'Reproducible bioinformatics pipelines in python. Import any Unix tool/command in python. ', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['0.0.5', '0.0.4', '0.0.3', '0.0.2', '0.0.1'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/urmi-21/pyrpipe', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/rna-seq', 'term': 'rna-seq', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/rna-seq-pipeline', 'term': 'rna-seq-pipeline', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/ncbi-sra', 'term': 'ncbi-sra', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/bioinformatics', 'term': 'bioinformatics', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/bioinformatics-pipeline', 'term': 'bioinformatics-pipeline', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/bioinformatics-analysis', 'term': 'bioinformatics-analysis', 'vocabulary': ''}, 'id': 5}, {'term': {'uri': 'https://github.com/topics/python', 'term': 'python', 'vocabulary': ''}, 'id': 6}, {'term': {'uri': 'https://github.com/topics/rna-seq-workflows', 'term': 'rna-seq-workflows', 'vocabulary': ''}, 'id': 7}, {'term': {'uri': 'https://github.com/topics/conda', 'term': 'conda', 'vocabulary': ''}, 'id': 8}, {'term': {'uri': 'https://github.com/topics/bioconda', 'term': 'bioconda', 'vocabulary': ''}, 'id': 9}], 'operations': [], 'authors': [{'term': {'name': 'Urminder Singh', 'type': 'person', 'email': 'mrbains21@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'eve-syrkin-wurtele', 'type': 'person', 'email': 'mash@iastate.edu', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'code_of_conduct', 'url': 'https://github.com/urmi-21/pyrpipe/blob/main/CODE_OF_CONDUCT.md'}, 'id': 0}, {'term': {'type': 'contributing', 'url': 'https://github.com/urmi-21/pyrpipe/blob/main/CONTRIBUTING.md'}, 'id': 1}, {'term': {'type': 'readme', 'url': 'https://github.com/urmi-21/pyrpipe/blob/main/README.md'}, 'id': 2}, {'term': {'type': 'requirements', 'url': 'https://github.com/urmi-21/pyrpipe/blob/main/requirements.txt'}, 'id': 3}, {'term': {'type': 'root', 'url': 'https://github.com/urmi-21/pyrpipe/blob/main/rtd_requirements.txt'}, 'id': 4}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from http://pyrpipe.rtfd.io/:\n```\nContent:\n['## Introduction pyrpipe (pronounced as \u201cpyre-pipe\u201d) is a python package to easily develop computational pipelines in pure python, in an object-oriented manner. pyrpipe provides an easy-to-use object oriented framework to import any UNIX executable command or tool in python. pyrpipe also provides flexible and easy handling of tool options and parameters and can automatically load the tool parameters from .yaml files. This framework minimizes the commands user has to write, rather the tools are available as objects and are fully re-usable. All commands executed via pyrpipe are automatically logged extensively and could be compiled into reports using the pyrpipe_diagnostic tool. To enable easy and easy processing of RNA-Seq data, we have implemented specialized classes build on top of the pyrpipe framework. These classes provide high level APIs to many popular RNA-Seq tools for easier and faster development of RNA-Seq pipelines. These pipelines can be fully customizable and users can easily add/replace the tools using the pyrpipe framework. Finally, pyrpipe can be used on local computers or on HPC environments and pyrpipe scripts can be easily integrated into workflow management systems such as Snakemake and Nextflow. Get started with the tutorial: [Tutorial](tutorial/index.html#tutorial-overview) or look at real data [examples](https://github.com/urmi-21/pyrpipe/tree/master/case_studies) . ### Key Features * Import any UNIX executable command/tool in python * Dry-runnable pipelines to check dependencies and commands before execution * Flexible and robust handling of tool arguments and parameters * Auto load parameters from .yaml files * Easily override threads and memory options using global values * Extensive logging and reports with MultiQC reports for bioinformatic pipelines * Specify GNU make like targets and verify the integrity of the targets * Automatically resume pipelines/jobs from where interrupted * Easily integrated into workflow managers like Snakemake and NextFlow ### Installation To install the latest stable release via conda: conda install -c bioconda pyrpipe To install the latest stable release via pip: pip install pyrpipe --upgrade Install latest development version : git clone https://github.com/urmi-21/pyrpipe.git pip install -r pyrpipe/requirements.txt pip install -e pyrpipe See the [Installation notes](installation.html#id1) for details.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bioconductor-atsnp/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bioconductor-atsnp/lib/1.18.0\",\n    \"name\": \"bioconductor-atsnp\",\n    \"description\": [\n      \"AtSNP performs affinity tests of motif matches with the SNP or the reference genomes and SNP-led changes in motif matches.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/3.18/bioc/html/atSNP.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-2.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-2.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/atsnp/lib/None\",\n    \"name\": \"atsnp\",\n    \"description\": [\n      \"Affinity Test for regulatory SNP detection package is a bioinformatics tool for computing and testing large-scale motif-SNP interactions.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/chandlerzuo/atsnp\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Chandler Zu\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": \"http://pages.stat.wisc.edu/~zuo/\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67e2c2feb57beba26405d557\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/3.18/bioc/html/atSNP.html:\n```\nContent:\n['# atSNP This package is for version 3.18 of Bioconductor; for the stable, up-to-date release version, see [atSNP](/packages/atSNP/) . ## Affinity test for identifying regulatory SNPs **Bioconductor version:** 3.18 atSNP performs affinity tests of motif matches with the SNP or the reference genomes and SNP-led changes in motif matches. **Author:** Chandler Zuo [aut], Sunyoung Shin [aut, cre], Sunduz Keles [aut] **Maintainer:** Sunyoung Shin <sunyoung.shin at utdallas.edu> **Citation (from within R, entercitation(\"atSNP\")):** ### Installation To install this package, start R (version \"4.3\") and enter: if (! require ( \"BiocManager\" , quietly = TRUE )) install. packages ( \"BiocManager\" ) BiocManager :: install ( \"atSNP\" ) For older versions of R, please refer to the appropriate [Bioconductor release](/about/release-announcements/) . ### Documentation To view documentation for the version of this package installed in your system, start R and enter: browseVignettes (\"atSNP\") ### Details biocViews [ChIPSeq](../../BiocViews.html#___ChIPSeq) , [GenomeAnnotation](../../BiocViews.html#___GenomeAnnotation) , [MotifAnnotation](../../BiocViews.html#___MotifAnnotation) , [Software](../../BiocViews.html#___Software) , [Visualization](../../BiocViews.html#___Visualization) Version 1.18.0 In Bioconductor since BioC 3.9 (R-3.6) (5 years) License GPL-2 Depends R (>= 3.6) Imports [BSgenome](../../bioc/html/BSgenome.html) , [BiocFileCache](../../bioc/html/BiocFileCache.html) , [BiocParallel](../../bioc/html/BiocParallel.html) , Rcpp, data.table, ggplot2, grDevices, graphics, grid, [motifStack](../../bioc/html/motifStack.html) , rappdirs, stats, testthat, utils, lifecycle System Requirements URL [https://github.com/sunyoungshin/atSNP](https://github.com/sunyoungshin/atSNP) Bug Reports [https://github.com/sunyoungshin/atSNP/issues](https://github.com/sunyoungshin/atSNP/issues) See More Suggests [BiocStyle](../../bioc/html/BiocStyle.html) , knitr, rmarkdown Linking To Rcpp Enhances Depends On Me Imports Me Suggests Me Links To Me Build Report [Build Report](https://bioconductor.org/checkResults/3.18/bioc-LATEST/atSNP/) ### Package Archives Follow [Installation](/install/#install-bioconductor-packages) instructions to use this package in your R session.']```"}, {"role": "user", "content": "Content from https://github.com/chandlerzuo/atsnp:\n```\nREADME content:\n['atSNP ====== OVERVIEW -------- atSNP ( *Affinity Test for regulatory SNP detection* ) package is a bioinformatics tool for computing and testing large-scale motif-SNP interactions. It provides three main functions: - Compute the binding affinity scores for both the reference and the SNP alleles based on position weight matrices; - Compute the p-values of the affinity scores for each allele; - Compute the p-values of the affinity score changes between the reference and the SNP alleles. atSNP implements the importance sampling algorithm to compute the p-values. Existing tools, such as FIMO and is-rSNP, compute the p-values analytically. This is computationally intensive because the probability sample space is a exponential order of the motif length. By implementing the importance sampling algorithm, atSNP is able to evaluate the p-value without exhausting the sample space, thereby significantly reduces running time. In one of our research projects, we have used atSNP to evaluate interactions between 26K SNPs and 2K motifs within 5 hours. To our knowledge, no other existing tool can finish the analysis of such a scale. INSTALLATION ------------ atSNP will be available at Bioconductor. Currently you can download the development version here and install in R by: library(devtools) install_github(\"sunyoungshin/atSNP\") REFERENCES ---------- Chandler Zuo, Sunyoung Shin and Sunduz Keles (2015). \"atSNP: affinity test for regulatory SNP detection\". Bioinformatics, 31 (20), 3353-3355.']```"}, {"role": "user", "content": "Content from https://github.com/chandlerzuo/atsnp:\n```\nRepository metadata:\n{'name': 'atSNP', 'label': ['atSNP'], 'description': [{'term': 'Large-scale testing for SNP-motif interactions.', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [], 'repository': [{'term': 'https://github.com/chandlerzuo/atSNP', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Chandler Zuo', 'type': 'person', 'email': 'chandler.c.zuo@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Chandler Zuo', 'type': 'person', 'email': 'chandlerzuo@fb.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Sunyoung Shin', 'type': 'person', 'email': 'sunyoung.shin@utdallas.edu', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Nitesh Turaga', 'type': 'person', 'email': 'nitesh.turaga@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Sunyoung Shin', 'type': 'person', 'email': 'sunyoungshin@cometnet-10-21-18-219.utdallas.edu', 'maintainer': False}, 'id': 4}, {'term': {'name': 'sunyoungshin', 'type': 'person', 'email': 'sunyoungshin@users.noreply.github.com', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Sunyoung Shin', 'type': 'person', 'email': 'sunyoungshin@Sunyoungs-MacBook-Pro.local', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Sunyoung Shin', 'type': 'person', 'email': 'shin@stat.wisc.edu', 'maintainer': False}, 'id': 7}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/chandlerzuo/atsnp/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"expressionview/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconductor/expressionview/lib/1.45.0\",\n    \"name\": \"expressionview\",\n    \"description\": [\n      \"ExpressionView visualizes possibly overlapping biclusters in a gene expression matrix. It can use the result of the ISA method (eisa package) or the algorithms in the biclust package or others. The viewer itself was developed using Adobe Flex and runs in a flash-enabled web browser.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://git.bioconductor.org/packages/ExpressionView\",\n        \"kind\": \"bioconductor\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/ExpressionView\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-2.0-or-later\",\n        \"url\": \"https://spdx.org/licenses/GPL-2.0-or-later.html\"\n      }\n    ],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Gabor Csardi\",\n        \"email\": \"csardi.gabor@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"bioconductor\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/expressionview/lib/1.26.0\",\n    \"name\": \"expressionview\",\n    \"description\": [\n      \"R package that provides an interactive environment to explore biclusters identified in gene expression data. A sophisticated ordering algorithm is used to present the biclusters in a visually appealing layout. From this overview, the user can select individual biclusters and access all the biologically relevant data associated with it. The package is aimed to facilitate the collaboration between bioinformaticians and life scientists who are not familiar with the R language.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://www2.unil.ch/cbg/index.php?title=ExpressionView\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Gabor Csardi\",\n        \"email\": \"csardi.gabor@gmail.com\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7aab1d3b8acd1a2f9bce1\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/ExpressionView:\n```\nContent:\n['# Removed Packages A list of packages removed from Bioconductor along with their last-available landing pages. ## Packages deprecated in Bioconductor 3.21 (to be removed in 3.22) Software Packages: Experiment Data Packages: Annotation Data Packages: Workflow Packages: ## Packages removed with Bioconductor 3.21 release Software Packages: Note: [zlibbioc](/packages/3.19/bioc/html/zlibbioc.html) is deprecated but will be given an extra 6 months (removed in 3.22) to allow multiple dependencies to adapt. Experiment Data Packages: ## Packages removed with Bioconductor 3.20 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.19 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.18 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.17 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.16 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Packages: ## Packages removed with Bioconductor 3.15 release Software Packages: Experiment Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.14 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.13 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.12 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.11 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.10 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.9 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.8 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.7 release Software Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.6 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.5 release Software Packages: Experimental Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.4 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.3 release No packages were removed with this release. 17 packages were marked as deprecated, to be removed in the next release. ## Packages removed with Bioconductor 3.2 release No packages were removed with this release. ## Packages removed with Bioconductor 3.1 release ## Packages removed with Bioconductor 3.0 release ## Packages removed with Bioconductor 2.14 release ## Packages removed with Bioconductor 2.13 release ## Packages removed with Bioconductor 2.12 release ## Packages removed with Bioconductor 2.11 release ## Packages removed with Bioconductor 2.10 release ## Packages removed with Bioconductor 2.9 release ## Packages removed with Bioconductor 2.8 release ## Packages removed with Bioconductor 2.7 release ## Packages removed with Bioconductor 2.6 release ## Packages removed with Bioconductor 2.5 release ## Packages removed with Bioconductor 2.4 release ## Packages removed with Bioconductor 2.3 release ## Packages removed with Bioconductor 2.2 release ## Packages removed with Bioconductor 2.1 release ## Packages removed with Bioconductor 2.0 release ## Packages removed with Bioconductor 1.9 release ## Packages removed with Bioconductor 1.8 release ## Packages removed with Bioconductor 1.7 release']```"}, {"role": "user", "content": "Content from https://git.bioconductor.org/packages/ExpressionView:\n```\nRepository metadata:\n{'url': 'https://git.bioconductor.org/packages/ExpressionView'}```"}, {"role": "user", "content": "Content from https://www2.unil.ch/cbg/index.php?title=ExpressionView:\n```\nContent:\n['ExpressionView is an R package that provides an interactive environment to explore biclusters identified in gene expression data. A sophisticated ordering algorithm is used to present the biclusters in a visually appealing layout. From this overview, the user can select individual biclusters and access all the biologically relevant data associated with it. The package is aimed to facilitate the collaboration between bioinformaticians and life scientists who are not familiar with the R language. # Demos # Requirements and installation ## Download the R package (includes the Flash applet) To use the ExpressionView R package you will need a working [GNU R](http://www.r-project.org) installation. As of the 23rd of April, 2010, the ExpressionView package is an official [BioConductor](http://www.bioconductor.org) package. ExpressionView depends on a number of other R packages: isa2, Biobase, AnnotationDbi, etc. The good news is that all these dependencies are installed automatically, and all you need to do is to start R and type in source(\" [http://bioconductor.org/biocLite.R](http://bioconductor.org/biocLite.R) \") biocLite(\"ExpressionView\") at your R prompt. See [the ExpressionView package page at the BioConductor website](http://bioconductor.org/packages/2.8/bioc/html/ExpressionView.html) for details. Alternatively, you can also download the package from here: The Flash applet requires a Flash-enabled web browser. Please install Adobe Flash Player from the [Adobe web site](http://get.adobe.com/flashplayer/) if your browser does not have it yet. ## Download the stand-alone viewer (Adobe AIR) If you prefer a stand-alone viewer, you can download and install the Adobe AIR build [ExpressionView.air](http://www2.unil.ch/cbg/software/expressionview/air/ExpressionView.air) (right-click to download file). To run the program, you need the AIR runtime environment which you can get from [Adobe](http://get.adobe.com/air) . ExpressionView creates files associations to .evf files, allowing you to simply double-click on such files to launch the viewer and load the data. ## Download sample data # License The ExpressionView package is licensed under the GNU General Public License, version 2 or later. For details, see [http://www.gnu.org/licenses/old-licenses/gpl-2.0.html](http://www.gnu.org/licenses/old-licenses/gpl-2.0.html) . # Screenshots * Global view after loading datasetHighlighting genes (probes) and samplesModules view emphasizing the underlying gene expression dataGlobal view without gene expression data # Tutorials There are several tutorials describing how to use ExpressionView. The features of the R package are documented within the program. Just have a look at the ExpressionView help page after you have installed the package. Below, you can download the tutorial presenting the basic workflow and the description of the ordering algorithm. For the Flash applet, we have produced a few videos showing you how to use the program. [HTML](/cbg/index.php?title=Getting_started_with_ExpressionView) [PDF](http://www2.unil.ch/cbg/homepage/downloads/ExpressionView.tutorial.pdf) [Rnw](http://www2.unil.ch/cbg/homepage/downloads/ExpressionView.tutorial.Rnw) [R code](http://www2.unil.ch/cbg/homepage/downloads/ExpressionView.tutorial.R) [HTML](/cbg/index.php?title=Ordering_algorithm_used_in_ExpressionView) [PDF](http://www2.unil.ch/cbg/homepage/downloads/ExpressionView.ordering.pdf) [HTML](/cbg/index.php?title=ExpressionView_File_Format) [PDF](http://www2.unil.ch/cbg/homepage/downloads/ExpressionView.format.pdf) [PDF](http://www2.unil.ch/cbg/software/expressionview/r/ExpressionView.pdf) ### Flash applet ### Installing the stand-alone version # Additional documentation and downloads The ExpressionView data file is an XML file. We have created a corresponding XML Schema file that defines its structure. The Flash applet is written in ActionScript. It is open source and can be built from the command line using the Adobe Flex SDK or more conveniently with the Adobe Flex Builder IDE. For more information, visit the [Flex website](http://www.adobe.com/flex) . For the Flash applet, we have implemented components that could also be used in other applications. The most important one being the LargeBitmapData class that allows one to work with BitmapData of arbitrary size. In the [standard BitmapData class](http://livedocs.adobe.com/flex/3/langref/flash/display/BitmapData.html) , the maximum size for a BitmapData object is 8,192 pixels in width or height, and the total number of pixels cannot exceed 16,777,216 pixels. Note that the ResizablePanel class is no longer used in ExpressionView.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"cryfa/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/cryfa/cmd/20.04\",\n    \"name\": \"cryfa\",\n    \"description\": [\n      \"A secure encryption tool for genomic data.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/smortezah/cryfa\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/smortezah/cryfa\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/cryfa/cmd/None\",\n    \"name\": \"cryfa\",\n    \"description\": [\n      \"Secure encryption tool for genomic data, namely in Fasta, Fastq, VCF, SAM and BAM formats, which is also capable of reducing the storage size of Fasta and Fastq files.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/cobilab/cryfa\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Morteza Hosseini\",\n        \"email\": \"seyedmorteza@ua.pt\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7ac4fd3b8acd1a2f9bf85\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/cobilab/cryfa:\n```\nREADME content:\n['<p align=\"center\"><img src=\"logo.png\" alt=\"Cryfa\" height=\"150\"/></p> [![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](LICENSE) [![CI](https://github.com/cobilab/cryfa/actions/workflows/ci.yml/badge.svg)](https://github.com/cobilab/cryfa/actions/workflows/ci.yml) Cryfa is an ultrafast encryption tool specifically designed for genomic data. Besides providing robust security, it also compresses FASTA/FASTQ sequences by a factor of three, making it an efficient solution for managing genomic data. # Installation ## Conda ```bash conda install -c bioconda -y cryfa ``` ## Docker ```bash # Pull & Run the image docker pull smortezah/cryfa; docker run -it smortezah/cryfa; ``` ## Build from source ### Linux ```bash # Install git and cmake sudo apt update; sudo apt install git cmake; # Clone and install Cryfa git clone https://github.com/cobilab/cryfa.git; cd cryfa; sh install.sh; ``` ### macOS ```bash # Install Homebrew, git and cmake /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"; brew install git cmake; # Clone and install Cryfa git clone https://github.com/cobilab/cryfa.git; cd cryfa; sh install.sh; ``` > [!NOTE] > Pre-compiled versions of Cryfa, optimized for 64-bit Linux and macOS, can be found in the `bin/` directory. # Usage To execute Cryfa in stand-alone mode, utilize the command below: ```bash ./cryfa [OPTION]... -k [KEY_FILE] [-d] [IN_FILE] > [OUT_FILE] ``` For instance, to compact and encrypt data, execute the following command: ```bash ./cryfa -k pass.txt in.fq > comp ``` To decrypt and unpack the data, execute the command below: ```bash ./cryfa -k pass.txt -d comp > orig.fq ``` A sample file, \"in.fq\", is available in the example/ directory. Detailed descriptions of the options are provided in the subsequent sections. > [!NOTE] > Cryfa supports a maximum file size of 64 GB. For larger files, consider splitting them into smaller chunks, e.g. using the `split` command in Linux, and then encrypt each chunk separately. After decryption, you can reassemble the chunks using the `cat` command. ## Input file format Cryfa identifies the format of a genomic data file by examining its content, not its extension. For instance, a FASTA file named \"test\" can be input into Cryfa with any extension, such as \"test\", \"test.fa\", \"test.fasta\", \"test.fas\", \"test.fsa\", etc. Based on this, executing the command ```bash ./cryfa -k pass.txt test > comp ``` is equivalent to running ```bash ./cryfa -k pass.txt test.fa > comp ``` > [!NOTE] > The password file extension is not a limiting factor for Cryfa. It can have any extension or even no extension at all. For instance, \"pass\", \"pass.txt\", \"pass.dat\", and so on, are all valid and yield the same result. ## Benchmarking Cryfa Against Other Methods To benchmark Cryfa against other methods, configure the parameters in the **run.sh** bash script and execute it: ```bash ./run.sh ``` This script automates the process of downloading datasets, installing dependencies, setting up compression and encryption tools, executing these tools, and finally, displaying the results. # Options To explore the available options in Cryfa, execute the command below: ```bash ./cryfa ``` which will yield the following: ``` SYNOPSIS ./cryfa [OPTION]... -k [KEY_FILE] [-d] [IN_FILE] > [OUT_FILE] SAMPLE Encrypt and compact: ./cryfa -k pass.txt in.fq > comp Decrypt and unpack: ./cryfa -k pass.txt -d comp > orig.fq Encrypt: ./cryfa -k pass.txt in > enc Decrypt: ./cryfa -k pass.txt -d enc > orig OPTIONS Compact & encrypt FASTA/FASTQ files. Encrypt any text-based genomic data, e.g., VCF/SAM/BAM. -k [KEY_FILE], --key [KEY_FILE] key file name -- MANDATORY The KEY_FILE should contain a password. To make a strong password, the \"keygen\" program can be used via the command \"./keygen\". -d, --dec decrypt & unpack -f, --force force to consider input as non-FASTA/FASTQ Forces Cryfa not to compact, but shuffle and encrypt. If the input is FASTA/FASTQ, it is considered as non-FASTA/FASTQ; so, compaction will be ignored, but shuffling and encryption will be performed. -s, --stop_shuffle stop shuffling the input -t [NUMBER], --thread [NUMBER] number of threads -v, --verbose verbose mode (more information) -h, --help usage guide --version version information ``` Cryfa leverages the standard output stream, allowing seamless integration with existing data processing pipelines. ## Creating a Key File There are two approaches to create a \"KEY_FILE\" that can be used with the `-k` or `--key` flags. You can either save a raw password in a file or use the provided \"keygen\" program to generate a robust password. The latter method is strongly recommended for enhanced security. To utilize the first method, use the commands below to save a raw password in a file, which can then be passed to Cryfa. In this example, \"Such a strong password!\" is the raw password and \"pass.txt\" is the file where the password is stored. Alternatively, you can use a text editor to save the password in a file: ```bash echo \"Such a strong password!\" > pass.txt ./cryfa -k pass.txt IN_FILE > OUT_FILE ``` While the password must contain at least 8 characters, it\\'s highly recommended to use a strong password for better security. A strong password: - Is at least 12 characters long - Includes a mix of lowercase (a-z) and uppercase (A-Z) letters, digits (0-9), and symbols (e.g., !, #, $, %, and }) - Is not a simple repetition of characters (e.g., zzzzzz), a keyboard pattern (e.g., qwerty), or a sequence of digits (e.g., 123456) Alternatively, you can leverage the \"keygen\" program to automatically generate a robust password. To do this, execute: ```bash ./keygen ``` You\\'ll be prompted with: ```text Enter a password, then press \\'Enter\\': ``` At this point, input a raw password, for example, \"A keygen raw pass!\", and press \"Enter\". Subsequently, you\\'ll see: ```text Enter a file name to save the generated key, then press \\'Enter\\': ``` The robust password generated by the \"keygen\" program will be stored in the file you specify, such as \"key.txt\". Note that \"keygen\" requires an initial raw password to generate a strong password, but this initial password doesn\\'t need to be particularly strong. Once the key file is created, you can use it with Cryfa as shown below: ```bash ./cryfa -k key.txt IN_FILE > OUT_FILE ``` For a deeper understanding of \"key management\" - which encompasses the generation, exchange, storage, usage, and replacement of keys - consider exploring [[1]](https://en.wikipedia.org/wiki/Key_management), [[2]](https://info.townsendsecurity.com/definitive-guide-to-encryption-key-management-fundamentals), [[3]](https://csrc.nist.gov/projects/key-management/cryptographic-key-management-systems) and [[4]](https://www.cryptomathic.com/news-events/blog/what-is-key-management-a-ciso-perspective). # Citation If you utilize Cryfa in your research, please acknowledge the tool by citing the following references: - M. Hosseini, D. Pratas and A.J. Pinho, \"Cryfa: a secure encryption tool for genomic data,\" *Bioinformatics*, vol. 35, no. 1, pp. 146--148, 2018. [DOI: 10.1093/bioinformatics/bty645](https://doi.org/10.1093/bioinformatics/bty645) - **[OPTIONAL]** D. Pratas, M. Hosseini and A.J. Pinho, \"Cryfa: a tool to compact and encrypt FASTA files,\" *11\\'th International Conference on Practical Applications of Computational Biology & Bioinformatics* (PACBB), Springer, June 2017. [DOI: 10.1007/978-3-319-60816-7_37](https://doi.org/10.1007/978-3-319-60816-7_37) # Codebase ![Visualization of this repo](./diagram.svg) # License Cryfa is licensed under the [GPLv3](http://www.gnu.org/licenses/gpl-3.0.html).']```"}, {"role": "user", "content": "Content from https://github.com/cobilab/cryfa:\n```\nRepository metadata:\n{'name': 'cryfa', 'label': ['cryfa'], 'description': [{'term': 'A secure encryption tool for genomic data', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v20.04', 'v18.06', 'V1.17.05'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/cobilab/cryfa', 'id': 0}], 'topics': [{'term': {'uri': 'https://github.com/topics/encryption', 'term': 'encryption', 'vocabulary': ''}, 'id': 0}, {'term': {'uri': 'https://github.com/topics/compact', 'term': 'compact', 'vocabulary': ''}, 'id': 1}, {'term': {'uri': 'https://github.com/topics/keygen', 'term': 'keygen', 'vocabulary': ''}, 'id': 2}, {'term': {'uri': 'https://github.com/topics/fastq-format', 'term': 'fastq-format', 'vocabulary': ''}, 'id': 3}, {'term': {'uri': 'https://github.com/topics/fasta-format', 'term': 'fasta-format', 'vocabulary': ''}, 'id': 4}, {'term': {'uri': 'https://github.com/topics/genomics', 'term': 'genomics', 'vocabulary': ''}, 'id': 5}], 'operations': [], 'authors': [{'term': {'name': 'repo-visualizer', 'type': 'person', 'email': 'repo-visualizer@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Morteza Hosseini', 'type': 'person', 'email': 's_morteza_h@yahoo.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Diogo Pratas', 'type': 'person', 'email': 'pratas@ua.pt', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Morteza Hosseini', 'type': 'person', 'email': 'morteza@sqlsrv-cl2.servers.ua.pt', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/cobilab/cryfa/blob/main/CMakeLists.txt'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/cobilab/cryfa/blob/main/README.md'}, 'id': 1}, {'term': {'type': 'root', 'url': 'https://github.com/cobilab/cryfa/blob/main/pass.txt'}, 'id': 2}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"gffcompare/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/gffcompare/cmd/0.12.6\",\n    \"name\": \"gffcompare\",\n    \"description\": [\n      \"GffCompare by Geo Pertea.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/gpertea/gffcompare\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://ccb.jhu.edu/software/stringtie/gffcompare.shtml\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"Artistic-2.0\",\n        \"url\": \"https://spdx.org/licenses/Artistic-2.0.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"galaxy/gffcompare/cmd/0.9.8\",\n    \"name\": \"gffcompare\",\n    \"description\": [\n      \"Compare assembled transcripts to a reference annotation.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://galaxy.bi.uni-freiburg.de/tool_runner?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Fgffcompare%2Fgffcompare%2F0.9.8\"\n    ],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"galaxy\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/gpertea/gffcompare:\n```\nREADME content:\n['## GffCompare * compare and evaluate the accuracy of RNA-Seq transcript assemblers (Cufflinks, Stringtie). * collapse (merge) duplicate transcripts from multiple GTF/GFF3 files (e.g. resulted from assembly of different samples) * classify transcripts from one or multiple GTF/GFF3 files as they relate to reference transcripts provided in a annotation file (also in GTF/GFF3 format) More details and usage examples can be found in the paper: [DOI: 10.12688/f1000research.23297.1](http://dx.doi.org/10.12688/f1000research.23297.1) which can be also used to cite this software. The official documentation and download packages for this utility can be found online here: http://ccb.jhu.edu/software/stringtie/gffcompare.shtml For more information about the GFF3/GTF file formats expected by GffCompare please refer to the available online documentation -- a quick review can be found at: http://ccb.jhu.edu/software/stringtie/gff.shtml The original version of this program was distributed as part of the Cufflinks suite, under the name \"CuffCompare\". The overall functionality and most of the options of CuffCompare are still supported by GffCompare, while new functionality is being added to GffCompare only, as it is the program which is actively maintained. An example of a new feature of GffCompare (compared to its predecessor CuffCompare) is this: when a single query GTF/GFF file is given as input for analysis, along with a reference annotation (-r option), GffCompare switches into *annotation mode* and it generates a *.annotated.gtf* file instead of the *.combined.gtf* produced by CuffCompare with the same parameters. This file has the same general format as CuffCompare\\'s *.combined.gtf* file (with \"class codes\" assigned to transcripts as per their relationship with the matching/overlapping reference transcript), but the original transcript IDs are preserved, so GffCompare can thus be used as a simple way of annotating a set of transcripts. Another important difference is that the input transcripts are by default no longer discarded when they are found to be \"intron redundant\", i.e. contained within other, longer isoforms. CuffCompare had the -G option to prevent collapsing of such intron redundant isoforms into their longer \"containers\", but GffCompare has made this the default mode of operation (hence the -G option is no longer needed and is simply ignored when given). However please note that \"matching\" transcripts with fully identical intron chains (i.e. with the same exact intron coordinates, hence the same intron-exon structure except the terminal exon ends) are *still* discarded when GTF/GFF files are loaded. ## trmap Some pipelines can produce a very large number of potential or partial transcripts (\"transfrags\"), for example when merging the transcript assemblies from tens or hundreds of RNA-Seq experiments assemblies with `stringtie --merge`. Running GffCompare on such large GTF/GFF files could be slow and memory intensive (because GffCompare always loads the whole transcript data in memory for clustering and other analysis). One may only be interested to know _if_ and _how_ these many transcripts overlap the reference annotation, and further analyze only those which have specific types of overlap with the reference annotation transcripts (or none at all, i.e. if they do not overlap any of it, which may be the case for putative _novel_ transcripts). That\\'s where the `trmap` utility comes in, as this program reports, for each query transcript, all the reference overlaps found, along with their _classification codes_ as described in the GffCompare documentation. The main feature of \\'trmap\\' is that it allows _streaming_ of a very large file of query transcripts (in GFF or BED format) to be checked and classified against a reference annotation file (again, in GFF or BED format). `trmap` (\"Tanscript vs. Refererence MAPping\") first loads the reference annotation file in memory as an interval tree and then streams the query file (which can be also provided at stdin) while checking and reporting any overlaps found, and classifies the relationship with reference transcripts using a (subset of) the \"class codes\" like those assigned by gffcompare (see http://ccb.jhu.edu/software/stringtie/gffcompare.shtml). The streaming input GFF query input file to be streamed must be _well-formed_ -- i.e. exons MUST be grouped together by transcript ID and immediately follow their parent feature if present. (for BED this is always the case due to the fact that exons are embedded in the same line). ## Building from source Steps for building this package from github: ``` cd /some/build/dir git clone https://github.com/gpertea/gffcompare cd gffcompare make release ``` If you downloaded the standalone source package `gffread-*.tar.gz` then just unpack that, change to the unpacked directory and run `make release` there. This should build the **gffcompare** and **trmap** binary executables in the current directory.']```"}, {"role": "user", "content": "Content from https://github.com/gpertea/gffcompare:\n```\nRepository metadata:\n{'name': 'gffcompare', 'label': ['gffcompare'], 'description': [{'term': 'classify, merge, tracking and annotation of GFF files by comparing to a reference annotation GFF', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v0.12.6', 'v0.12.2', 'v0.12.1', 'v0.11.7', 'v0.11.6'], 'license': [{'term': {'name': 'MIT License', 'url': 'http://choosealicense.com/licenses/mit/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/gpertea/gffcompare', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Geo Pertea', 'type': 'person', 'email': 'geo.pertea@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'sparrow', 'type': 'person', 'email': 'alevar@gmail.com', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/gpertea/gffcompare/blob/main/README.md'}, 'id': 0}, {'term': {'type': 'root', 'url': 'https://github.com/gpertea/gffcompare/blob/main/tx_split.txt'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://ccb.jhu.edu/software/stringtie/gffcompare.shtml:\n```\nContent:\n['# The GffCompare utility The program gffcompare can be used to compare, merge, annotate and estimate accuracy of one or more GFF files (the \u201cquery\u201d files), when compared with a reference annotation (also provided as GFF). Usage: gffcompare [options]* {-i <input_gtf_list> | <input1.gtf> [<input2.gtf> .. <inputN.gtf>]} ##### Options -i provide a text file with a list of (query) GTF files to process instead of expecting them as command-line arguments (useful when a large number of GTF files should be processed). -h/-help Prints the help message and exits -v/--version Prints the version number and exits -o <outprefix> All output files created by gffcompare will have this prefix (e.g. .loci, .tracking, etc.). If this option is not provided the default output prefix being used is: \u201cgffcmp\u201d -r An optional \u201creference\u201d annotation GFF file. Each sample is matched against this file, and sample isoforms are tagged as overlapping, matching, or novel where appropriate. See the .refmap and .tmap output file descriptions below. -R If -r was specified, this option causes gffcompare to ignore reference transcripts that are not overlapped by any transcript in one of input1.gtf,\u2026,inputN.gtf. Useful for ignoring annotated transcripts that are not present in your RNA-Seq samples and thus adjusting the \u201csensitivity\u201d calculation in the accuracy report written in the file. -Q If -r was specified, this option causes gffcompare to ignore input transcripts that are not overlapped by any transcript in the reference. Useful for adjusting the \u201cprecision\u201d calculation in the accuracy report written in the file. -M discard (ignore) single-exon transfrags and reference transcripts (i.e. consider only multi-exon transcripts) -N discard (ignore) single-exon reference transcripts; single-exon transfrags are still considered, but they will never find an exact match -D discard \"duplicate\" (redundant) query transfrags (i.e. those with the same intron chain) within a single sample (and thus disable \"annotation\" mode) -s <genome_path> path to genome sequences (optional); this will enable the \"repeat\" (\\'r\\') classcode assessment; <genome_path> should be a full path to a multi-FASTA file, preferrably indexed with samtools faidx ; repeats must be soft-masked (lower case) in the genomic sequence -e <dist> Maximum distance (range) allowed from free ends of terminal exons of reference transcripts when assessing exon accuracy. By default, this is 100. -d <dist> Maximum distance (range) for grouping transcript start sites, by default 100. -p <tprefix> The name prefix to use for consensus/combined transcripts in the <outprefix>.combined.gtf file (default: \\'TCONS\\') -C Discard the \u201ccontained\u201d transfrags from the .combined.gtf output. By default, without this option, gffcompare writes in that file isoforms that were found to be fully contained/covered (with the same compatible intron structure) by other transfrags in the same locus, with the attribute \u201ccontained_in\u201d showing the first container transfrag found. (Note: this behavior is the opposite of Cuffcompare\\'s -C option). -A Like -C but will not discard intron-redundant transfrags if they start on a different 5\\' exon (keep alternate transcript start sites) -X Like -C but also discard contained transfrags if transfrag ends stick out within the container\\'s introns -K For -C/-A/-X, do NOT discard any redundant transfrag matching a reference -T Do not generate .tmap and .refmap files for each input file -V Gffcompare is a little more verbose about what it\\'s doing, printing messages to stderr, and it will also show warning messages about any inconsistencies or potential issues found while reading the given GFF file(s). --debug Enables debug mode, which enables -V and generates additional files: <outprefix>.Q_discarded.lst, <outprefix>.missed_introns.gtf and <outprefix>.R_missed.lst ## Gffcompare output files Gffcompare produces the following output files: ### Data summary and accuracy estimation: <outprefix>.stats In this output file Gffcompare reports various statistics related to the \u201caccuracy\u201d (or a measure of agreement) of the input transcripts when compared to reference annotation data (provided with the -r option). These accuracy measures are calculated under the assumption that the input GFF/GTF file(s) (the \"query\" transcripts, or transfrags, from one or multiple \"samples\") are coming from some transcript discovery/assembly pipeline (e.g. Cufflinks or StringTie), or from any other gene/transcript prediction pipeline. GffCompare can be used to assess the accuracy of such pipelines, when comparing their results to a known reference annotation (provided with the -r option). The measures of \u201csensitivity\u201d and \u201cprecision\u201d are calculated at various levels (nucleotide, exon, intron, transcript, gene) for each input file and reported in this .stats file. These generic accuracy metrics are calculated as described in Burset, M., Guigo\u0301, R. : **Evaluation of gene structure prediction programs** (1996) Genomics, 34 (3), pp. 353-367. [doi: 10.1006/geno.1996.0298](http://dx.doi.org/10.1006/geno.1996.0298) (please note that in that paper the authors used the term _\u201cSpecificity\u201d_ for what we currently call _\u201cPrecision\u201d_ ). The following general formulae are used for calculation of _Sensitivity_ and _Precision_ at various levels (base, exon, intron chain, transcript, gene): _Sensitivity = TP / (TP+FN)_ _Precision = TP / (TP+FP)_ Here _TP_ means \"true positives\", which in this case are the query features (bases, exons, introns, transcripts, etc.) which **agree** with the corresponding reference annotation features. _FN_ means \"false negatives\", i.e. features which are found in the reference annotation but missed (not present) in the \"query\" data. Accordingly, _FP_ are features present in the input \"query\" data but not confirmed by any reference annnotation data. Notice that _FP_ + _TP_ amounts to the whole input set of \"query\" features (per sample/input file). If multiple \"query\" GTF/GFF files are given as input, these metrics are computed separately for each sample. **Sensitivity** and **Precision** values are estimated at various _levels_ , which are largely an increasingly stringent way of evaluating the accuracy/correctness of a set of predicted (assembled) transcripts (transfrags), when compared to the reference annotation provided with the -r option. The levels are as follows: ##### Base level * **TP**(true positives) number is the number of exon bases that are reported at the same coordinate on both the query (the assembled transfrag) and *any* reference transcript (that is, the overlap length of exons) * **FN**(false negatives) is the number of bases in reference data exons which are not covered at all by any of the predicted transcripts(transfrags) exons; * **FP**(false positives) then is the number of bases which are only covered by any predicted transcript\\'s exons but not covered by any reference transcript exons ##### Exon level This works similarly to the Base level TP/FN/FP concept described above, but now the unit of comparison (\"matching\") is the _exon_ interval on the genome, i.e. if an exon of the predicted transcript overlaps and matches the boundaries of a reference transcript exon, then it is counted as **TP** . ##### Intron Level Similar to the Exon level, only now the units being \"matched\" and counted are the _intron_ intervals instead of exon intervals: each intron of the predicted transcript is checked against any introns of the reference transcripts in the area and is there is one with the same exact start-end coordinates, it is counted as a TP. ##### Intron chain level Here we count as a TP any predicted (assembled) transcript for which _all_ of its introns can be found, with the same exact _intron_ coordinates in a reference transcript (with the same number of introns). Matching all the introns at this level is the same as matching all the internal exons, but allowing for the possibility that the external boundaries of the terminal exons might not match at all. However it is quite realistic to consider transcripts \"matching\" when their intron chains are identical (and GffCompare does annotate \"matching\" transcripts with the class code \\'=\\' when intron chains are the same), considering that: * the reference annotation usually does not have perfectly accurate transcription start and stop coordinates for each transcript * the biological transcription length may also be inexact, especially at the transcription end (3\\'), so the transcription start and stop may be slightly different within the same sample, and this may not impact the function of the transcript at all (e.g. for coding transcripts, whenever the start/stop codon is not affected). It\\'s important to note that _Sensitivity_ and _Precision_ values at this _\"Intron chain level\"_ are calculated only by looking at multi-exon transcripts (for obvious reasons), so it completely ignores the single-exon transcripts, which are quite numerous in a RNA-Seq experiment (possibly due to a lot of transcriptional and alignment noise). ##### Transcript level At this level all transcripts (single and multi-exon) are compared to any overlapping reference transcripts in order to assess \"matching\" (TP). That means that besides the _\"Intron chain level\"_ matching which only applies to multi-exon transcripts, this analysis level also looks at single exon transcripts and considers them \"matching\" when there is a very significant overlap with a single exon reference transcript (while still allowing their boundaries to differ). Using --strict-match option can make the accuracy estimation at this level much more stringtent by only allowing a limited variation of the outer coordinates of the terminal exons (by at most 100 bases by default, but this value can be changed with the -e option). ##### Locus level At this level gffcompare considers that an observed locus (cluster of exon-overlapping transcripts) \"matches\" a similarly built reference locus (i.e. by clustering transcripts by exon overlaps), and thus counts as a TP at this level, if at least one predicted transcript has a \"Transcript level match\" with a reference transcript in the corresponding (overlapping) reference locus. ##### The \"super-locus\" concept The _XLOC__ prefix is assigned to _super-locus_ identifiers in many of the GffCompare output files. A **super-locus** is a region of the genome where predicted transcripts _and_ reference transcripts get clustered (grouped or linked) together by exon overlaps. When multiple samples (multiple GTF files with assembled transfrags) are provided as input to GffCompare, this clustering is performed across all the samples. Due to the transitive nature of this clustering by exon overlaps, these super-loci can occasionally get very large, sometimes merging a few distinct reference gene regions together -- especially if there is a lot of transcription or alignment noise around the individual gene regions. Not all super-loci have reference transcripts assigned, sometimes they are just a bunch of transfrags clumped together, possibly from multiple samples, all linked together by exon overlaps, with no reference transcripts present in that region. ### The \u201cunion\u201d of all transfrags in all assemblies: <outprefix>.combined.gtf When multiple input (query) GTF/GFF files are provided, Gffcompare reports a GTF file containing the \u201cunion\u201d of all transfrags in each sample. If a transfrag with the same exact intron chain is present in both samples, it is thus reported only once in the combined.gtf output ### Annotation of a single file: <outprefix>.annotated.gtf If a single GTF/GFF query file is provided as input and no specific options to remove \"duplicated\"/redundant transfrags were given ( -D, -S, -C, -A, -X ), GffCompare outputs a file called <outprefix> .annotated.gtf instead of <outprefix> .combined.gtf Its format is similar to the above, but preserves transcript IDs (so the -p option is ignored) ### Tracking transfrags through multiple samples: <outprefix>.tracking This file matches transcripts up between samples. This file matches transcripts up between samples. Each row represents a transcript structure that is preserved (structurally equivalent) across all the input GTF files. GffCompare considers transcripts \"matching\" (i.e. structurally equivalent) if all their introns are identical. Note that \"matching\" transcripts are allowed to differ on the length of the first and last exons, since these lengths can usually vary across samples for the same biological transcript. If GffCompare was run with the -r option, the 3rd column contains information about the reference annotation transcript that was found to be \"closest\" (best match or overlap) to the transcript structure represented by this row. Here\\'s an example of a line from the tracking file, where GffCompare was run with two input files vs. a reference annotation (the \\\\ characters here indicates that the next line should be in fact an extension of the current line) : TCONS_00403479 XLOC_006534 TCEA3|rna-XM_006710864.2 j \\\\ q1:STRG.377|STRG.377.2|10|0.304785|0.760185|2.205239|2767 \\\\ q2:STRG.382|STRG.382.3|10|0.650162|1.688384|3.312878|3016 In this example an assembled transcript present in the two input files (q1 and q2), called STRG.377.2 in the first and STRG.382.3 in the second, doesn\\'t fully match any reference transcript but shares some exon-intron junction(s) with reference transcript rna-XM_006710864.2 (which is an isoform of the reference gene TCEA3) and this type of reference overlap is indicated by the class code \\'j\\'. The first 4 columns of this file are fixed, as follows: Column number Column name Example Description 1 Query transfrag id TCONS_00403479 A unique internal id for the transfrag 2 Query locus id XLOC_006534 A unique internal id for the super-locus containing these transcripts across all samples and the reference annotation 3 Reference gene id TCEA3|rna-XM_006710864.2 The gene name and transcript ID of the reference record associated to this transcript (separated by \\'|\\'), or \\'-\\' if no such reference transcript is available 4 Class code c The type of overlap or relationship between the reference transcripts and the transcript structure represented by this row Each of the columns following the 4th column shows the transcript from each sample, in the following format (assuming that FPKM, TPM and coverage values were found in the transcript attributes): qJ:<gene_id>|<transcript_id>|<num_exons>|<FPKM>|<TPM>|<cov>|<len> A transcript need not be present in all samples in order to be reported in the tracking file. A sample not containing a structurally equivalent transcript will have a \\'-\\' in its corresponding column, so a transcript present in only one sample will have a \\'-\\' character in all the other columns corresponding to the other samples. **Note** : the .refmap and .tmap output files described below are created for each query <input_file> given, in the same directories where those input files reside. ### Transfrags matching each reference transcript: <outprefix>.<input_file>.refmap This tab-delimited file lists, for each reference transcript, which query transcript either fully or partially matches that reference transcript. Note that one such output file is generated for each of the query <input_files> given, in the directory where that input file resides (which might not be the current working directory). This file has one row per reference transcript, and the columns are as follows: Column number Column name Example Description 1 Reference gene Myog The gene_name attribute of the reference GTF record for this transcript, if present. Otherwise gene_id is used. 2 Reference transcript id uc007crl.1 The transcript_id attribute of the reference GTF record for this transcript 3 Class code c The type of match between the query transcripts in column 4 and the reference transcript. One of either \u2018c\u2019 for partial match, or \u2018=\u2019 for full match. 4 Matches STRG.223|STRG.223.1,STRG.224|STRG.224.1 A comma separated list of transcripts matching the reference transcript ### Best reference transcript for each transfrag: <outprefix>.<input_file>.tmap This tab delimited file lists the most closely matching reference transcript for each query transcript. Note that one such output file is generated for each of the query <input_files> given, in the directory where that input file resides (which might not be the current working directory). This file has one row per query transcript, and the columns are as follows: Column number Column name Example Description 1 Reference gene name Myog The gene_name attribute of the reference GTF record for this transcript, if present. Otherwise gene_id is used. 2 Reference transcript id uc007crl.1 The transcript_id attribute of the reference GTF record for this transcript 3 Class code c The type of relationship between the query transcripts in column 4 and the reference transcript (as described in the Class Codes section below) 4 Query gene id STRG.23567 The query (e.g., Stringtie) internal gene id 5 Query transcript id STRG.23567.0 The query internal transcript id 6 Number of exons 7 The number of exons in the query transcript 7 FPKM 1.4567 The expression of this transcript expressed in FPKM 8 TPM 0.000000 the estimated TPM for the transcript, if found in the query input file 9 Coverage 3.2687 The estimated average depth of read coverage across the transcript. 10 Length 1426 The length of the transcript 11 Major isoform ID STRG.23567.0 The query ID of the gene\\'s major isoform 12 Reference match length 4370 The length of the longest overlap with a reference, \u2018-\u2019 if there is no such exonic overlap ## Transcript classification codes If GffCompare was run with the -r option (i.e. comparing with a reference annotation), tracking rows will contain a \"class code\" value showing the relationship between a transfrag and the closest reference transcript (where applicable). If the -r option was not used the rows will all contain \u201c-\u201d in their class code column. The same codes are also shown as the value of the attribute \"class_code\" in the output GTF file. The class codes are shown below in decreasing order of their priority. ## Example: evaluating transcript discovery accuracy Gffcompare can be used to evaluate and compare the accuracy of transcript assemblers - in terms of their structural correctness (exon/intron coordinates). This assessment can even be performed in case of more generic \"transcript discovery\" programs like gene finders. The best way to do this would be to use a simulated data set (where the \"reference annotation\" is also the set of the expressed transcripts being simulated), but for well annotated reference genomes (human, mouse etc.), gffcompare can be used to evaluate and compare the general accuracy of isoform discovery programs on a real data set, using just the known (reference) annotation of that genome. As a practical example, let\\'s assume we ran both Cufflinks and StringTie on a mouse RNA-Seq sample and we want to compare the overall accuracy of the two programs. In order to compare the baseline _de novo_ transcript assembly accuracy, both Cufflinks and StringTie should be run without using any reference annotation data (i.e. no -G or -g options were used). Assuming that Cufflinks\\' transcript assembly output file name is cufflinks_asm.gtf and StringTie\\'s output is in stringtie_asm.gtf , while the reference annotation would be in a file called mm10.gff, the gffcompare commands would be: gffcompare -R -r mm10.gff -o cuffcmp cufflinks_asm.gtf gffcompare -R -r mm10.gff -o strtcmp stringtie_asm.gtf The -R option is used here in order to adjust the sensitivity calculation as to only consider the \"expressed\" genes in the sequencing sample -- for practical purposes, those are the reference genes for which GffCompare found at least one overlapping transfrag in the assembled data (*_asm.gtf file). (Of course this option would not be needed in the case of simulated RNA-Seq experiments if the reference transcripts were all \"expressed\"). Multiple output files will be generated by gffcompare, with the given prefix - in the example above, for the stringtie assemblies, the output files will be: strtcmp.annotated.gtf strtcmp.loci strtcmp.stats strtcmp.tracking strtcmp.stringtie_asm.gtf.refmap strtcmp.stringtie_asm.gtf.tmap Comparing the transcript assembly accuracy of the two programs is done by looking at the _Sensitivity_ and _Precision_ columns in the *.stats output files for each program (strtcmp.stats vs cuffcmp.stats as generated by the gffcompare commands above). That output looks like this (partially): #= Summary for dataset: stringtie_asm.gtf # Query mRNAs : 23555 in 17628 loci (17231 multi-exon transcripts) # (3731 multi-transcript loci, ~1.3 transcripts per locus) # Reference mRNAs : 16628 in 12062 loci (15850', 'multi-exon) # Super-loci w/ reference transcripts: 11552 #-----------------| Sensitivity | Precision | Base level: 82.4 | 76.5 | Exon level: 81.2 | 82.9 | Intron level: 86.1 | 94.8 | Intron chain level: 56.9 | 52.4 | Transcript level: 55.2 | 38.9 | Locus level: 70.1 | 48.0 | ## Query vs Reference transcripts GffCompare\\'s original purpose has been about assessing how accurately a set of \"novel\" (\"query\") transcripts matches a reference annotation. In order to properly evaluate precision and sensitivity when comparing two sets of transcripts, special care must be taken for \"duplicate\" or \"redundant\" entries within each set. GffCompare tries to detect and remove such duplicate transcripts, and it uses slightly different assumptions for the *reference* set (the one given with the -r option) compared to the \"query\" set. GffCompare is by default more strict when assessing redundancy in the reference set than in the query set. This \"stringency\" also affects differently single-exon transcripts (SETs) and multi-exon transcripts (METs): * When loading the reference transcripts:SETs (single exon transcripts) are considered \"duplicated\" (i.e. matching and redundant) if the shorter one is_fully contained_within the boundaries of the larger one AND if the shorter one is at least 80% of the length of the larger oneMETs are considered redundant if they have a fully matching intron chain AND if the boundaries of the shorter transcript are_fully contained_within the boundaries of the larger one (so the terminal exons of one transcript are both always longer or equal to the overlapping terminal exons of the other). * When loading the \"query\" transcripts:SETs are considered redundant using a fuzzy \"match\" assessment like this:the overlapping region is >= 80% of the length of the larger transcript OR ( >=70% of the length of larger transcript AND >= 80% of the length of the shorter transcript)The latter clause seems a bit redundant but that particular check is also used somewhere else in gffcompare, namely for the \"reverse containment\" cases when we wanted to relax the way we assess a \"match\" with a reference when the reference is shorter than the query and possibly \"almost contained\" within the query.METs are considered redundant simply when they have the exact_same intron chain_(same number of introns with the same intron coordinates); their terminal exons do not have to be \"boundary contained\" in any way. Because of these different ways of assessing \"redundancy\" within the set of query transcripts vs. the reference transcripts, one may find that running GffCompare of a set of transcripts versus itself may in some cases lead to accuracy numbers that are NOT all 100% in the stats output file, if that set has transcripts which might be found \"redundant\" differently when the file is loaded as a \"query\" set compared to when the same file is loaded as a _reference_ set. GffCompare can be forced to use the same \"redundancy\" criteria for both query and reference data sets by using the **-S** option which enforces the same \"strict checking\" of boundary containment etc. for the \"query\" data set as for the reference data set. ## Overlap classification for a large set of transcripts Some pipelines can produce a very large number of potential or partial transcripts ( _\"transfrags\"_ ), for example when merging the transcript assemblies from tens or hundreds of RNA-Seq experiments assemblies with stringtie --merge . Running GffCompare on such large GTF/GFF files could be slow and memory intensive (because GffCompare always loads the whole transcript data in memory for clustering etc.). One may only be interested to know _if_ and _how_ these many transcripts overlap the reference annotation, and then further analyze only those which have specific types of overlaps with the reference annotation transcripts (or none at all, i.e. if they do not overlap any of it, which may be the case for putative novel transcripts). That\\'s where the **trmap** program comes in. trmap reports for each query transcript all the reference that were found to overlap it, along with their overlap classification codes as described in the GffCompare documentation above. The main feature of trmap is that it allows streaming of a very large file with query transcripts (in GFF or BED format) to be checked and classified against a reference annotation (again, in GFF or BED format). **trmap** (\" **t** ranscript vs. **r** efererence **map** ping\") first loads the reference annotation file in memory as an interval tree and then streams the query file (which can also be provided at stdin) while checking and reporting any overlaps found, and classifies the relationship with reference transcripts using a (subset of) the _\"class codes\"_ like those assigned by GffCompare. _trmap_ is distributed with GffCompare and it shares the same overlap classification code. Usage: trmap [-S] [-o ] <ref_gff> <query_gff> Positional arguments: <ref_gff> reference annotation file name (GFF/BED format) <query_gff> query file name (GFF/BED format) or \"-\" for stdin Options: -o <outfile> write output to <outfile> instead of stdout -S report only simple reference overlap percentages, without classification (one line per query) The default output is a pseudo-FASTA format showing a record for each query transcript that had at least one reference overlap/relationship. Example: The query transcript (MSTRG.36.1 in this example) is shown in the header of the record, with space delimited fields showing the genomic location and strand. Each reference overlap follows, as a line with tab delimited fields, starting with the \"classification code\" for the overlap and then providing the genomic location of the transcript (chromosome, strand, transcript-start, transcript-end, reference_transcriptID, exons). The exons for both query and reference transcripts are shown as comma delimited lists of intervals. These are all 1-based coordinates like in the GTF/GFF format (even when input is BED). ## Obtaining GffCompare ### Publication Pertea G and Pertea M. [GFF Utilities: GffRead and GffCompare](https://doi.org/10.12688/f1000research.23297.1) . _F1000Research_ 2020, 9:304 [DOI: 10.12688/f1000research.23297.1](https://doi.org/10.12688/f1000research.23297.1)']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"kmc/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/kmc/cmd/None\",\n    \"name\": \"kmc\",\n    \"description\": [\n      \"KMC is a utility designed for counting k-mers (sequences of consecutive k symbols) in a set of reads from genome sequencing projects.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"http://sun.aei.polsl.pl/REFRESH/index.php?page=projects&project=kmc&subpage=about\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": null,\n        \"email\": \"sebastian.deorowicz@polsl.pl\",\n        \"maintainer\": false,\n        \"url\": \"http://sun.aei.polsl.pl/REFRESH/index.php?page=home\",\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8289510e2f3aa0b3b7d6e\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/kmc/cmd/3.2.1\",\n    \"name\": \"kmc\",\n    \"description\": [\n      \"KMC is a utility designed for counting k-mers (sequences\\nof consecutive k symbols) in a set of DNA sequences. KMC tools allow performing various operations on k-mers sets.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/refresh-bio/kmc\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/refresh-bio/kmc\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from http://sun.aei.polsl.pl/REFRESH/index.php?page=projects&project=kmc&subpage=about:\n```\nContent:\n['#### News **24 Dec 2024** Paper describing sc-SPLASH, SPLASH2 extension for single-cell data is deposited in bioRxiv.org. **23 Oct 2024** Paper about painting genomes using a colored de Bruijn graph is now at NAR GAB. **23 Sep 2024** Paper introducing SPLASH2 for unsupervised discovery from raw sequencing reads is published in Nature Biotechnology. **10 Jul 2024** Paper describing ProteStAr, our CIF/PDF file compressor is published in Bioinformatics. **2 Jul 2024** Preprint describing Vclust, ultrafast and accurate sequence alignment and clustering of viral genomes is deposited in bioRxiv.org. **25 May 2024** Preprint describing application of SPLASH2 for reference-free discoveries in plants is deposited in bioRxiv.org. **18 Mar 2024** AGC 3.1 (assembled genome compressor) is published. **26 Jan 2024** Paper describing new version of RECKONER, our read corrector is published in Scientific Reports. **22 Jan 2024** Paper describing discoveries made by SPLASH in microbial data is deposited in bioRxiv.']```"}, {"role": "user", "content": "Content from https://github.com/refresh-bio/kmc:\n```\nREADME content:\n['KMC = [![GitHub downloads](https://img.shields.io/github/downloads/refresh-bio/kmc/total.svg?style=flag&label=GitHub%20downloads)](https://github.com/refresh-bio/KMC/releases) [![Bioconda downloads](https://img.shields.io/conda/dn/bioconda/kmc.svg?style=flag&label=Bioconda%20downloads)](https://anaconda.org/bioconda/kmc) [![Biocontainer downloads](https://img.shields.io/endpoint?url=https%3A%2F%2Fmmseqs.com%2Fbiocontainer.php%3Fcontainer%3Dkmc)](https://biocontainers.pro/tools/kmc) [![GitHub Actions CI](../../actions/workflows/main.yml/badge.svg)](../../actions/workflows/main.yml) [![Join the chat at https://gitter.im/refresh-bio/KMC](https://badges.gitter.im/refresh-bio/KMC.svg)](https://gitter.im/refresh-bio/KMC?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) KMC is a disk-based program for counting k-mers from (possibly gzipped) FASTQ/FASTA files. KMC is one of many projects developed by [REFRESH Bioinformatics Group](http://sun.aei.polsl.pl/REFRESH/). For accessing k-mers stored in database produced by KMC there is an API (kmc_api directory). Note that for KMC versions 0.x and 1.x dababase format differs from produced by KMC version 2.x. From version 2.2.0 API is unified for both formats and all new features/bug fixes are present only for 2.x branch (standalone API for older KMC version is not longer under development, so new version of API should be used even for databases produced by older KMC version). Quick start = #### Getting the executable The simplest way to get the KMC is to download newest release for appropriate operating system from [KMC releases](https://github.com/refresh-bio/KMC/releases). #### Counting the k-mers from a single fastq file ``` ./kmc -k27 input.fastq 27mers . ``` The command above will count all the 27-mers occurring in ```input.fastq``` at least twice (configurable with ```-ci``` switch). The result will be stored in a KMC database, which is split into two files: ```27mers.kmc_pre``` and ```27mers.kmc_suf```. KMC will create hundreds of intermediate files. In the case of the above command, those will be created in the current working directory(the ```.``` at the end of the command). It may be more convinient to use dedicated directory for KMC temporary files, for example: ``` mkdir kmc_tmp # create directory for kmc temporary files ./kmc -k27 input.fastq 27mers kmc_tmp ``` #### Create text dump from KMC database binary format Having the k-mers counted it is possible to dump KMC binary database to textual form with ```kmc_tools```. ``` ./kmc_tools transform 27mers dump 27mers.txt ``` Installation details ##### Compile from sources ``` git clone --recurse-submodules https://github.com/refresh-bio/kmc.git cd kmc make -j32 ``` = The following libraries come with KMC in a binary (64-bit compiled for x86 platform) form. If your system needs other binary formats, you should put the following libraries in kmc_core/libs: * zlib - for support for gzip-compressed input FASTQ/FASTA files The following libraries come with KMC in a source coude form. * pybind11 - used to create python wrapper of KMC API (https://github.com/pybind/pybind11) If needed, you can also redefine maximal length of k-mer, which is 256 in the current version. Note: KMC is highly optimized and spends only as many bytes for k-mer (rounded up to 8) as necessary, so using large values of MAX_K does not affect the KMC performance for short k-mers. Some parts of KMC use C++17 features, so you need a compatible C++ compiler After that, you can run make to compile kmc and kmc_dump applications. ##### Additional infromation for MAC OS installation There might be a need to change g++ path in makefile_mac. If needed we recommend install g++ with brew (http://brew.sh/). Note that KMC creates a hundreds of temporary files, while default limit for opened files is small for under MAC OS platform. To increase this number use following command before running KMC: ulimit -n 2048 Directory structure = * bin - after compilation executables and libraries after compilation will be stored here * include - after compilation header file to use kmc core through the C++ API will be stored here * kmc_core - source code of kmc core library * kmc_CLI - source code of kmc command line interface * kmc_tools - source codes of kmc_tools program * kmc_core/libs - libraries used by KMC * kmc_api - C++ source codes implementing API to access KMC databases; must be used by any program that wants to process databases produced by kmc * kmc_dump - source codes of kmc_dump program listing k-mers in databases produced by kmc (deprecated, use kmc_tools instead) * py_kmc_api - python wrapper for kmc API * tests - tests files Use the KMC directly from code through the API = It is possible to use the KMC directly from C++ code through. Detailed API description is available at [wiki](https://github.com/refresh-bio/KMC/wiki/Use-the-KMC-directly-from-code-through-the-API) Python wrapper for KMC API = Python wrapper for KMC API was created using pybind11. **Warning:** python binding is experimental. The library used to create binding as well as public interface may change in the future. **Warning 2:** python wrapper for C++ KMC API is **much** slower (much, much more than I have been expecting) than native C++ API. In fact the first attempt to create python wrapper was to use [ctypes](https://docs.python.org/2/library/ctypes.html), but it turned out it was even slower than in case when pybind11 is used. The wrapper is designed and was tested only for python3. The main goal was to make it as similar to C++ API as possible. For this reason the API may be not [pythonic] (https://blog.startifact.com/posts/older/what-is-pythonic.html) enough for regular python programmer. Suggestions or pull requests to make it more robust are welcome. Python module wrapping KMC API must be compiled. * for windows there is a visual studio project (note that there will be probably the need to change include directories and library directories to point python include and libs location) * for linux or mac one should run ```make py_kmc_api``` As a result of pybind11 *.so file (for linux and mac os) or *.pyd (for windows) is created and may be used as a python module. *.pyd file is in fact DLL file, the only difference is its extension. * for windows following file is created: x64/Release/py_kmc_api.pyd * for linux/mac os the following file is created: bin/py_kmc_api`python3-config --extension-suffix` To be able to use this file one should make it visible for python. One way to do this is to extend PYTHONPATH environment variable. For linux/mac os one may just ``` source py_kmc_api/set_path.sh ``` while, for windows: ``` py_kmc_api\\\\set_path.bat ``` it will export apropriate file. The example of Python wrapper for KMC API is presented in file: py_kmc_api/py_kmc_dump.py Detailed API description is available at [wiki](https://github.com/refresh-bio/KMC/wiki/Python-wrapper-for-KMC-API) Binaries = After compilation you will obtain two binaries: * bin/kmc - the main program for counting k-mer occurrences * bin/kmc_dump - the program listing k-mers in a database produced by kmc * bin/kmc_tools - the program allowing to manipulate kmc databases (set operations, transformations, etc.) * bin/libkmc_core.a - compiled KMC code sources * py_kmc_api.cpython-39-x86_64-linux-gnu.so - compiled python wrapper for KMC API License = * KMC software distributed under GNU GPL 3 licence. * gzip is free, open-source * pybind11 (https://github.com/pybind/pybind11) is open-source (BDS-style license) In case of doubt, please consult the original documentations. Archival source codes, binaries and documentation = Archival source codes, binaries and documentation are available at [wiki](https://github.com/refresh-bio/KMC/wiki/Archive-versions-of-sources,-binaries-and-documentations). Warranty = THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ## Citing [Marek Kokot, Maciej D\u0142ugosz, Sebastian Deorowicz, KMC 3: counting and manipulating k-mer statistics, Bioinformatics, Volume 33, Issue 17, 01 September 2017, Pages 2759\u20132761, https://doi.org/10.1093/bioinformatics/btx304](https://academic.oup.com/bioinformatics/article/33/17/2759/3796399) [Sebastian Deorowicz, Marek Kokot, Szymon Grabowski, Agnieszka Debudaj-Grabysz, KMC 2: fast and resource-frugal k-mer counting, Bioinformatics, Volume 31, Issue 10, 15 May 2015, Pages 1569\u20131576, https://doi.org/10.1093/bioinformatics/btv022](https://academic.oup.com/bioinformatics/article/31/10/1569/177467) [Deorowicz, S., Debudaj-Grabysz, A. & Grabowski, S. Disk-based k-mer counting on a PC. BMC Bioinformatics 14, 160 (2013). https://doi.org/10.1186/1471-2105-14-160](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-160)']```"}, {"role": "user", "content": "Content from https://github.com/refresh-bio/kmc:\n```\nRepository metadata:\n{'name': 'KMC', 'label': ['KMC'], 'description': [{'term': 'Fast and frugal disk based k-mer counter', 'id': 0}], 'links': [], 'webpage': [{'term': '', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v3.2.4', 'v3.2.3', 'v3.2.2', 'v3.2.1', 'v3.2.0', 'v3.1.1', 'v3.1.1rc1', 'v3.1.0', 'v3.0.1', 'v3.0.0', '2.3', '2.2', '2.1.1'], 'license': [], 'repository': [{'term': 'https://github.com/refresh-bio/KMC', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'marekkokot', 'type': 'person', 'email': 'marek.koki@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'maciejdlugosz', 'type': 'person', 'email': 'maciej.dlugosz@polsl.pl', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Tommi M\u00e4klin', 'type': 'person', 'email': 'tommi@maklin.fi', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Marco Matthies', 'type': 'person', 'email': '71844+marcom@users.noreply.github.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Marek Kokot', 'type': 'person', 'email': 'mkokot@mac-mini-1.local', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Sebastian Deorowicz', 'type': 'person', 'email': 'sebastian.deorowicz@polsl.pl', 'maintainer': False}, 'id': 5}, {'term': {'name': 'The Gitter Badger', 'type': 'person', 'email': 'badger@gitter.im', 'maintainer': False}, 'id': 6}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/refresh-bio/kmc/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"esco/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconductor/esco/lib/0.99.12\",\n    \"name\": \"esco\",\n    \"description\": [\n      \"This package borrowed the infrastructure from package splatter, which is a package for the simulation of single-cell RNA sequencing count data. This new pacakge allows users to simulate single cell data with any given correlations among genes, while allowing discrete / tree-structured cell groups and smooth trajectories. It provides a simple interface for creating complex simulations that are reproducible and well-documented. Most parameters can be estimated from real data and functions are provided for comparing and visualizing real and simulated datasets.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://git.bioconductor.org/packages/ESCO\",\n        \"kind\": \"bioconductor\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/ESCO\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [\n      {\n        \"type\": null,\n        \"name\": \"Jinjin Tian\",\n        \"email\": \"jinjint@andrew.cmu.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [],\n    \"source\": [\n      \"bioconductor\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/esco/lib/None\",\n    \"name\": \"esco\",\n    \"description\": [\n      \"ESCO is an R package for the simulation of single-cell RNA sequencing data with special consideration of gene co-expression.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/JINJINT/SplatterESCO\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Kathryn Roeder\",\n        \"email\": \"roeder@andrew.cmu.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82e6710e2f3aa0b3ba4be\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/ESCO:\n```\nContent:\n['# Removed Packages A list of packages removed from Bioconductor along with their last-available landing pages. ## Packages deprecated in Bioconductor 3.21 (to be removed in 3.22) Software Packages: Experiment Data Packages: Annotation Data Packages: Workflow Packages: ## Packages removed with Bioconductor 3.21 release Software Packages: Note: [zlibbioc](/packages/3.19/bioc/html/zlibbioc.html) is deprecated but will be given an extra 6 months (removed in 3.22) to allow multiple dependencies to adapt. Experiment Data Packages: ## Packages removed with Bioconductor 3.20 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.19 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.18 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.17 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.16 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Packages: ## Packages removed with Bioconductor 3.15 release Software Packages: Experiment Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.14 release Software Packages: Experiment Data Packages: Workflow Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.13 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.12 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.11 release Software Packages: Experiment Data Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.10 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.9 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.8 release Software Packages: Experiment Data Packages: ## Packages removed with Bioconductor 3.7 release Software Packages: Annotation Data Packages: ## Packages removed with Bioconductor 3.6 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.5 release Software Packages: Experimental Data Packages: Annotation Packages: ## Packages removed with Bioconductor 3.4 release Software Packages: Experimental Data Packages: ## Packages removed with Bioconductor 3.3 release No packages were removed with this release. 17 packages were marked as deprecated, to be removed in the next release. ## Packages removed with Bioconductor 3.2 release No packages were removed with this release. ## Packages removed with Bioconductor 3.1 release ## Packages removed with Bioconductor 3.0 release ## Packages removed with Bioconductor 2.14 release ## Packages removed with Bioconductor 2.13 release ## Packages removed with Bioconductor 2.12 release ## Packages removed with Bioconductor 2.11 release ## Packages removed with Bioconductor 2.10 release ## Packages removed with Bioconductor 2.9 release ## Packages removed with Bioconductor 2.8 release ## Packages removed with Bioconductor 2.7 release ## Packages removed with Bioconductor 2.6 release ## Packages removed with Bioconductor 2.5 release ## Packages removed with Bioconductor 2.4 release ## Packages removed with Bioconductor 2.3 release ## Packages removed with Bioconductor 2.2 release ## Packages removed with Bioconductor 2.1 release ## Packages removed with Bioconductor 2.0 release ## Packages removed with Bioconductor 1.9 release ## Packages removed with Bioconductor 1.8 release ## Packages removed with Bioconductor 1.7 release']```"}, {"role": "user", "content": "Content from https://github.com/JINJINT/SplatterESCO:\n```\nREADME content:\n['# ESCO Welcome to ``ESCO``! ``ESCO`` is an R package for the simulation of single-cell RNA sequencing data with special consideration of gene co-expression, built ultilizing the infrastructure in the [`splatter`](https://github.com/Oshlack/splatter) package. ## Install from Github This package can be installed with R package devtools: ```{r} library(\"devtools\") devtools::install_github(\"JINJINT/ESCO\") ``` ## Quick start: For a simple example to simulate 100 genes and 50 cells of one cell group with gene co-expression: ```{r} library(ESCO) #===== start simulation ======# sim <- escoSimulateSingle(nGenes = 100, nCells = 50, withcorr = TRUE, verbose = FALSE) #===== access the data ======# datalist = list(\"simulated truth\"=assays(sim)$TrueCounts, \"zero-inflated\" = assays(sim)$counts, \"down-sampled\" = assays(sim)$observedcounts) #====== plot the data ======# heatdata(datalist, norm = FALSE, size = 2, ncol = 3) #====== plot the Gene correlation ======# # object that saved all simulation configurations simparams = metadata(sim)$Params # object that particularly saved the correlation structure rholist = slot(simparams,\"corr\") # arrange the true correlation and simulated correlation corrgenes = rownames(rholist[[1]]) gcnlist = lapply(datalist, function(data)gcn(data, genes = corrgenes)) gcnlist = append(gcnlist, list(\"given truth\" = rholist[[1]]), 0) heatgcn(gcnlist, size = 3, ncol = 4) ``` For more complicated examples of simulating multiple cell groups and even trees and trajectories with gene co-expression, please check out the [vignettes](https://www.dropbox.com/s/ly1x20c7bommsvi/esco.html?dl=0), which can also be built locally if installed by ```{r} devtools::install_github(\"JINJINT/ESCO\", build_vignettes=TRUE) ``` ## Reference: Check out our paper for ESCO here: [ESCO: single cell expression simulation incorporating gene co-expression.](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btab116/6149079?guestAccessKey=64c91aa4-1d5e-42da-92df-678b1b08af79)']```"}, {"role": "user", "content": "Content from https://github.com/JINJINT/SplatterESCO:\n```\nRepository metadata:\n{'name': 'ESCO', 'label': ['ESCO'], 'description': [{'term': 'Single cell simulator with gene co-expression', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/JINJINT/ESCO', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Jinjin Tian', 'type': 'person', 'email': '42779536+JINJINT@users.noreply.github.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Jinjin Tian', 'type': 'person', 'email': 'jinjint@andrew.cmu.edu', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/JINJINT/ESCO/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://git.bioconductor.org/packages/ESCO:\n```\nRepository metadata:\n{'url': 'https://git.bioconductor.org/packages/ESCO'}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"a5/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/a5/cmd/None\",\n    \"name\": \"a5\",\n    \"description\": [\n      \"A5 is an integrative pipeline for genome assembly that automates sequence data cleaning, error correction, assembly, and quality control by chaining a number of programs together with additional custom algorithms.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://sourceforge.net/projects/ngopt/\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Aaron E. Darling\",\n        \"email\": \"aarondarling@ucdavis.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c7a6add3b8acd1a2f9b616\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/a5-miseq/cmd/20160825\",\n    \"name\": \"a5-miseq\",\n    \"description\": [\n      \"A5-miseq is a pipeline for assembling DNA sequence data generated on the Illumina sequencing platform. This README will take you through the steps necessary for running _A5-miseq_.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://sourceforge.net/projects/ngop\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/ngop:\n```\nProject metadata:\n{'description': None, 'sections': []}```"}, {"role": "user", "content": "Content from https://sourceforge.net/projects/ngopt/:\n```\nProject metadata:\n{'description': 'de novo assembly & analysis of Illumina sequence data, including the A5 pipeline, A5-miseq, tools to evaluate assembly quality, and scripts to facilitate data submission to NCBI and the RAST annotation system', 'sections': [{'text': 'Project Activity\\nSee All Activity >\\nProject Activity', 'hrefs': ['https://sourceforge.net/auth/?return_to=%2Fprojects%2Fngopt%2F', '/p/ngopt/activity/feed', '/p/ngopt/activity/']}, {'text': 'Follow ngopt\\nngopt Web Site\\nFollow ngopt\\nngopt Web Site', 'hrefs': ['https://ngopt.sourceforge.io/']}, {'text': 'User Reviews\\nBe the first to\\npost a review\\nof ngopt!\\nUser Reviews\\nBe the first to\\npost a review\\nof ngopt!', 'hrefs': ['/projects/ngopt/reviews/new']}, {'text': 'Additional Project Details\\nRegistered\\n2014-04-02\\nAdditional Project Details\\nRegistered\\n2014-04-02', 'hrefs': []}]}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"rcorrector/cmd": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/rcorrector/cmd/1.0.6\",\n    \"name\": \"rcorrector\",\n    \"description\": [\n      \"Rcorrector (RNA-seq error CORRECTOR) is a kmer-based error correction method for RNA-seq data. Rcorrector can also be applied to other type of sequencing data where the read coverage is non-uniform, such as single-cell sequencing.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/mourisl/Rcorrector\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [\n      \"https://github.com/mourisl/Rcorrector/\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"toolshed/rcorrector/cmd/1.0.3+galaxy1\",\n    \"name\": \"rcorrector\",\n    \"description\": [\n      \"A kmer-based error correction method for RNA-seq data.\"\n    ],\n    \"repository\": [\n      {\n        \"url\": \"https://github.com/harvardinformatics/TranscriptomeAssemblyTools\",\n        \"kind\": \"github\",\n        \"source_hasAnonymousAccess\": null,\n        \"source_isDownloadRegistered\": null,\n        \"source_isFree\": null,\n        \"source_isRepoAccessible\": null\n      }\n    ],\n    \"webpage\": [],\n    \"license\": [],\n    \"authors\": [],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82a4410e2f3aa0b3b8c4c\"\n      }\n    ],\n    \"source\": [\n      \"toolshed\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/mourisl/Rcorrector:\n```\nREADME content:\n['Rcorrector ========= Described in: Song, L., Florea, L., [Rcorrector: Efficient and accurate error correction for Illumina RNA-seq reads](http://www.gigasciencejournal.com/content/4/1/48). GigaScience. 2015, 4:48. Copyright (C) 2012-2013, and GNU GPL, by Li Song and Liliana Florea Rcorrrector includes the program [Jellyfish2](http://www.genome.umd.edu/jellyfish.html) ### What is Rcorrector? Rcorrector(RNA-seq error CORRECTOR) is a kmer-based error correction method for RNA-seq data. Rcorrector can also be applied to other type of sequencing data where the read coverage is non-uniform, such as single-cell sequencing. ### Install 1. Clone the [GitHub repo](https://github.com/mourisl/rcorrector), e.g. with `git clone https://github.com/mourisl/rcorrector.git` 2. Run `make` in the repo directory During the `make` procedure, the script will check whether you have jellyfish2 in $PATH. If not, it will download and compile jellyfish2 from its repository. ### Usage Usage: perl run_rcorrector.pl [OPTIONS] OPTIONS: Required -s seq_files: comma separated files for single-end data sets -1 seq_files_left: comma separated files for the first mate in the paried-end data sets -2 seq_files_right: comma separated files for the second mate in the paired-end data sets -i seq_files_interleaved: comma sperated files for interleaved paired-end data sets Optional -k INT: kmer_length (<=32, default: 23) -od STRING: output_file_directory (default: ./) -t INT: number of threads to use (default: 1) -maxcorK INT: the maximum number of correction within k-bp window (default: 4) -wk FLOAT: the proportion of kmers that are used to estimate weak kmer count threshold, lower for more divergent genome (default: 0.95) -ek INT: expected number of kmers; does not affect the correctness of program but affects the memory usage (default: 100000000) -stdout: output the corrected reads to stdout (default: not used) -tmpd temp_file_directory: directory for tempoary files (default: ./) -verbose: output some correction information to stdout (default: not used) -stage INT: start from which stage (default: 0) 0-start from begining(storing kmers in bloom filter) ; 1-start from count kmers showed up in bloom filter; 2-start from dumping kmer counts into a jf_dump file; 3-start from error correction. ### Output For each input file, Rcorrector will generate the corresponding output file with \"*.cor.fq/fa\" in the directory specified by \"-od\". In the header line for each read, Rcorrector will append some information. \"cor\": some bases of the sequence are corrected \"unfixable_error\": the errors could not be corrected \"l:INT m:INT h:INT\": the lowest, median and highest kmer count of the kmers from the read ### Example We put a small sample data set, you can run them by: perl run_rcorrector.pl -1 Sample/sample_read1.fq -2 Sample/sample_read2.fq ### Contact lsong10@jhu.edu ### Terms of use This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received (LICENSE.txt) a copy of the GNU General Public License along with this program; if not, you can obtain one from http://www.gnu.org/licenses/gpl.txt or by writing to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA ### Support Create a [GitHub issue](https://github.com/mourisl/rcorrector/issues).']```"}, {"role": "user", "content": "Content from https://github.com/mourisl/Rcorrector:\n```\nRepository metadata:\n{'name': 'Rcorrector', 'label': ['Rcorrector'], 'description': [{'term': 'Error correction for Illumina RNA-seq reads', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.0.7', 'v1.0.6', 'v1.0.5', 'v1.0.4', 'v1.0.3.1', 'v1.0.3', 'v1.0.2', 'v1.0.1', 'v1.0.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/mourisl/Rcorrector', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'mourisl', 'type': 'person', 'email': 'mourisl@hotmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Li Song', 'type': 'person', 'email': 'mourisl@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Raymond Underwood', 'type': 'person', 'email': '36328498+raymond-u@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Karolis Ramanauskas', 'type': 'person', 'email': 'karolisr@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Li Song', 'type': 'person', 'email': 'lsong10@jhu.edu', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Li Song', 'type': 'person', 'email': 'lsong@gwln1.pha.jhu.edu', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Li', 'type': 'person', 'email': 'lsong10@jhu.edu@login-node04.cm.cluster', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Chris Boursnell', 'type': 'person', 'email': 'cmb211@cam.ac.uk', 'maintainer': False}, 'id': 7}, {'term': {'name': 'Li Song', 'type': 'person', 'email': 'lsong@igm3.(none)', 'maintainer': False}, 'id': 8}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/mourisl/Rcorrector/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/harvardinformatics/TranscriptomeAssemblyTools:\n```\nREADME content:\n['# TranscriptomeAssemblyTools A collection of scripts for pre-processing fastq files prior to *de novo* transcriptome assembly, as well as slurm scripts for running what we view as a reasonable best practice workflow, the steps of which are: 1. run *fastqc* on raw fastq reads to identify potential issues with Illumina sequencing libraries such as retained adapter, over-represented (often low complexity) sequences, greater than expected decrease in base quality with read cycle 1. perform kmer-based read corrections with with [rCorrector](https://github.com/mourisl/Rcorrector), see [Song and Florea 2015, Gigascience](https://gigascience.biomedcentral.com/articles/10.1186/s13742-015-0089-y) 1. remove read pairs where at least one read has been flagged by *rCorrector* as containing an erroneous kmer, and where it was not possible to computationally correct the errors 1. remove read pairs where at least read contains an over-represented sequence 1. perform light quality trimming with [TrimGalore](https://github.com/FelixKrueger/TrimGalore) 1. optionally, to map the remaining reads to the [SILVA rRNA database](https://www.arb-silva.de/), then filtering read pairs where at least one read aligns to an rRNA sequence 1. assembly reads with [Trinity](https://github.com/trinityrnaseq/trinityrnaseq) From our years of experience troubleshooting and evaluating *de novo* transcriptome assemblies, we have identified a number of statistical issues regarding the robustness of downstream analyses based upon them. A summary of these issues can be found at [Freedman et al. 2020, *Molecular Ecology Resources*](https://onlinelibrary.wiley.com/doi/abs/10.1111/1755-0998.13156). Given these issues and the rapidly decreasing cost of generating a genome assembly, we suggest that during the study design phase of your project, you consider the feasibility of assembling and annotating a genome before choosing to generate a *de novo* transcriptome assembly. ## Workflow steps To the extent possible, to improve reproducibility (not to mention ease of implementation!), we run analyses from within conda environments. Below, we explain how to execute particular steps assuming that a separate conda enviornment is created for each step, with job scripts designed to be run on the SLURM job scheduler. These can easily be modified to work with other schedulers such as SGE and LSF. ### 1. Running fastqc *Fastqc* generated a bunch of quality metrics such as quality-by-cycle, frequency by cycle, of adapter sequence, and overrepresentation of sequences. This information can provide some initial information about potential issues with sequencing library quality, and may highlight the importance of particular downstream filtering steps. We can create a conda environment for *fastqc* as follows ```bash conda create -n fastqc -c bioconda fastqc ``` Then, we can run *fastqc*. Assuming we run each file as a separate job, the command line looks like this: ```bash fastqc --outdir `pwd`/fastqc <name of your fastq file> ``` where *--outdir* simply indicates where you want to store the output. In this case, we are storing it in a directory called fastqc nested inside the current working directory. This command can be submitted as a SLURM job with [fastqc.sh](https://github.com/harvardinformatics/TranscriptomeAssemblyTools/blob/master/job_scripts/fastqc.sh) ```bash sbatch fastqc.sh <name of fastq file> ``` Note, if one supplies more threads to *fastqc*, with the *-t* switch, one can supply multiple fastq files at once. When choosing whether or not to run *fastqc* in multi-threaded mode, remember that the program only allocates 1 thread per file, i.e. there is no benefit to specifying more than one thread when only one file is being quality-checked. ### 2. Kmer-based error corrections with *rCorrector* For a given RNA-seq experiment, kmers observed in reads very rarely are likely to be those containing (at least one) sequencing error. Such errors can negatively impact the quality of a *de novo* transcsriptome assembly, and so ideally we either corrector the errors or remove erroneous reads that are computationally \"uncorrectable\". We can do this with *rCorrector*, a bioinformatics tool that first builds a kmer library for a set of reads, then identifies rare kmers, attempts to find a more frequent kmer that doesn\\'t differ by it too much, correct the read to the more frequent kmer. If there isn\\'t a detectable correction for a flagged rare kmer, *rCorrector* flags it as unfixable. *rCorrector* takes as input all of the paired-end reads that are to be used to generate the assembly. We can create a conda environment as follows: ```bash conda create -n rcorrector -c bioconda rcorrector ``` Then, we can run the following command line: ```bash run_rcorrector.pl -t 16 -1 <comma-separated list of R1 files> -2 <comma-separated list of R2 files> ``` where \"-t\" is a command line switch to specify how many threads to use in the analysis. This command can be submitted as a SLURM job with [rcorrector.sh](https://github.com/harvardinformatics/TranscriptomeAssemblyTools/blob/master/job_scripts/rcorector.sh) ```bash sbatch rcorrector.sh <comma-separated list of R1 files> <comma-separated list of R2 files> ```']```"}, {"role": "user", "content": "Content from https://github.com/harvardinformatics/TranscriptomeAssemblyTools:\n```\nRepository metadata:\n{'name': 'TranscriptomeAssemblyTools', 'label': ['TranscriptomeAssemblyTools'], 'description': [{'term': 'A collection of scripts for processing fastq files in ways to improve de novo transcriptome assemblies, and for evaluating those assemblies.', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [], 'repository': [{'term': 'https://github.com/harvardinformatics/TranscriptomeAssemblyTools', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'afreedman', 'type': 'person', 'email': 'adamfreedman@fas.harvard.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Leo Guan', 'type': 'person', 'email': '56712064+Leo-Guan-git@users.noreply.github.com', 'maintainer': False}, 'id': 1}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/harvardinformatics/TranscriptomeAssemblyTools/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://github.com/mourisl/Rcorrector/:\n```\nREADME content:\n['Rcorrector ========= Described in: Song, L., Florea, L., [Rcorrector: Efficient and accurate error correction for Illumina RNA-seq reads](http://www.gigasciencejournal.com/content/4/1/48). GigaScience. 2015, 4:48. Copyright (C) 2012-2013, and GNU GPL, by Li Song and Liliana Florea Rcorrrector includes the program [Jellyfish2](http://www.genome.umd.edu/jellyfish.html) ### What is Rcorrector? Rcorrector(RNA-seq error CORRECTOR) is a kmer-based error correction method for RNA-seq data. Rcorrector can also be applied to other type of sequencing data where the read coverage is non-uniform, such as single-cell sequencing. ### Install 1. Clone the [GitHub repo](https://github.com/mourisl/rcorrector), e.g. with `git clone https://github.com/mourisl/rcorrector.git` 2. Run `make` in the repo directory During the `make` procedure, the script will check whether you have jellyfish2 in $PATH. If not, it will download and compile jellyfish2 from its repository. ### Usage Usage: perl run_rcorrector.pl [OPTIONS] OPTIONS: Required -s seq_files: comma separated files for single-end data sets -1 seq_files_left: comma separated files for the first mate in the paried-end data sets -2 seq_files_right: comma separated files for the second mate in the paired-end data sets -i seq_files_interleaved: comma sperated files for interleaved paired-end data sets Optional -k INT: kmer_length (<=32, default: 23) -od STRING: output_file_directory (default: ./) -t INT: number of threads to use (default: 1) -maxcorK INT: the maximum number of correction within k-bp window (default: 4) -wk FLOAT: the proportion of kmers that are used to estimate weak kmer count threshold, lower for more divergent genome (default: 0.95) -ek INT: expected number of kmers; does not affect the correctness of program but affects the memory usage (default: 100000000) -stdout: output the corrected reads to stdout (default: not used) -tmpd temp_file_directory: directory for tempoary files (default: ./) -verbose: output some correction information to stdout (default: not used) -stage INT: start from which stage (default: 0) 0-start from begining(storing kmers in bloom filter) ; 1-start from count kmers showed up in bloom filter; 2-start from dumping kmer counts into a jf_dump file; 3-start from error correction. ### Output For each input file, Rcorrector will generate the corresponding output file with \"*.cor.fq/fa\" in the directory specified by \"-od\". In the header line for each read, Rcorrector will append some information. \"cor\": some bases of the sequence are corrected \"unfixable_error\": the errors could not be corrected \"l:INT m:INT h:INT\": the lowest, median and highest kmer count of the kmers from the read ### Example We put a small sample data set, you can run them by: perl run_rcorrector.pl -1 Sample/sample_read1.fq -2 Sample/sample_read2.fq ### Contact lsong10@jhu.edu ### Terms of use This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received (LICENSE.txt) a copy of the GNU General Public License along with this program; if not, you can obtain one from http://www.gnu.org/licenses/gpl.txt or by writing to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA ### Support Create a [GitHub issue](https://github.com/mourisl/rcorrector/issues).']```"}, {"role": "user", "content": "Content from https://github.com/mourisl/Rcorrector/:\n```\nRepository metadata:\n{'name': 'Rcorrector', 'label': ['Rcorrector'], 'description': [{'term': 'Error correction for Illumina RNA-seq reads', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': ['v1.0.7', 'v1.0.6', 'v1.0.5', 'v1.0.4', 'v1.0.3.1', 'v1.0.3', 'v1.0.2', 'v1.0.1', 'v1.0.0'], 'license': [{'term': {'name': 'GNU General Public License v3.0', 'url': 'http://choosealicense.com/licenses/gpl-3.0/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/mourisl/Rcorrector', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'mourisl', 'type': 'person', 'email': 'mourisl@hotmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Li Song', 'type': 'person', 'email': 'mourisl@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Raymond Underwood', 'type': 'person', 'email': '36328498+raymond-u@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'Karolis Ramanauskas', 'type': 'person', 'email': 'karolisr@gmail.com', 'maintainer': False}, 'id': 3}, {'term': {'name': 'Li Song', 'type': 'person', 'email': 'lsong10@jhu.edu', 'maintainer': False}, 'id': 4}, {'term': {'name': 'Li Song', 'type': 'person', 'email': 'lsong@gwln1.pha.jhu.edu', 'maintainer': False}, 'id': 5}, {'term': {'name': 'Li', 'type': 'person', 'email': 'lsong10@jhu.edu@login-node04.cm.cluster', 'maintainer': False}, 'id': 6}, {'term': {'name': 'Chris Boursnell', 'type': 'person', 'email': 'cmb211@cam.ac.uk', 'maintainer': False}, 'id': 7}, {'term': {'name': 'Li Song', 'type': 'person', 'email': 'lsong@igm3.(none)', 'maintainer': False}, 'id': 8}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'readme', 'url': 'https://github.com/mourisl/Rcorrector/blob/main/README.md'}, 'id': 0}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bioconductor-multiwgcna/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bioconductor-multiwgcna/lib/1.0.0\",\n    \"name\": \"bioconductor-multiwgcna\",\n    \"description\": [\n      \"An R package for deeping mining gene co-expression networks in multi-trait expression data. Provides functions for analyzing, comparing, and visualizing WGCNA networks across conditions. multiWGCNA was designed to handle the common case where there are multiple biologically meaningful sample traits, such as disease vs wildtype across development or anatomical region.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/3.18/bioc/html/multiWGCNA.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"GPL-3.0-only\",\n        \"url\": \"https://spdx.org/licenses/GPL-3.0-only.html\"\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/multiwgcna/lib/None\",\n    \"name\": \"multiwgcna\",\n    \"description\": [\n      \"R package for deep mining gene co-expression networks in multi-trait expression data.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/fogellab/multiWGCNA\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Brent L. Fogel\",\n        \"email\": \"bfogel@ucla.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      },\n      {\n        \"type\": \"Person\",\n        \"name\": \"Dario Tommasini\",\n        \"email\": null,\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c8383e10e2f3aa0b3bcd39\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/fogellab/multiWGCNA:\n```\nREADME content:\n['# multiWGCNA: an R package for deep mining gene co-expression networks in multi-trait expression data The multiWGCNA R package builds on the existing weighted gene co-expression network analysis (WGCNA) package by extending workflows to expression data with two dimensions. multiWGCNA is especially useful for the study of disease-associated modules across time or space. For more information, please see the multiWGCNA paper available at https://doi.org/10.1186/s12859-023-05233-z. # Installation The multiWGCNA R package can be installed from Bioconductor like this: ``` if (!require(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"multiWGCNA\") ``` The development version of multiWGCNA can be installed from GitHub like this: ``` if (!require(\"devtools\", quietly = TRUE)) install.packages(\"devtools\") devtools::install_github(\"fogellab/multiWGCNA\") ``` # Vignettes We recommend running through both of the vignettes before applying multiWGCNA to your own data: * The autism_full_workflow.Rmd vignette provides a quick example of how to use multiWGCNA. * The astrocyte_map2.Rmd vignette provides a more in-depth tutorial discussing the preservation analyses from the manuscript and functions for visualization. # Citation To cite multiWGCNA in publications, please use: Tommasini, D, Fogel, BL (2023). multiWGCNA: an R package for deep mining gene co-expression networks in multi-trait expression data. BMC Bioinformatics, 24, 1:115. For LaTeX users, a BibTeX entry is available here: ``` @Article{, title = {multiWGCNA: an R package for deep mining gene co-expression networks in multi-trait expression data}, author = {Dario Tommasini and Brent L. Fogel}, journal = {BMC Bioinformatics}, year = {2023}, volume = {24}, number = {1}, pages = {115}, } ```']```"}, {"role": "user", "content": "Content from https://github.com/fogellab/multiWGCNA:\n```\nRepository metadata:\n{'name': 'multiWGCNA', 'label': ['multiWGCNA'], 'description': [{'term': 'an R package for deep mining gene co-expression networks in multi-trait expression data', 'id': 0}], 'links': [], 'webpage': [], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [], 'repository': [{'term': 'https://github.com/fogellab/multiWGCNA', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'dariotommasini', 'type': 'person', 'email': 'dtommasini0@gmail.com', 'maintainer': False}, 'id': 0}, {'term': {'name': 'J Wokaty', 'type': 'person', 'email': 'jwokaty@users.noreply.github.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'Dario Tommasini', 'type': 'person', 'email': '103676484+dariotommasini@users.noreply.github.com', 'maintainer': False}, 'id': 2}, {'term': {'name': 'J Wokaty', 'type': 'person', 'email': 'jennifer.wokaty@sph.cuny.edu', 'maintainer': False}, 'id': 3}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'root', 'url': 'https://github.com/fogellab/multiWGCNA/blob/main/NEWS.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/fogellab/multiWGCNA/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/3.18/bioc/html/multiWGCNA.html:\n```\nContent:\n['This package is for version 3.18 of Bioconductor; for the stable, up-to-date release version, see [multiWGCNA](/packages/multiWGCNA/) . ## multiWGCNA **Bioconductor version:** 3.18 An R package for deeping mining gene co-expression networks in multi-trait expression data. Provides functions for analyzing, comparing, and visualizing WGCNA networks across conditions. multiWGCNA was designed to handle the common case where there are multiple biologically meaningful sample traits, such as disease vs wildtype across development or anatomical region. **Author:** Dario Tommasini [aut, cre] <https://orcid.org/0000-0002-1214-6547> , Brent Fogel [aut, ctb] **Maintainer:** Dario Tommasini <dtommasini0 at gmail.com> **Citation (from within R, entercitation(\"multiWGCNA\")):** ### Installation To install this package, start R (version \"4.3\") and enter: if (! require ( \"BiocManager\" , quietly = TRUE )) install. packages ( \"BiocManager\" ) BiocManager :: install ( \"multiWGCNA\" ) For older versions of R, please refer to the appropriate [Bioconductor release](/about/release-announcements/) . ### Documentation To view documentation for the version of this package installed in your system, start R and enter: browseVignettes (\"multiWGCNA\") ### Details biocViews [Clustering](../../BiocViews.html#___Clustering) , [DifferentialExpression](../../BiocViews.html#___DifferentialExpression) , [GeneExpression](../../BiocViews.html#___GeneExpression) , [RNASeq](../../BiocViews.html#___RNASeq) , [Regression](../../BiocViews.html#___Regression) , [Sequencing](../../BiocViews.html#___Sequencing) , [Software](../../BiocViews.html#___Software) Version 1.0.0 In Bioconductor since BioC 3.18 (R-4.3) (0.5 years) License GPL-3 Depends R (>= 4.3.0), ggalluvial Imports stringr, readr, WGCNA, dplyr, reshape2, data.table, patchwork, scales, igraph, flashClust, ggplot2, [dcanr](../../bioc/html/dcanr.html) , cowplot, ggrepel, methods, [SummarizedExperiment](../../bioc/html/SummarizedExperiment.html) System Requirements URL <> See More ### Package Archives Follow [Installation](/install/#install-bioconductor-packages) instructions to use this package in your R session.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
{"bioconductor-dune/lib": [{"role": "user", "content": "### Task: Software Metadata Verification\n\nI am integrating software metadata from multiple sources. I have two metadata entries that may or may not belong to the same software.\n\nYour task is to compare one entry against the other and determine whether they refer to the same tool.\n\n### Input Format\n\nYou will receive inputs in **multiple parts**. Each part may include:\n\n- JSON-formatted metadata for a software tool\n- Extracted content from the tool\u2019s repository (e.g., README files) or webpage (e.g., HTML, project descriptions). **These contents are provided explicitly because you do not have internet access.** Treat all provided content as final and in-scope. Do not assume any external URLs can be accessed.\n\nThe two tools are introduced as:\n- `\"The first software metadata_entry\"` \n- `\"The second software metadata_entry\"` \n\n### Processing Instructions\n\n1. Compare the metadata of each tool, prioritizing:\n   - repository URLs\n   - webpages\n   - descriptions\n   - documentation content (e.g., README)\n   - authorship or contact info\n   - associated publications or citations\n\n2. **IMPORTANT**: The tool names are often the same across entries. This is expected and should **not be used alone** to decide if they refer to the same software.\n\n3. Carefully analyze the provided repository or webpage content. Look for:\n   - Link similarity (e.g., GitHub, SourceForge)\n   - Shared or related descriptions\n   - Overlapping contributors, emails, or institutions\n   - Common citations or programming languages\n   - Usage instructions or tool behavior that match\n\n4. Wait until the final message says:\n\n   **\"All parts have been sent. Please now analyze the entries and provide the output as specified.\"**\n\nOnly then should you perform the analysis.\n\n5. Your response must be a Python dictionary with the following keys:\n   - `verdict`: one of `\"Same\"`, `\"Different\"`, or `\"Unclear\"`\n   - `explanation`: 2\u20133 sentences explaining your decision, based only on the provided data\n   - `confidence`: one of `\"high\"`, `\"medium\"`, or `\"low\"`\n   - `features`: a list of metadata features you relied on (e.g., `\"repo match\"`, `\"shared authors\"`)\n\nDo not use generic templates or placeholder text. Your answer must reflect the actual input.\n\n### Output Format\n\nUse this structure for your answer:\n\n```python\n{\n  \"verdict\": \"<Same | Different | Unclear>\",\n  \"explanation\": \"<brief reasoning based on the actual data>\",\n  \"confidence\": \"<high | medium | low>\",\n  \"features\": [\"<feature1>\", \"<feature2>\", \"...\"]\n}\n```"}, {"role": "user", "content": "The second software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"bioconda_recipes/bioconductor-dune/lib/1.14.0\",\n    \"name\": \"bioconductor-dune\",\n    \"description\": [\n      \"Given a set of clustering labels, Dune merges pairs of clusters to increase mean ARI between labels, improving replicability.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://bioconductor.org/packages/3.18/bioc/html/Dune.html\"\n    ],\n    \"license\": [\n      {\n        \"name\": \"MIT\",\n        \"url\": null\n      }\n    ],\n    \"authors\": [],\n    \"publication\": [],\n    \"source\": [\n      \"bioconda_recipes\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "The first software metadata_entry - part 1:\n```json\n[\n  {\n    \"id\": \"biotools/dune/lib/None\",\n    \"name\": \"dune\",\n    \"description\": [\n      \"Dune is an R Package that provides a parameter-free method for optimizing the trade-off between the resolutionof the clusters and their replicability across datasets. Dune method takes as input a set of clustering results on a dataset, and iteratively merges clusters within those clusterings in order to maximize their concordance.\"\n    ],\n    \"repository\": [],\n    \"webpage\": [\n      \"https://github.com/HectorRDB/Dune\"\n    ],\n    \"license\": [],\n    \"authors\": [\n      {\n        \"type\": \"Person\",\n        \"name\": \"Sandrine Dudoit\",\n        \"email\": \"sandrine@stat.berkeley.edu\",\n        \"maintainer\": false,\n        \"url\": null,\n        \"orcid\": null\n      }\n    ],\n    \"publication\": [\n      {\n        \"$oid\": \"67c82e4f10e2f3aa0b3ba402\"\n      }\n    ],\n    \"source\": [\n      \"biotools\"\n    ]\n  }\n]\n```"}, {"role": "user", "content": "Content from https://github.com/HectorRDB/Dune:\n```\nREADME content:\n['# Dune <!-- badges: start --> [![R-CMD-check](https://github.com/HectorRDB/Dune/workflows/R-CMD-check/badge.svg?branch=master)](https://github.com/HectorRDB/Dune/actions) [![codecov](https://codecov.io/gh/HectorRDB/Dune/branch/master/graph/badge.svg?token=snxfXtj87B)](https://codecov.io/gh/HectorRDB/Dune) <!-- badges: end --> <p align=\"center\"> <img src=\"vignettes/logo.png\" width=\"30%\"/> </p> ## Contents - [Overview](#overview) - [Installation](#installation) - [Demo](#demo) - [Issues](#Issues-and-bug-reports) - [License](./LICENSE.md) - [Citation](./inst/CITATION) ## Overview Dune is an R Package that provides a parameter-free method for optimizing the trade-off between the resolutionof the clusters and their replicability across datasets. Dune method takes as input a set of clustering results on a dataset, and iteratively merges clusters within those clusterings in order to maximize their concordance. ## Installation To install the current version of *Dune* , you will need Bioconductor 3.11 (i.e devel version): ``` if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") # The following initializes usage of Bioc devel BiocManager::install(version=\\'devel\\') BiocManager::install(\"Dune\") ``` A version compatible with R 3.6 is also available with the following commands: ``` if(!requireNamespace(\"devtools\", quietly = TRUE)) { install.packages(\"devtools\") } devtools::install_github(\"HectorRDB/Dune\", ref = \"before_bioc_release\") ``` The installation should only take a few seconds. The dependencies of the package are listed in the DESCRIPTION file of the package. To install R and Bioconductor, please refer to the [Bioconductor install page](https://www.bioconductor.org/install/). ## Issues and bug reports Please use the [github issues](https://github.com/HectorRDB/Dune/issues) to submit issues, bug reports, and comments. ## Demo Start with the vignette [online](https://hectorRDB.github.io/Dune/articles/Dune.html). If you to look at the vignette source code, you can either run `browseVignettes(package = \"Dune\")` if you install the package from Bioconductor find the code for the vignette [here](./vignettes/Dune.Rmd)']```"}, {"role": "user", "content": "Content from https://github.com/HectorRDB/Dune:\n```\nRepository metadata:\n{'name': 'Dune', 'label': ['Dune'], 'description': [{'term': 'Iterative merging of clusters to improve mean ARI ', 'id': 0}], 'links': [], 'webpage': [{'term': 'https://hectorrdb.github.io/Dune', 'id': 0}], 'isDisabled': False, 'isEmpty': False, 'isLocked': False, 'isPrivate': False, 'isTemplate': False, 'version': [], 'license': [{'term': {'name': 'Other', 'url': 'http://choosealicense.com/licenses/other/'}, 'id': 0}], 'repository': [{'term': 'https://github.com/HectorRDB/Dune', 'id': 0}], 'topics': [], 'operations': [], 'authors': [{'term': {'name': 'Hector Roux de B\u00e9zieux', 'type': 'person', 'email': 'hector.rouxdebezieux@berkeley.edu', 'maintainer': False}, 'id': 0}, {'term': {'name': 'Nitesh Turaga', 'type': 'person', 'email': 'nitesh.turaga@gmail.com', 'maintainer': False}, 'id': 1}, {'term': {'name': 'koenvandenberge', 'type': 'person', 'email': 'koen.vandenberge@ugent.be', 'maintainer': False}, 'id': 2}], 'bioschemas': False, 'contribPolicy': [], 'dependencies': [], 'documentation': [{'term': {'type': 'license', 'url': 'https://github.com/HectorRDB/Dune/blob/main/LICENSE.md'}, 'id': 0}, {'term': {'type': 'readme', 'url': 'https://github.com/HectorRDB/Dune/blob/main/README.md'}, 'id': 1}], 'download': [], 'edam_operations': [], 'edam_topics': [], 'https': True, 'input': [], 'inst_instr': False, 'operational': False, 'os': [], 'output': [], 'publication': [], 'semantics': {'inputs': [], 'outputs': [], 'topics': [], 'operations': []}, 'source': ['github'], 'src': [], 'ssl': True, 'tags': [], 'test': [], 'type': ''}```"}, {"role": "user", "content": "Content from https://bioconductor.org/packages/3.18/bioc/html/Dune.html:\n```\nContent:\n['# Dune This package is for version 3.18 of Bioconductor; for the stable, up-to-date release version, see [Dune](/packages/Dune/) . ## Improving replicability in single-cell RNA-Seq cell type discovery **Bioconductor version:** 3.18 Given a set of clustering labels, Dune merges pairs of clusters to increase mean ARI between labels, improving replicability. **Author:** Hector Roux de Bezieux [aut, cre] <https://orcid.org/0000-0002-1489-8339> , Kelly Street [aut] **Maintainer:** Hector Roux de Bezieux <hector.rouxdebezieux at berkeley.edu> **Citation (from within R, entercitation(\"Dune\")):** ### Installation To install this package, start R (version \"4.3\") and enter: if (! require ( \"BiocManager\" , quietly = TRUE )) install. packages ( \"BiocManager\" ) BiocManager :: install ( \"Dune\" ) For older versions of R, please refer to the appropriate [Bioconductor release](/about/release-announcements/) . ### Documentation To view documentation for the version of this package installed in your system, start R and enter: browseVignettes (\"Dune\") ### Details biocViews [Clustering](../../BiocViews.html#___Clustering) , [GeneExpression](../../BiocViews.html#___GeneExpression) , [RNASeq](../../BiocViews.html#___RNASeq) , [SingleCell](../../BiocViews.html#___SingleCell) , [Software](../../BiocViews.html#___Software) , [Transcriptomics](../../BiocViews.html#___Transcriptomics) , [Visualization](../../BiocViews.html#___Visualization) Version 1.14.0 In Bioconductor since BioC 3.11 (R-4.0) (4 years) License MIT + file [LICENSE](../licenses/Dune/LICENSE) Depends R (>= 3.6) Imports [BiocParallel](../../bioc/html/BiocParallel.html) , [SummarizedExperiment](../../bioc/html/SummarizedExperiment.html) , utils, ggplot2, dplyr, tidyr, RColorBrewer, magrittr, gganimate, purrr, aricode System Requirements URL <> See More Suggests knitr, rmarkdown, testthat (>= 2.1.0) Linking To Enhances Depends On Me Imports Me Suggests Me Links To Me Build Report [Build Report](https://bioconductor.org/checkResults/3.18/bioc-LATEST/Dune/) ### Package Archives Follow [Installation](/install/#install-bioconductor-packages) instructions to use this package in your R session.']```"}, {"role": "user", "content": "All parts have been sent. Please now analyze the entries and provide the output as specified. \n\nIMPORTANT: Return ONLY a valid Python dictionary with the following keys: 'verdict', 'explanation', 'confidence', and 'features'. Do NOT explanation, or extra commentary. This is a strict output constraint."}]}
